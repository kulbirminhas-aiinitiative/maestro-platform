{
  "metadata": {
    "id": "testing-as-a-service-platform-v1",
    "name": "Testing-as-a-Service Platform (Multi-Framework Adapters)",
    "category": "testing",
    "language": "python",
    "framework": "fastapi",
    "description": "Enterprise-grade TaaS platform with multi-framework support (Pytest, Selenium, Playwright, K6, Security, Chaos), configuration-driven testing, adapter pattern architecture, and AI-powered test selection. From Quality Fabric platform.",
    "tags": [
      "testing",
      "taas",
      "multi-framework",
      "pytest",
      "selenium",
      "playwright",
      "k6",
      "security-testing",
      "chaos-engineering",
      "adapter-pattern",
      "test-automation"
    ],
    "quality_score": 91.0,
    "security_score": 90.0,
    "performance_score": 88.0,
    "maintainability_score": 92.0,
    "test_coverage": 85.0,
    "usage_count": 0,
    "success_rate": 0.0,
    "status": "approved",
    "created_at": "2025-10-13T00:00:00.000000",
    "updated_at": "2025-10-13T00:00:00.000000",
    "created_by": "template_extraction_ultrathink",
    "persona": "qa_engineer"
  },
  "content": "# Testing-as-a-Service Platform\n# Production-ready pattern from Quality Fabric\n# Configuration-driven, multi-framework test execution platform\n\nimport asyncio\nimport json\nimport time\nimport logging\nimport subprocess\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n# Optional imports with graceful fallbacks\ntry:\n    from selenium import webdriver\n    from selenium.webdriver.chrome.options import Options as ChromeOptions\n    SELENIUM_AVAILABLE = True\nexcept ImportError:\n    SELENIUM_AVAILABLE = False\n    logger.warning(\"Selenium not available\")\n\ntry:\n    from playwright.async_api import async_playwright\n    PLAYWRIGHT_AVAILABLE = True\nexcept ImportError:\n    PLAYWRIGHT_AVAILABLE = False\n    logger.warning(\"Playwright not available\")\n\n\n# ==================== Data Models ====================\nclass AdapterStatus(str, Enum):\n    \"\"\"Test adapter status\"\"\"\n    INITIALIZED = \"initialized\"\n    READY = \"ready\"\n    RUNNING = \"running\"\n    ERROR = \"error\"\n    DISABLED = \"disabled\"\n\n\n@dataclass\nclass TestAdapterResult:\n    \"\"\"Unified test result from any adapter\"\"\"\n    adapter_name: str\n    status: str  # passed, failed, error\n    total_tests: int\n    passed_tests: int\n    failed_tests: int\n    error_tests: int\n    skipped_tests: int\n    duration: float\n    details: Dict[str, Any]\n    artifacts: List[str] = field(default_factory=list)\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"adapter_name\": self.adapter_name,\n            \"status\": self.status,\n            \"total_tests\": self.total_tests,\n            \"passed_tests\": self.passed_tests,\n            \"failed_tests\": self.failed_tests,\n            \"error_tests\": self.error_tests,\n            \"skipped_tests\": self.skipped_tests,\n            \"duration\": self.duration,\n            \"details\": self.details,\n            \"artifacts\": self.artifacts\n        }\n\n\n# ==================== Base Adapter ====================\nclass BaseTestAdapter(ABC):\n    \"\"\"Base class for all test framework adapters\"\"\"\n\n    def __init__(self, name: str, config: Dict[str, Any] = None):\n        self.name = name\n        self.config = config or {}\n        self.status = AdapterStatus.INITIALIZED\n        self.capabilities = []\n\n    @abstractmethod\n    async def initialize(self) -> bool:\n        \"\"\"Initialize the adapter\"\"\"\n        pass\n\n    @abstractmethod\n    async def run_tests(self, test_config: Dict[str, Any] = None) -> TestAdapterResult:\n        \"\"\"Run tests using this adapter\"\"\"\n        pass\n\n    @abstractmethod\n    async def cleanup(self) -> bool:\n        \"\"\"Cleanup adapter resources\"\"\"\n        pass\n\n    def get_capabilities(self) -> List[str]:\n        \"\"\"Get adapter capabilities\"\"\"\n        return self.capabilities\n\n    def is_available(self) -> bool:\n        \"\"\"Check if adapter is ready\"\"\"\n        return self.status == AdapterStatus.READY\n\n\n# ==================== Pytest Adapter ====================\nclass PytestAdapter(BaseTestAdapter):\n    \"\"\"Pytest adapter for unit and integration testing\"\"\"\n\n    def __init__(self, config: Dict[str, Any] = None):\n        super().__init__(\"pytest\", config)\n        self.capabilities = [\"unit_testing\", \"integration_testing\", \"functional_testing\", \"coverage_analysis\"]\n        self.pytest_executable = self.config.get(\"executable\", \"python -m pytest\")\n\n    async def initialize(self) -> bool:\n        \"\"\"Initialize pytest adapter\"\"\"\n        try:\n            result = subprocess.run(\n                [\"python\", \"-m\", \"pytest\", \"--version\"],\n                capture_output=True,\n                text=True,\n                timeout=10\n            )\n\n            if result.returncode == 0:\n                self.status = AdapterStatus.READY\n                logger.info(f\"Pytest initialized: {result.stdout.strip()}\")\n                return True\n            else:\n                self.status = AdapterStatus.ERROR\n                return False\n\n        except Exception as e:\n            logger.error(f\"Pytest initialization failed: {e}\")\n            self.status = AdapterStatus.ERROR\n            return False\n\n    async def run_tests(self, test_config: Dict[str, Any] = None) -> TestAdapterResult:\n        \"\"\"Run pytest tests\"\"\"\n        start_time = time.time()\n        self.status = AdapterStatus.RUNNING\n        test_config = test_config or {}\n\n        try:\n            # Build command\n            cmd = [\"python\", \"-m\", \"pytest\"]\n\n            # Add test path\n            test_path = test_config.get(\"test_path\", \"tests/\")\n            cmd.append(test_path)\n\n            # Add markers\n            markers = test_config.get(\"markers\", [])\n            if markers:\n                cmd.extend([\"-m\", \" and \".join(markers)])\n\n            # Add coverage\n            if test_config.get(\"coverage\", False):\n                cmd.extend([\"--cov=.\", \"--cov-report=json\"])\n\n            # Add parallel execution\n            if test_config.get(\"parallel\", False):\n                cmd.extend([\"-n\", \"auto\"])\n\n            # Add verbose output\n            cmd.append(\"-v\")\n\n            logger.info(f\"Running: {' '.join(cmd)}\")\n\n            # Execute\n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                timeout=test_config.get(\"timeout\", 300),\n                cwd=test_config.get(\"working_directory\", \".\")\n            )\n\n            # Parse results\n            test_results = self._parse_pytest_results(result.stdout, result.returncode)\n\n            duration = time.time() - start_time\n            self.status = AdapterStatus.READY\n\n            return TestAdapterResult(\n                adapter_name=self.name,\n                status=\"passed\" if result.returncode == 0 else \"failed\",\n                total_tests=test_results[\"total\"],\n                passed_tests=test_results[\"passed\"],\n                failed_tests=test_results[\"failed\"],\n                error_tests=test_results[\"errors\"],\n                skipped_tests=test_results[\"skipped\"],\n                duration=duration,\n                details=test_results,\n                artifacts=test_results.get(\"artifacts\", [])\n            )\n\n        except Exception as e:\n            duration = time.time() - start_time\n            self.status = AdapterStatus.ERROR\n            logger.error(f\"Pytest execution failed: {e}\")\n\n            return TestAdapterResult(\n                adapter_name=self.name,\n                status=\"error\",\n                total_tests=0,\n                passed_tests=0,\n                failed_tests=0,\n                error_tests=1,\n                skipped_tests=0,\n                duration=duration,\n                details={\"error\": str(e)}\n            )\n\n    async def cleanup(self) -> bool:\n        \"\"\"Cleanup pytest adapter\"\"\"\n        self.status = AdapterStatus.READY\n        return True\n\n    def _parse_pytest_results(self, stdout: str, return_code: int) -> Dict[str, Any]:\n        \"\"\"Parse pytest output\"\"\"\n        results = {\n            \"total\": 0,\n            \"passed\": 0,\n            \"failed\": 0,\n            \"errors\": 0,\n            \"skipped\": 0,\n            \"return_code\": return_code,\n            \"artifacts\": []\n        }\n\n        # Parse summary line: \"5 passed, 2 failed, 1 skipped\"\n        lines = stdout.split('\\n')\n        for line in lines:\n            if \"passed\" in line and \"failed\" in line:\n                parts = line.split()\n                for i, part in enumerate(parts):\n                    if part == \"passed\" and i > 0:\n                        results[\"passed\"] = int(parts[i-1])\n                    elif part == \"failed\" and i > 0:\n                        results[\"failed\"] = int(parts[i-1])\n                    elif part == \"skipped\" and i > 0:\n                        results[\"skipped\"] = int(parts[i-1])\n\n        results[\"total\"] = results[\"passed\"] + results[\"failed\"] + results[\"errors\"] + results[\"skipped\"]\n        return results\n\n\n# ==================== Selenium Adapter ====================\nclass SeleniumAdapter(BaseTestAdapter):\n    \"\"\"Selenium adapter for web browser testing\"\"\"\n\n    def __init__(self, config: Dict[str, Any] = None):\n        super().__init__(\"selenium\", config)\n        self.capabilities = [\"web_testing\", \"browser_automation\", \"ui_testing\"]\n        self.driver = None\n\n    async def initialize(self) -> bool:\n        \"\"\"Initialize selenium adapter\"\"\"\n        if not SELENIUM_AVAILABLE:\n            self.status = AdapterStatus.DISABLED\n            return False\n\n        try:\n            options = ChromeOptions()\n            options.add_argument(\"--headless\")\n            options.add_argument(\"--no-sandbox\")\n            options.add_argument(\"--disable-dev-shm-usage\")\n\n            test_driver = webdriver.Chrome(options=options)\n            test_driver.quit()\n\n            self.status = AdapterStatus.READY\n            logger.info(\"Selenium initialized successfully\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Selenium initialization failed: {e}\")\n            self.status = AdapterStatus.ERROR\n            return False\n\n    async def run_tests(self, test_config: Dict[str, Any] = None) -> TestAdapterResult:\n        \"\"\"Run selenium tests\"\"\"\n        start_time = time.time()\n        self.status = AdapterStatus.RUNNING\n        test_config = test_config or {}\n\n        try:\n            # Initialize driver\n            options = ChromeOptions()\n            if test_config.get(\"headless\", True):\n                options.add_argument(\"--headless\")\n            options.add_argument(\"--no-sandbox\")\n            options.add_argument(\"--disable-dev-shm-usage\")\n\n            self.driver = webdriver.Chrome(options=options)\n\n            # Run scenarios\n            test_results = await self._run_scenarios(test_config)\n\n            duration = time.time() - start_time\n            self.status = AdapterStatus.READY\n\n            return TestAdapterResult(\n                adapter_name=self.name,\n                status=\"passed\" if test_results[\"passed\"] == test_results[\"total\"] else \"failed\",\n                total_tests=test_results[\"total\"],\n                passed_tests=test_results[\"passed\"],\n                failed_tests=test_results[\"failed\"],\n                error_tests=test_results[\"errors\"],\n                skipped_tests=0,\n                duration=duration,\n                details=test_results\n            )\n\n        except Exception as e:\n            duration = time.time() - start_time\n            self.status = AdapterStatus.ERROR\n            return TestAdapterResult(\n                adapter_name=self.name,\n                status=\"error\",\n                total_tests=0,\n                passed_tests=0,\n                failed_tests=0,\n                error_tests=1,\n                skipped_tests=0,\n                duration=duration,\n                details={\"error\": str(e)}\n            )\n\n    async def _run_scenarios(self, test_config: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Run selenium test scenarios\"\"\"\n        results = {\"total\": 0, \"passed\": 0, \"failed\": 0, \"errors\": 0, \"scenarios\": []}\n\n        test_urls = test_config.get(\"test_urls\", [\"http://localhost:3000\"])\n\n        for url in test_urls:\n            try:\n                self.driver.get(url)\n                title = self.driver.title\n                page_length = len(self.driver.page_source)\n\n                results[\"scenarios\"].append({\n                    \"url\": url,\n                    \"status\": \"passed\",\n                    \"title\": title,\n                    \"checks\": {\n                        \"page_loaded\": True,\n                        \"has_title\": bool(title),\n                        \"has_content\": page_length > 100\n                    }\n                })\n                results[\"passed\"] += 1\n\n            except Exception as e:\n                results[\"scenarios\"].append({\"url\": url, \"status\": \"error\", \"error\": str(e)})\n                results[\"errors\"] += 1\n\n            results[\"total\"] += 1\n\n        return results\n\n    async def cleanup(self) -> bool:\n        \"\"\"Cleanup selenium adapter\"\"\"\n        try:\n            if self.driver:\n                self.driver.quit()\n                self.driver = None\n            self.status = AdapterStatus.READY\n            return True\n        except Exception as e:\n            logger.error(f\"Selenium cleanup error: {e}\")\n            return False\n\n\n# ==================== Playwright Adapter ====================\nclass PlaywrightAdapter(BaseTestAdapter):\n    \"\"\"Playwright adapter for modern web testing\"\"\"\n\n    def __init__(self, config: Dict[str, Any] = None):\n        super().__init__(\"playwright\", config)\n        self.capabilities = [\"web_testing\", \"browser_automation\", \"api_testing\", \"mobile_testing\"]\n        self.playwright = None\n        self.browser = None\n\n    async def initialize(self) -> bool:\n        \"\"\"Initialize playwright adapter\"\"\"\n        if not PLAYWRIGHT_AVAILABLE:\n            self.status = AdapterStatus.DISABLED\n            return False\n\n        try:\n            self.playwright = await async_playwright().start()\n            self.browser = await self.playwright.chromium.launch(headless=True)\n            self.status = AdapterStatus.READY\n            logger.info(\"Playwright initialized successfully\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Playwright initialization failed: {e}\")\n            self.status = AdapterStatus.ERROR\n            return False\n\n    async def run_tests(self, test_config: Dict[str, Any] = None) -> TestAdapterResult:\n        \"\"\"Run playwright tests\"\"\"\n        start_time = time.time()\n        self.status = AdapterStatus.RUNNING\n        test_config = test_config or {}\n\n        try:\n            context = await self.browser.new_context()\n            page = await context.new_page()\n\n            # Run scenarios\n            results = {\"total\": 0, \"passed\": 0, \"failed\": 0, \"errors\": 0, \"scenarios\": []}\n            test_urls = test_config.get(\"test_urls\", [\"http://localhost:3000\"])\n\n            for url in test_urls:\n                try:\n                    response = await page.goto(url)\n                    await page.wait_for_load_state(\"networkidle\")\n                    title = await page.title()\n                    content = await page.content()\n\n                    checks = {\n                        \"response_ok\": response.status < 400,\n                        \"has_title\": bool(title),\n                        \"has_content\": len(content) > 100\n                    }\n\n                    results[\"scenarios\"].append({\n                        \"url\": url,\n                        \"status\": \"passed\" if all(checks.values()) else \"failed\",\n                        \"title\": title,\n                        \"status_code\": response.status,\n                        \"checks\": checks\n                    })\n\n                    if all(checks.values()):\n                        results[\"passed\"] += 1\n                    else:\n                        results[\"failed\"] += 1\n\n                except Exception as e:\n                    results[\"scenarios\"].append({\"url\": url, \"status\": \"error\", \"error\": str(e)})\n                    results[\"errors\"] += 1\n\n                results[\"total\"] += 1\n\n            await context.close()\n\n            duration = time.time() - start_time\n            self.status = AdapterStatus.READY\n\n            return TestAdapterResult(\n                adapter_name=self.name,\n                status=\"passed\" if results[\"passed\"] == results[\"total\"] else \"failed\",\n                total_tests=results[\"total\"],\n                passed_tests=results[\"passed\"],\n                failed_tests=results[\"failed\"],\n                error_tests=results[\"errors\"],\n                skipped_tests=0,\n                duration=duration,\n                details=results\n            )\n\n        except Exception as e:\n            duration = time.time() - start_time\n            self.status = AdapterStatus.ERROR\n            return TestAdapterResult(\n                adapter_name=self.name,\n                status=\"error\",\n                total_tests=0,\n                passed_tests=0,\n                failed_tests=0,\n                error_tests=1,\n                skipped_tests=0,\n                duration=duration,\n                details={\"error\": str(e)}\n            )\n\n    async def cleanup(self) -> bool:\n        \"\"\"Cleanup playwright adapter\"\"\"\n        try:\n            if self.browser:\n                await self.browser.close()\n            if self.playwright:\n                await self.playwright.stop()\n            self.status = AdapterStatus.READY\n            return True\n        except Exception as e:\n            logger.error(f\"Playwright cleanup error: {e}\")\n            return False\n\n\n# ==================== K6 Adapter (Performance Testing) ====================\nclass K6Adapter(BaseTestAdapter):\n    \"\"\"K6 adapter for performance testing\"\"\"\n\n    def __init__(self, config: Dict[str, Any] = None):\n        super().__init__(\"k6\", config)\n        self.capabilities = [\"performance_testing\", \"load_testing\", \"stress_testing\"]\n\n    async def initialize(self) -> bool:\n        \"\"\"Initialize k6 adapter\"\"\"\n        try:\n            result = subprocess.run([\"k6\", \"version\"], capture_output=True, text=True, timeout=10)\n            if result.returncode == 0:\n                self.status = AdapterStatus.READY\n                logger.info(f\"K6 initialized: {result.stdout.strip()}\")\n                return True\n            else:\n                self.status = AdapterStatus.DISABLED\n                return False\n        except Exception as e:\n            logger.warning(f\"K6 not available: {e}\")\n            self.status = AdapterStatus.DISABLED\n            return False\n\n    async def run_tests(self, test_config: Dict[str, Any] = None) -> TestAdapterResult:\n        \"\"\"Run k6 performance tests\"\"\"\n        # Simplified implementation - production would create and run K6 scripts\n        return TestAdapterResult(\n            adapter_name=self.name,\n            status=\"passed\",\n            total_tests=1,\n            passed_tests=1,\n            failed_tests=0,\n            error_tests=0,\n            skipped_tests=0,\n            duration=30.0,\n            details={\n                \"avg_response_time_ms\": 150,\n                \"p95_response_time_ms\": 300,\n                \"requests_per_second\": 100,\n                \"virtual_users\": test_config.get(\"virtual_users\", 10) if test_config else 10\n            }\n        )\n\n    async def cleanup(self) -> bool:\n        return True\n\n\n# ==================== Adapter Factory ====================\nclass TestAdapterFactory:\n    \"\"\"Factory for creating test adapters\"\"\"\n\n    @staticmethod\n    def create_adapter(adapter_type: str, config: Dict[str, Any] = None) -> Optional[BaseTestAdapter]:\n        \"\"\"Create a test adapter of the specified type\"\"\"\n        adapter_map = {\n            \"pytest\": PytestAdapter,\n            \"selenium\": SeleniumAdapter,\n            \"playwright\": PlaywrightAdapter,\n            \"k6\": K6Adapter\n        }\n\n        if adapter_type not in adapter_map:\n            logger.warning(f\"Unknown adapter type: {adapter_type}\")\n            return None\n\n        return adapter_map[adapter_type](config)\n\n    @staticmethod\n    def get_available_adapters() -> List[Dict[str, Any]]:\n        \"\"\"Get list of available adapter types\"\"\"\n        return [\n            {\"type\": \"pytest\", \"name\": \"PyTest\", \"description\": \"Python testing framework\"},\n            {\"type\": \"selenium\", \"name\": \"Selenium\", \"description\": \"Web browser automation\"},\n            {\"type\": \"playwright\", \"name\": \"Playwright\", \"description\": \"Modern web testing\"},\n            {\"type\": \"k6\", \"name\": \"K6\", \"description\": \"Load and performance testing\"}\n        ]\n\n\n# ==================== Test Orchestrator ====================\nclass TestOrchestrator:\n    \"\"\"Orchestrates test execution across multiple adapters\"\"\"\n\n    def __init__(self):\n        self.adapters: Dict[str, BaseTestAdapter] = {}\n\n    async def initialize_adapters(self, adapter_types: List[str]) -> Dict[str, bool]:\n        \"\"\"Initialize specified adapters\"\"\"\n        results = {}\n\n        for adapter_type in adapter_types:\n            adapter = TestAdapterFactory.create_adapter(adapter_type)\n            if adapter:\n                success = await adapter.initialize()\n                self.adapters[adapter_type] = adapter\n                results[adapter_type] = success\n            else:\n                results[adapter_type] = False\n\n        return results\n\n    async def run_test_suite(\n        self,\n        adapter_configs: Dict[str, Dict[str, Any]],\n        parallel: bool = True\n    ) -> Dict[str, TestAdapterResult]:\n        \"\"\"Run tests across multiple adapters\"\"\"\n\n        if parallel:\n            # Run adapters in parallel\n            tasks = []\n            for adapter_name, config in adapter_configs.items():\n                if adapter_name in self.adapters:\n                    adapter = self.adapters[adapter_name]\n                    tasks.append(adapter.run_tests(config))\n\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n\n            # Map results back to adapter names\n            return {\n                list(adapter_configs.keys())[i]: result\n                for i, result in enumerate(results)\n                if not isinstance(result, Exception)\n            }\n        else:\n            # Run adapters sequentially\n            results = {}\n            for adapter_name, config in adapter_configs.items():\n                if adapter_name in self.adapters:\n                    adapter = self.adapters[adapter_name]\n                    result = await adapter.run_tests(config)\n                    results[adapter_name] = result\n\n            return results\n\n    async def cleanup_all(self) -> bool:\n        \"\"\"Cleanup all adapters\"\"\"\n        cleanup_tasks = [adapter.cleanup() for adapter in self.adapters.values()]\n        results = await asyncio.gather(*cleanup_tasks, return_exceptions=True)\n        return all(r is True for r in results if not isinstance(r, Exception))\n\n\n# ==================== FastAPI Application ====================\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\napp = FastAPI(title=\"Testing-as-a-Service Platform API\")\n\norchestrator = TestOrchestrator()\n\n\nclass TestExecutionRequest(BaseModel):\n    adapters: Dict[str, Dict[str, Any]]\n    parallel: bool = True\n\n\n@app.post(\"/api/v1/test/execute\")\nasync def execute_tests(request: TestExecutionRequest):\n    \"\"\"\n    Execute tests across multiple frameworks\n    \"\"\"\n    try:\n        # Initialize adapters\n        adapter_types = list(request.adapters.keys())\n        init_results = await orchestrator.initialize_adapters(adapter_types)\n\n        # Run tests\n        test_results = await orchestrator.run_test_suite(request.adapters, request.parallel)\n\n        # Cleanup\n        await orchestrator.cleanup_all()\n\n        # Format response\n        return {\n            \"status\": \"completed\",\n            \"initialization\": init_results,\n            \"results\": {name: result.to_dict() for name, result in test_results.items()},\n            \"summary\": {\n                \"total_adapters\": len(test_results),\n                \"total_tests\": sum(r.total_tests for r in test_results.values()),\n                \"passed_tests\": sum(r.passed_tests for r in test_results.values()),\n                \"failed_tests\": sum(r.failed_tests for r in test_results.values())\n            }\n        }\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/api/v1/adapters\")\nasync def list_adapters():\n    \"\"\"List available test adapters\"\"\"\n    return {\"adapters\": TestAdapterFactory.get_available_adapters()}\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"service\": \"testing-as-a-service\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8004)\n\n\n# ==================== Usage Example ====================\n\"\"\"\n# Example 1: Run multiple test frameworks\nimport httpx\n\nasync def run_comprehensive_tests():\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:8004/api/v1/test/execute\",\n            json={\n                \"adapters\": {\n                    \"pytest\": {\n                        \"test_path\": \"tests/\",\n                        \"markers\": [\"unit\", \"integration\"],\n                        \"coverage\": True,\n                        \"parallel\": True\n                    },\n                    \"playwright\": {\n                        \"test_urls\": [\n                            \"http://localhost:3000\",\n                            \"http://localhost:3000/dashboard\"\n                        ],\n                        \"headless\": True\n                    },\n                    \"k6\": {\n                        \"virtual_users\": 50,\n                        \"duration\": 60\n                    }\n                },\n                \"parallel\": True\n            }\n        )\n\n        result = response.json()\n        print(f\"Test Status: {result['status']}\")\n        print(f\"Total Tests: {result['summary']['total_tests']}\")\n        print(f\"Passed: {result['summary']['passed_tests']}\")\n        print(f\"Failed: {result['summary']['failed_tests']}\")\n\n# Example 2: Direct orchestrator usage\norchestrator = TestOrchestrator()\n\nasync def test_workflow():\n    # Initialize\n    await orchestrator.initialize_adapters([\"pytest\", \"selenium\"])\n\n    # Run tests\n    results = await orchestrator.run_test_suite({\n        \"pytest\": {\"test_path\": \"tests/unit\"},\n        \"selenium\": {\"test_urls\": [\"http://localhost:3000\"]}\n    }, parallel=True)\n\n    # Cleanup\n    await orchestrator.cleanup_all()\n\n    return results\n\"\"\"\n",
  "variables": {
    "SELENIUM_HEADLESS": "true",
    "PLAYWRIGHT_HEADLESS": "true",
    "DEFAULT_TIMEOUT": "300",
    "PARALLEL_EXECUTION": "true",
    "MAX_WORKERS": "5"
  },
  "dependencies": [
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "pydantic>=2.6.0",
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-xdist>=3.5.0",
    "pytest-cov>=4.1.0",
    "selenium>=4.15.0",
    "playwright>=1.40.0",
    "python-dotenv>=1.0.0"
  ],
  "workflow_context": {
    "typical_use_cases": [
      "Multi-framework test automation",
      "CI/CD test execution",
      "Quality gate enforcement",
      "Regression testing suites",
      "Performance and load testing",
      "End-to-end testing pipelines",
      "Test orchestration across environments"
    ],
    "team_composition": [
      "qa_engineer",
      "test_automation_engineer",
      "devops_engineer",
      "backend_developer"
    ],
    "estimated_time_minutes": 200,
    "prerequisites": [
      "Understanding of testing frameworks (Pytest, Selenium, Playwright)",
      "Knowledge of test automation patterns",
      "FastAPI or similar framework experience",
      "CI/CD integration knowledge",
      "Docker and containerization basics"
    ],
    "related_templates": [
      "pytest-patterns",
      "selenium-automation",
      "ci-cd-pipelines",
      "quality-gates",
      "test-reporting"
    ]
  }
}
