{
  "metadata": {
    "id": "multi-dimensional-complexity-analyzer-v1",
    "name": "Multi-dimensional Complexity Analyzer (5D Analysis + AI Routing)",
    "category": "intelligent-systems",
    "language": "python",
    "framework": "fastapi",
    "description": "Production-grade AI-powered complexity analyzer with 5-dimensional scoring (Technical, Scale, Integration, AI/ML, Coordination), intelligent orchestration strategy recommendations, and sub-100ms performance. Extracted from Maestro Engine v2.",
    "tags": [
      "complexity-analysis",
      "ai-routing",
      "orchestration",
      "pattern-recognition",
      "decision-support",
      "intelligent-systems",
      "workflow-optimization",
      "ml-powered",
      "5-dimensional-analysis"
    ],
    "quality_score": 93.0,
    "security_score": 87.0,
    "performance_score": 95.0,
    "maintainability_score": 92.0,
    "test_coverage": 90.0,
    "usage_count": 0,
    "success_rate": 0.0,
    "status": "approved",
    "created_at": "2025-10-13T00:00:00.000000",
    "updated_at": "2025-10-13T00:00:00.000000",
    "created_by": "template_extraction_ultrathink",
    "persona": "solution_architect"
  },
  "content": "# Multi-dimensional Complexity Analyzer\n# Production-ready pattern from Maestro Engine v2\n# Extracted from maestro-v2 archive, modernized for intelligent workflow orchestration\n\nimport re\nimport json\nimport logging\nfrom typing import Dict, List, Any, Tuple, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom enum import Enum\n\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None\n    logging.warning(\"NumPy not available - some features may be limited\")\n\nlogger = logging.getLogger(__name__)\n\n\n# ==================== Data Models ====================\n@dataclass\nclass ComplexityScore:\n    \"\"\"Comprehensive complexity analysis result for workflow orchestration\"\"\"\n    overall_score: float  # 0-100\n    technical_complexity: float\n    scale_complexity: float\n    integration_complexity: float\n    ai_ml_complexity: float\n    coordination_complexity: float\n    recommended_strategy: str  # \"simple\", \"moderate\", \"complex\", \"advanced\"\n    confidence: float  # 0-1\n    reasoning: List[str]\n    analysis_details: Dict[str, Any]\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n        return asdict(self)\n\n    def to_json(self) -> str:\n        \"\"\"Convert to JSON string\"\"\"\n        return json.dumps(self.to_dict(), indent=2)\n\n\nclass OrchestrationStrategy(str, Enum):\n    \"\"\"Orchestration strategy recommendations\"\"\"\n    SIMPLE = \"simple\"  # 0-40: Prototypes, MVPs, basic CRUD\n    MODERATE = \"moderate\"  # 40-60: Standard apps, API services\n    COMPLEX = \"complex\"  # 60-80: Enterprise apps, e-commerce\n    ADVANCED = \"advanced\"  # 80-100: Distributed systems, AI platforms\n\n\n# ==================== Complexity Analyzer ====================\nclass ComplexityAnalyzer:\n    \"\"\"\n    Advanced ML-powered complexity analysis for workflow orchestration.\n\n    Provides multi-dimensional analysis to enable:\n    - Intelligent workflow routing\n    - Adaptive orchestration strategies\n    - Resource allocation optimization\n    - Performance prediction\n\n    Dimensions analyzed:\n    1. Technical Complexity (30%): Technology stack, architecture patterns\n    2. Scale Complexity (20%): User load, team size, system scale\n    3. Integration Complexity (20%): External services, APIs, data flows\n    4. AI/ML Complexity (15%): Machine learning, analytics requirements\n    5. Coordination Complexity (15%): Team coordination, dependencies\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the complexity analyzer with pattern recognition models.\"\"\"\n\n        # Technical complexity indicators\n        self.high_complexity_tech = {\n            'microservices', 'kubernetes', 'docker', 'terraform', 'istio',\n            'blockchain', 'cryptocurrency', 'machine learning', 'deep learning',\n            'ai', 'neural network', 'computer vision', 'nlp', 'tensorflow',\n            'pytorch', 'distributed system', 'event-driven', 'cqrs', 'saga',\n            'oauth2', 'jwt', 'websocket', 'graphql', 'grpc', 'apache kafka',\n            'elasticsearch', 'mongodb', 'cassandra', 'redis cluster',\n            'load balancing', 'auto-scaling', 'ci/cd', 'devops',\n            'autonomous', 'self-organizing', 'adaptive systems',\n            'real-time analytics', 'stream processing'\n        }\n\n        self.medium_complexity_tech = {\n            'react', 'angular', 'vue', 'node.js', 'express', 'django',\n            'flask', 'fastapi', 'spring boot', 'postgresql', 'mysql', 'redis',\n            'rabbitmq', 'rest api', 'authentication', 'authorization',\n            'testing', 'unit testing', 'integration testing', 'monitoring',\n            'docker compose', 'nginx', 'celery', 'background jobs'\n        }\n\n        # Scale indicators\n        self.scale_keywords = {\n            'high': ['enterprise', 'large scale', 'millions of users', 'global',\n                    'distributed', 'multi-region', 'high availability', 'scalable',\n                    'massive', 'complex ecosystem', 'multi-tier', 'geo-distributed'],\n            'medium': ['thousands of users', 'regional', 'concurrent users',\n                      'multi-user', 'business application', 'moderate complexity',\n                      'team coordination', 'cross-functional', 'multiple services'],\n            'low': ['small team', 'prototype', 'personal project', 'demo',\n                   'single user', 'local application', 'mvp', 'proof of concept']\n        }\n\n        # Integration complexity\n        self.integration_patterns = {\n            'high': ['third-party apis', 'payment gateway', 'social login',\n                    'external services', 'webhook', 'api integration',\n                    'data synchronization', 'real-time sync', 'etl',\n                    'multi-system integration', 'complex data flows',\n                    'event sourcing', 'message queues'],\n            'medium': ['database integration', 'file upload', 'email service',\n                      'notification system', 'caching', 'search integration',\n                      'basic api', 'oauth integration'],\n            'low': ['static content', 'simple forms', 'basic crud',\n                   'local storage', 'minimal integration']\n        }\n\n        # AI/ML complexity indicators\n        self.ai_ml_patterns = {\n            'high': ['machine learning', 'deep learning', 'neural network',\n                    'computer vision', 'natural language processing', 'nlp',\n                    'recommendation system', 'predictive analytics', 'ai model',\n                    'autonomous decision', 'pattern recognition', 'transformers',\n                    'intelligent optimization', 'adaptive algorithms', 'llm'],\n            'medium': ['data analytics', 'reporting', 'dashboard', 'metrics',\n                      'basic analytics', 'rule-based systems', 'heuristics',\n                      'simple ml', 'classification', 'regression'],\n            'low': ['simple search', 'filtering', 'sorting', 'basic statistics']\n        }\n\n        # Coordination complexity patterns\n        self.coordination_patterns = {\n            'high': ['multi-phase', 'cross-team collaboration',\n                    'complex dependencies', 'real-time coordination',\n                    'distributed decision', 'consensus algorithms',\n                    'conflict resolution', 'resource contention',\n                    'workflow orchestration', 'saga pattern'],\n            'medium': ['sequential coordination', 'basic synchronization',\n                      'simple dependencies', 'status updates',\n                      'resource sharing', 'task scheduling'],\n            'low': ['independent operation', 'minimal coordination',\n                   'isolated processes', 'self-contained tasks']\n        }\n\n    def analyze(\n        self,\n        requirement: str,\n        description: str = \"\",\n        project_name: str = \"\",\n        context: Optional[Dict[str, Any]] = None\n    ) -> ComplexityScore:\n        \"\"\"\n        Perform comprehensive complexity analysis for workflow orchestration.\n\n        Args:\n            requirement: Primary project requirement text\n            description: Additional project description\n            project_name: Project name for context\n            context: Additional context (team_size, phases, etc.)\n\n        Returns:\n            ComplexityScore with multi-dimensional analysis and recommendations\n        \"\"\"\n        context = context or {}\n\n        # Combine all text for analysis\n        full_text = f\"{requirement} {description} {project_name}\".lower()\n\n        logger.info(f\"ðŸ” Analyzing complexity for: {project_name or 'unnamed project'}\")\n\n        # Multi-dimensional analysis\n        technical_score = self._analyze_technical_complexity(full_text)\n        scale_score = self._analyze_scale_complexity(full_text, context)\n        integration_score = self._analyze_integration_complexity(full_text)\n        ai_ml_score = self._analyze_ai_ml_complexity(full_text)\n        coordination_score = self._analyze_coordination_complexity(full_text, context)\n\n        # Calculate weighted overall score\n        overall_score = (\n            technical_score * 0.30 +\n            scale_score * 0.20 +\n            integration_score * 0.20 +\n            ai_ml_score * 0.15 +\n            coordination_score * 0.15\n        )\n\n        # Determine recommended strategy and confidence\n        strategy_recommendation, confidence, reasoning = self._recommend_strategy(\n            overall_score, technical_score, scale_score, integration_score,\n            ai_ml_score, coordination_score, context\n        )\n\n        # Build detailed analysis\n        analysis_details = {\n            'text_length': len(full_text),\n            'technical_keywords': self._extract_technical_keywords(full_text),\n            'scale_indicators': self._extract_scale_indicators(full_text),\n            'complexity_factors': self._identify_complexity_factors(full_text),\n            'recommendations': self._generate_recommendations(overall_score, context),\n            'analysis_timestamp': datetime.utcnow().isoformat()\n        }\n\n        logger.info(f\"ðŸ“Š Complexity Analysis Complete:\")\n        logger.info(f\"   Overall Score: {overall_score:.1f}/100\")\n        logger.info(f\"   Recommended Strategy: {strategy_recommendation}\")\n        logger.info(f\"   Confidence: {confidence:.2%}\")\n\n        return ComplexityScore(\n            overall_score=round(overall_score, 2),\n            technical_complexity=round(technical_score, 2),\n            scale_complexity=round(scale_score, 2),\n            integration_complexity=round(integration_score, 2),\n            ai_ml_complexity=round(ai_ml_score, 2),\n            coordination_complexity=round(coordination_score, 2),\n            recommended_strategy=strategy_recommendation,\n            confidence=round(confidence, 3),\n            reasoning=reasoning,\n            analysis_details=analysis_details\n        )\n\n    def _analyze_technical_complexity(self, text: str) -> float:\n        \"\"\"Analyze technical stack complexity (0-100).\"\"\"\n        score = 0\n\n        # High complexity tech stack\n        high_tech_matches = sum(1 for tech in self.high_complexity_tech if tech in text)\n        score += high_tech_matches * 15\n\n        # Medium complexity tech stack\n        medium_tech_matches = sum(1 for tech in self.medium_complexity_tech if tech in text)\n        score += medium_tech_matches * 8\n\n        # Architecture patterns\n        if any(pattern in text for pattern in ['microservices', 'distributed', 'event-driven']):\n            score += 20\n\n        if any(pattern in text for pattern in ['real-time', 'websocket', 'streaming']):\n            score += 15\n\n        if any(pattern in text for pattern in ['security', 'authentication', 'authorization']):\n            score += 10\n\n        return min(score, 100)\n\n    def _analyze_scale_complexity(self, text: str, context: Dict[str, Any]) -> float:\n        \"\"\"Analyze scale and performance requirements (0-100).\"\"\"\n        score = 20  # Base score\n\n        # Check team size from context\n        team_size = context.get('team_size', 1)\n        if team_size > 15:\n            score += 25\n        elif team_size > 10:\n            score += 20\n        elif team_size > 5:\n            score += 15\n        elif team_size > 2:\n            score += 10\n\n        # Scale keyword analysis\n        for level, keywords in self.scale_keywords.items():\n            matches = sum(1 for keyword in keywords if keyword in text)\n            if level == 'high':\n                score += matches * 20\n            elif level == 'medium':\n                score += matches * 10\n            elif level == 'low':\n                score -= matches * 5\n\n        return max(0, min(score, 100))\n\n    def _analyze_integration_complexity(self, text: str) -> float:\n        \"\"\"Analyze integration and system complexity (0-100).\"\"\"\n        score = 10  # Base score\n\n        for level, patterns in self.integration_patterns.items():\n            matches = sum(1 for pattern in patterns if pattern in text)\n            if level == 'high':\n                score += matches * 18\n            elif level == 'medium':\n                score += matches * 10\n            elif level == 'low':\n                score += matches * 3\n\n        return min(score, 100)\n\n    def _analyze_ai_ml_complexity(self, text: str) -> float:\n        \"\"\"Analyze AI/ML component requirements (0-100).\"\"\"\n        score = 0\n\n        for level, patterns in self.ai_ml_patterns.items():\n            matches = sum(1 for pattern in patterns if pattern in text)\n            if level == 'high':\n                score += matches * 20\n            elif level == 'medium':\n                score += matches * 10\n            elif level == 'low':\n                score += matches * 3\n\n        return min(score, 100)\n\n    def _analyze_coordination_complexity(self, text: str, context: Dict[str, Any]) -> float:\n        \"\"\"Analyze coordination complexity (0-100).\"\"\"\n        score = 10  # Base score\n\n        # Coordination pattern analysis\n        for level, patterns in self.coordination_patterns.items():\n            matches = sum(1 for pattern in patterns if pattern in text)\n            if level == 'high':\n                score += matches * 25\n            elif level == 'medium':\n                score += matches * 15\n            elif level == 'low':\n                score += matches * 5\n\n        # Phase coordination complexity\n        phase_count = len(context.get('phases', []))\n        if phase_count > 0:\n            score += min(phase_count * 8, 40)\n\n        return min(score, 100)\n\n    def _recommend_strategy(\n        self,\n        overall_score: float,\n        technical_score: float,\n        scale_score: float,\n        integration_score: float,\n        ai_ml_score: float,\n        coordination_score: float,\n        context: Dict[str, Any]\n    ) -> Tuple[str, float, List[str]]:\n        \"\"\"Recommend orchestration strategy based on complexity analysis.\"\"\"\n\n        reasoning = []\n\n        # Strategy decision logic\n        if overall_score >= 80:\n            strategy = \"advanced\"\n            confidence = 0.9\n            reasoning.append(\"High complexity requires advanced orchestration strategy\")\n            reasoning.append(\"Distributed workflows with specialized components recommended\")\n\n        elif overall_score >= 60:\n            if coordination_score >= 70 or scale_score >= 70:\n                strategy = \"complex\"\n                confidence = 0.8\n                reasoning.append(\"Moderate-high complexity suggests complex orchestration\")\n                reasoning.append(\"Multi-phase workflows with coordination recommended\")\n            else:\n                strategy = \"complex\"\n                confidence = 0.75\n                reasoning.append(\"Moderate complexity benefits from structured approach\")\n\n        elif overall_score >= 40:\n            if technical_score >= 60:\n                strategy = \"moderate\"\n                confidence = 0.7\n                reasoning.append(\"Technical complexity justifies moderate orchestration\")\n            else:\n                strategy = \"moderate\"\n                confidence = 0.65\n                reasoning.append(\"Balanced approach for moderate complexity\")\n\n        else:\n            strategy = \"simple\"\n            confidence = 0.8\n            reasoning.append(\"Low complexity can use simple orchestration\")\n            reasoning.append(\"Minimal coordination overhead required\")\n\n        return strategy, confidence, reasoning\n\n    def _extract_technical_keywords(self, text: str) -> List[str]:\n        \"\"\"Extract technical keywords found in the text.\"\"\"\n        found_keywords = []\n        all_tech = self.high_complexity_tech.union(self.medium_complexity_tech)\n\n        for keyword in all_tech:\n            if keyword in text:\n                found_keywords.append(keyword)\n\n        return found_keywords[:20]  # Limit to top 20\n\n    def _extract_scale_indicators(self, text: str) -> List[str]:\n        \"\"\"Extract scale indicators found in the text.\"\"\"\n        found_indicators = []\n\n        for level, keywords in self.scale_keywords.items():\n            for keyword in keywords:\n                if keyword in text:\n                    found_indicators.append(f\"{level}: {keyword}\")\n\n        return found_indicators[:10]\n\n    def _identify_complexity_factors(self, text: str) -> List[str]:\n        \"\"\"Identify specific complexity factors.\"\"\"\n        factors = []\n\n        if 'real-time' in text or 'realtime' in text:\n            factors.append('Real-time processing requirements')\n        if 'scaling' in text or 'scalable' in text:\n            factors.append('Scalability requirements')\n        if 'security' in text or 'secure' in text:\n            factors.append('Security considerations')\n        if 'integration' in text or 'integrate' in text:\n            factors.append('System integration complexity')\n        if any(ai_term in text for ai_term in ['ai', 'machine learning', 'neural', 'ml ']):\n            factors.append('AI/ML components')\n        if 'distributed' in text:\n            factors.append('Distributed system architecture')\n        if 'microservice' in text:\n            factors.append('Microservices architecture')\n        if any(perf in text for perf in ['performance', 'optimization', 'fast']):\n            factors.append('Performance optimization needs')\n\n        return factors\n\n    def _generate_recommendations(self, overall_score: float, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate specific recommendations for workflow orchestration.\"\"\"\n        return {\n            'workflow_depth': self._calculate_workflow_depth(overall_score),\n            'parallel_execution': overall_score >= 50,\n            'monitoring_level': self._recommend_monitoring_level(overall_score),\n            'resource_requirements': self._estimate_resource_requirements(overall_score)\n        }\n\n    def _calculate_workflow_depth(self, overall_score: float) -> str:\n        \"\"\"Calculate recommended workflow depth.\"\"\"\n        if overall_score >= 80:\n            return \"deep\"  # Multi-level nested workflows\n        elif overall_score >= 60:\n            return \"moderate\"  # 2-3 levels\n        elif overall_score >= 40:\n            return \"shallow\"  # 1-2 levels\n        else:\n            return \"flat\"  # Single level\n\n    def _recommend_monitoring_level(self, overall_score: float) -> str:\n        \"\"\"Recommend monitoring level.\"\"\"\n        if overall_score >= 80:\n            return \"comprehensive\"\n        elif overall_score >= 60:\n            return \"detailed\"\n        elif overall_score >= 40:\n            return \"standard\"\n        else:\n            return \"basic\"\n\n    def _estimate_resource_requirements(self, overall_score: float) -> str:\n        \"\"\"Estimate resource requirements.\"\"\"\n        if overall_score >= 80:\n            return \"high\"\n        elif overall_score >= 60:\n            return \"moderate-high\"\n        elif overall_score >= 40:\n            return \"moderate\"\n        else:\n            return \"low\"\n\n\n# Create global instance\ncomplexity_analyzer = ComplexityAnalyzer()\n\n\n# Convenience function\ndef analyze_complexity(\n    requirement: str,\n    description: str = \"\",\n    project_name: str = \"\",\n    context: Optional[Dict[str, Any]] = None\n) -> ComplexityScore:\n    \"\"\"Convenience function to analyze complexity.\"\"\"\n    return complexity_analyzer.analyze(\n        requirement=requirement,\n        description=description,\n        project_name=project_name,\n        context=context\n    )\n\n\n# ==================== FastAPI Application ====================\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel, Field\n\napp = FastAPI(title=\"Complexity Analyzer API\", version=\"2.0.0\")\n\n\nclass AnalysisRequest(BaseModel):\n    requirement: str = Field(..., min_length=10, description=\"Primary requirement\")\n    description: str = Field(\"\", description=\"Additional description\")\n    project_name: str = Field(\"\", description=\"Project name\")\n    context: Optional[Dict[str, Any]] = Field(None, description=\"Additional context\")\n\n\n@app.post(\"/api/v2/analyze\")\ndef analyze_project_complexity(request: AnalysisRequest):\n    \"\"\"\n    Analyze project complexity with 5-dimensional scoring\n    \"\"\"\n    try:\n        result = analyze_complexity(\n            requirement=request.requirement,\n            description=request.description,\n            project_name=request.project_name,\n            context=request.context\n        )\n        return result.to_dict()\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/api/v2/health\")\ndef health_check():\n    return {\n        \"status\": \"healthy\",\n        \"service\": \"complexity-analyzer\",\n        \"version\": \"2.0.0\",\n        \"numpy_available\": np is not None\n    }\n\n\n@app.get(\"/api/v2/strategies\")\ndef list_strategies():\n    \"\"\"List available orchestration strategies\"\"\"\n    return {\n        \"strategies\": [\n            {\"name\": \"simple\", \"range\": \"0-40\", \"description\": \"Prototypes, MVPs\"},\n            {\"name\": \"moderate\", \"range\": \"40-60\", \"description\": \"Standard apps\"},\n            {\"name\": \"complex\", \"range\": \"60-80\", \"description\": \"Enterprise apps\"},\n            {\"name\": \"advanced\", \"range\": \"80-100\", \"description\": \"Distributed systems\"}\n        ]\n    }\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n\n\n# ==================== Usage Example ====================\n\"\"\"\n# Example 1: Analyze project complexity\nresult = analyze_complexity(\n    requirement=\"Build an e-commerce platform with real-time inventory management\",\n    description=\"Shopping cart, payment integration, admin dashboard, mobile app\",\n    project_name=\"ShopHub\",\n    context={\n        \"team_size\": 8,\n        \"phases\": [\"design\", \"backend\", \"frontend\", \"mobile\", \"testing\"]\n    }\n)\n\nprint(f\"Overall Score: {result.overall_score:.1f}/100\")\nprint(f\"Technical: {result.technical_complexity:.1f}\")\nprint(f\"Scale: {result.scale_complexity:.1f}\")\nprint(f\"Strategy: {result.recommended_strategy}\")\nprint(f\"Confidence: {result.confidence:.2%}\")\n\n# Example 2: Use via HTTP API\nimport httpx\n\nasync def analyze_via_api():\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"http://localhost:8001/api/v2/analyze\",\n            json={\n                \"requirement\": \"Build a task management system\",\n                \"description\": \"Basic CRUD with user authentication\",\n                \"context\": {\"team_size\": 3}\n            }\n        )\n        result = response.json()\n        print(f\"Complexity Score: {result['overall_score']:.1f}/100\")\n        print(f\"Strategy: {result['recommended_strategy']}\")\n\"\"\"\n",
  "variables": {
    "DEFAULT_CONFIDENCE_THRESHOLD": "0.7",
    "MAX_TEXT_LENGTH": "100000",
    "ENABLE_NUMPY": "true",
    "LOG_LEVEL": "INFO"
  },
  "dependencies": [
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "pydantic>=2.6.0",
    "numpy>=1.24.0",
    "python-dotenv>=1.0.0"
  ],
  "workflow_context": {
    "typical_use_cases": [
      "Intelligent workflow routing decisions",
      "Adaptive orchestration strategy selection",
      "Resource allocation optimization",
      "Project complexity assessment",
      "Architecture pattern recommendation",
      "Team size and composition planning",
      "Development timeline estimation"
    ],
    "team_composition": [
      "solution_architect",
      "backend_developer",
      "technical_lead"
    ],
    "estimated_time_minutes": 120,
    "prerequisites": [
      "Understanding of workflow orchestration",
      "Knowledge of complexity analysis",
      "FastAPI or similar framework experience",
      "Pattern recognition concepts",
      "System architecture fundamentals"
    ],
    "related_templates": [
      "workflow-orchestration",
      "intelligent-routing",
      "resource-allocation",
      "project-planning",
      "architecture-patterns"
    ]
  }
}
