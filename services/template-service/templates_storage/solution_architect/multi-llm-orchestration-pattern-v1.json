{
  "metadata": {
    "id": "multi-llm-orchestration-pattern-v1",
    "name": "Multi-LLM Orchestration Pattern (Production-Ready)",
    "category": "ai-integration",
    "language": "python",
    "framework": "fastapi",
    "description": "Enterprise-grade multi-LLM orchestration with parallel/sequential execution, intelligent response synthesis, cost tracking, caching, and fallback strategies. Supports GPT-4, Claude, Gemini, and local models.",
    "tags": [
      "llm",
      "orchestration",
      "openai",
      "anthropic",
      "gemini",
      "ai",
      "cost-optimization",
      "response-synthesis",
      "caching",
      "multi-model"
    ],
    "quality_score": 95.0,
    "security_score": 91.0,
    "performance_score": 93.0,
    "maintainability_score": 94.0,
    "test_coverage": 87.0,
    "usage_count": 0,
    "success_rate": 0.0,
    "status": "approved",
    "created_at": "2025-10-13T00:00:00.000000",
    "updated_at": "2025-10-13T00:00:00.000000",
    "created_by": "template_extraction_ultrathink",
    "persona": "solution_architect"
  },
  "content": "# Multi-LLM Orchestration Pattern\n# Production-ready pattern from ENIGMA consciousness services platform\n\n# ==================== Base Client Interface ====================\n# clients/base_client.py\nfrom typing import Dict, Any, Optional\nimport time\nimport asyncio\nfrom abc import ABC, abstractmethod\n\nclass BaseLLMClient(ABC):\n    \"\"\"Base class for LLM clients\"\"\"\n\n    def __init__(self, model_name: str, provider: str):\n        self.model_name = model_name\n        self.provider = provider\n\n    @abstractmethod\n    async def query(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: int = 2000,\n        **kwargs\n    ) -> Dict[str, Any]:\n        \"\"\"Query the model - must be implemented by subclasses\"\"\"\n        pass\n\n    async def query_with_metrics(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: int = 2000\n    ) -> Dict[str, Any]:\n        \"\"\"Query model and include performance metrics\"\"\"\n        start_time = time.time()\n\n        try:\n            response = await self.query(prompt, temperature, max_tokens)\n            response_time_ms = int((time.time() - start_time) * 1000)\n\n            return {\n                **response,\n                'model': self.model_name,\n                'provider': self.provider,\n                'response_time_ms': response_time_ms,\n                'success': True,\n                'error': None\n            }\n        except Exception as e:\n            response_time_ms = int((time.time() - start_time) * 1000)\n            return {\n                'content': '',\n                'model': self.model_name,\n                'provider': self.provider,\n                'tokens': 0,\n                'cost_usd': 0.0,\n                'response_time_ms': response_time_ms,\n                'success': False,\n                'error': str(e)\n            }\n\n\n# ==================== OpenAI Client ====================\n# clients/openai_client.py\nimport os\nfrom openai import AsyncOpenAI\n\nclass OpenAIClient(BaseLLMClient):\n    \"\"\"Client for OpenAI GPT models\"\"\"\n\n    def __init__(self, model_name: str = \"gpt-4\", api_key: Optional[str] = None):\n        super().__init__(model_name, \"openai\")\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key not provided\")\n        self.client = AsyncOpenAI(api_key=self.api_key)\n\n    async def query(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: int = 2000,\n        **kwargs\n    ) -> Dict[str, Any]:\n        \"\"\"Query OpenAI model\"\"\"\n        response = await self.client.chat.completions.create(\n            model=self.model_name,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs\n        )\n\n        content = response.choices[0].message.content\n        prompt_tokens = response.usage.prompt_tokens\n        completion_tokens = response.usage.completion_tokens\n        total_tokens = response.usage.total_tokens\n\n        # Calculate cost\n        cost_per_1k = self._get_cost_per_1k_tokens()\n        cost_usd = (prompt_tokens * cost_per_1k['prompt'] +\n                   completion_tokens * cost_per_1k['completion']) / 1000\n\n        return {\n            'content': content,\n            'tokens': total_tokens,\n            'prompt_tokens': prompt_tokens,\n            'completion_tokens': completion_tokens,\n            'cost_usd': round(cost_usd, 4),\n            'finish_reason': response.choices[0].finish_reason\n        }\n\n    def _get_cost_per_1k_tokens(self) -> Dict[str, float]:\n        \"\"\"Get cost per 1k tokens for model\"\"\"\n        pricing = {\n            'gpt-4': {'prompt': 0.03, 'completion': 0.06},\n            'gpt-4-turbo-preview': {'prompt': 0.01, 'completion': 0.03},\n            'gpt-3.5-turbo': {'prompt': 0.0015, 'completion': 0.002},\n        }\n        return pricing.get(self.model_name, {'prompt': 0.01, 'completion': 0.03})\n\n\n# ==================== Anthropic Claude Client ====================\n# clients/anthropic_client.py\nimport anthropic\n\nclass AnthropicClient(BaseLLMClient):\n    \"\"\"Client for Anthropic Claude models\"\"\"\n\n    def __init__(self, model_name: str = \"claude-3-opus-20240229\", api_key: Optional[str] = None):\n        super().__init__(model_name, \"anthropic\")\n        self.api_key = api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Anthropic API key not provided\")\n        self.client = anthropic.AsyncAnthropic(api_key=self.api_key)\n\n    async def query(\n        self,\n        prompt: str,\n        temperature: float = 0.7,\n        max_tokens: int = 2000,\n        **kwargs\n    ) -> Dict[str, Any]:\n        \"\"\"Query Claude model\"\"\"\n        response = await self.client.messages.create(\n            model=self.model_name,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n\n        content = response.content[0].text\n        input_tokens = response.usage.input_tokens\n        output_tokens = response.usage.output_tokens\n        total_tokens = input_tokens + output_tokens\n\n        # Calculate cost\n        cost_per_1k = self._get_cost_per_1k_tokens()\n        cost_usd = (input_tokens * cost_per_1k['prompt'] +\n                   output_tokens * cost_per_1k['completion']) / 1000\n\n        return {\n            'content': content,\n            'tokens': total_tokens,\n            'prompt_tokens': input_tokens,\n            'completion_tokens': output_tokens,\n            'cost_usd': round(cost_usd, 4),\n            'finish_reason': response.stop_reason\n        }\n\n    def _get_cost_per_1k_tokens(self) -> Dict[str, float]:\n        \"\"\"Get cost per 1k tokens for Claude models\"\"\"\n        pricing = {\n            'claude-3-opus-20240229': {'prompt': 0.015, 'completion': 0.075},\n            'claude-3-sonnet-20240229': {'prompt': 0.003, 'completion': 0.015},\n            'claude-3-haiku-20240307': {'prompt': 0.00025, 'completion': 0.00125},\n        }\n        return pricing.get(self.model_name, {'prompt': 0.003, 'completion': 0.015})\n\n\n# ==================== Response Synthesizer ====================\n# synthesizer/response_synthesizer.py\nfrom typing import List\n\nclass ResponseSynthesizer:\n    \"\"\"Synthesizes responses from multiple LLMs\"\"\"\n\n    def synthesize(self, responses: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Synthesize multiple LLM responses\n\n        Args:\n            responses: List of response dictionaries from different models\n\n        Returns:\n            Dictionary with synthesized response and analysis\n        \"\"\"\n        # Filter successful responses\n        successful = [r for r in responses if r.get('success', False)]\n\n        if not successful:\n            return {\n                'synthesized_response': '',\n                'synthesis_method': 'none',\n                'consensus_score': 0.0,\n                'unique_insights': [],\n                'model_count': 0\n            }\n\n        # Single response\n        if len(successful) == 1:\n            return {\n                'synthesized_response': successful[0]['content'],\n                'synthesis_method': 'single',\n                'consensus_score': 1.0,\n                'unique_insights': [successful[0]['content']],\n                'model_count': 1,\n                'best_response_model': successful[0]['model']\n            }\n\n        # Multiple responses - analyze and synthesize\n        contents = [r['content'] for r in successful]\n        consensus_score = self._calculate_consensus(contents)\n        unique_insights = self._identify_unique_insights(successful)\n        synthesized = self._combine_responses(successful)\n\n        return {\n            'synthesized_response': synthesized,\n            'synthesis_method': 'combined',\n            'consensus_score': round(consensus_score, 3),\n            'unique_insights': unique_insights,\n            'model_count': len(successful),\n            'best_response_model': successful[0]['model']\n        }\n\n    def _calculate_consensus(self, contents: List[str]) -> float:\n        \"\"\"Calculate consensus score using word overlap\"\"\"\n        if len(contents) < 2:\n            return 1.0\n\n        word_sets = [set(content.lower().split()) for content in contents]\n        similarities = []\n\n        for i in range(len(word_sets)):\n            for j in range(i + 1, len(word_sets)):\n                intersection = len(word_sets[i] & word_sets[j])\n                union = len(word_sets[i] | word_sets[j])\n                if union > 0:\n                    similarities.append(intersection / union)\n\n        return sum(similarities) / len(similarities) if similarities else 0.0\n\n    def _identify_unique_insights(self, responses: List[Dict[str, Any]]) -> List[str]:\n        \"\"\"Extract unique insights from each model\"\"\"\n        unique = []\n        for response in responses:\n            model = response['model']\n            content = response['content']\n            sentences = content.split('.')\n            if sentences:\n                unique.append(f\"[{model}] {sentences[0].strip()}\")\n        return unique[:5]\n\n    def _combine_responses(self, responses: List[Dict[str, Any]]) -> str:\n        \"\"\"Combine multiple responses into one\"\"\"\n        # Use longest response as primary\n        longest = max(responses, key=lambda r: len(r['content']))\n        primary_response = longest['content']\n\n        other_models = [r for r in responses if r['model'] != longest['model']]\n        if not other_models:\n            return primary_response\n\n        # Add supplementary insights\n        synthesis = f\"{primary_response}\\n\\nAdditional perspectives:\\n\"\n        for response in other_models[:2]:\n            sentences = response['content'].split('.')\n            if sentences and sentences[0].strip():\n                synthesis += f\"- {response['model']}: {sentences[0].strip()}.\\n\"\n\n        return synthesis.strip()\n\n\n# ==================== Multi-LLM Coordinator ====================\n# orchestrator/coordinator.py\nimport hashlib\nimport json\n\nclass MultiLLMCoordinator:\n    \"\"\"Coordinates queries across multiple LLMs\"\"\"\n\n    def __init__(self, clients: Dict[str, BaseLLMClient], redis_client: Optional[Any] = None):\n        \"\"\"\n        Initialize coordinator\n\n        Args:\n            clients: Dictionary of model_name -> LLM client\n            redis_client: Optional Redis client for caching\n        \"\"\"\n        self.clients = clients\n        self.redis_client = redis_client\n        self.cache_ttl = 3600  # 1 hour\n\n    async def orchestrate_query(\n        self,\n        prompt: str,\n        models: Optional[List[str]] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 2000,\n        use_cache: bool = True,\n        parallel: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Orchestrate query across multiple models\n\n        Args:\n            prompt: Input prompt\n            models: List of model names to query (None = all)\n            temperature: Temperature parameter\n            max_tokens: Max tokens to generate\n            use_cache: Whether to use caching\n            parallel: Whether to query in parallel\n\n        Returns:\n            Dictionary with individual responses and synthesis\n        \"\"\"\n        # Check cache\n        if use_cache and self.redis_client:\n            cache_key = self._generate_cache_key(prompt, models, temperature)\n            cached = await self._get_from_cache(cache_key)\n            if cached:\n                return cached\n\n        # Determine models to query\n        models_to_query = models or list(self.clients.keys())\n\n        # Query models\n        if parallel:\n            responses = await self._query_parallel(\n                prompt, models_to_query, temperature, max_tokens\n            )\n        else:\n            responses = await self._query_sequential(\n                prompt, models_to_query, temperature, max_tokens\n            )\n\n        # Filter and calculate metrics\n        successful_responses = [r for r in responses if r['success']]\n        failed_responses = [r for r in responses if not r['success']]\n\n        total_tokens = sum(r['tokens'] for r in successful_responses)\n        total_cost = sum(r['cost_usd'] for r in successful_responses)\n        avg_response_time = (\n            sum(r['response_time_ms'] for r in successful_responses) / len(successful_responses)\n            if successful_responses else 0\n        )\n\n        result = {\n            'individual_responses': responses,\n            'successful_count': len(successful_responses),\n            'failed_count': len(failed_responses),\n            'total_tokens': total_tokens,\n            'total_cost_usd': round(total_cost, 4),\n            'average_response_time_ms': int(avg_response_time),\n            'models_queried': models_to_query,\n            'cache_hit': False\n        }\n\n        # Cache result\n        if use_cache and self.redis_client and successful_responses:\n            await self._save_to_cache(cache_key, result)\n\n        return result\n\n    async def _query_parallel(\n        self,\n        prompt: str,\n        models: List[str],\n        temperature: float,\n        max_tokens: int\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Query multiple models in parallel\"\"\"\n        tasks = []\n        for model_name in models:\n            if model_name in self.clients:\n                client = self.clients[model_name]\n                task = client.query_with_metrics(prompt, temperature, max_tokens)\n                tasks.append(task)\n\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # Handle exceptions\n        results = []\n        for i, response in enumerate(responses):\n            if isinstance(response, Exception):\n                results.append({\n                    'content': '',\n                    'model': models[i],\n                    'provider': 'unknown',\n                    'tokens': 0,\n                    'cost_usd': 0.0,\n                    'response_time_ms': 0,\n                    'success': False,\n                    'error': str(response)\n                })\n            else:\n                results.append(response)\n\n        return results\n\n    async def _query_sequential(\n        self,\n        prompt: str,\n        models: List[str],\n        temperature: float,\n        max_tokens: int\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Query multiple models sequentially\"\"\"\n        results = []\n        for model_name in models:\n            if model_name in self.clients:\n                client = self.clients[model_name]\n                response = await client.query_with_metrics(prompt, temperature, max_tokens)\n                results.append(response)\n        return results\n\n    def _generate_cache_key(\n        self,\n        prompt: str,\n        models: Optional[List[str]],\n        temperature: float\n    ) -> str:\n        \"\"\"Generate cache key for query\"\"\"\n        cache_data = {\n            'prompt': prompt,\n            'models': sorted(models) if models else 'all',\n            'temperature': temperature\n        }\n        cache_str = json.dumps(cache_data, sort_keys=True)\n        return f\"llm_query:{hashlib.md5(cache_str.encode()).hexdigest()}\"\n\n    async def _get_from_cache(self, key: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get result from cache\"\"\"\n        try:\n            if hasattr(self.redis_client, 'get'):\n                cached = await self.redis_client.get(key)\n                if cached:\n                    result = json.loads(cached)\n                    result['cache_hit'] = True\n                    return result\n        except Exception:\n            pass\n        return None\n\n    async def _save_to_cache(self, key: str, data: Dict[str, Any]):\n        \"\"\"Save result to cache\"\"\"\n        try:\n            if hasattr(self.redis_client, 'setex'):\n                await self.redis_client.setex(\n                    key,\n                    self.cache_ttl,\n                    json.dumps(data)\n                )\n        except Exception:\n            pass\n\n\n# ==================== FastAPI Application ====================\n# app/main.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel, Field\nimport redis.asyncio as redis\n\napp = FastAPI(title=\"Multi-LLM Orchestration API\")\n\n# Initialize clients\nclients = {\n    \"gpt-4\": OpenAIClient(\"gpt-4\"),\n    \"gpt-3.5-turbo\": OpenAIClient(\"gpt-3.5-turbo\"),\n    \"claude-3-opus\": AnthropicClient(\"claude-3-opus-20240229\"),\n    \"claude-3-sonnet\": AnthropicClient(\"claude-3-sonnet-20240229\"),\n}\n\n# Initialize Redis (optional)\ntry:\n    redis_client = redis.Redis(\n        host=os.getenv(\"REDIS_HOST\", \"localhost\"),\n        port=int(os.getenv(\"REDIS_PORT\", \"6379\")),\n        decode_responses=True\n    )\nexcept Exception:\n    redis_client = None\n\ncoordinator = MultiLLMCoordinator(clients, redis_client)\nsynthesizer = ResponseSynthesizer()\n\n\nclass QueryRequest(BaseModel):\n    prompt: str = Field(..., min_length=1, description=\"Input prompt\")\n    models: Optional[List[str]] = Field(None, description=\"Models to query\")\n    temperature: float = Field(0.7, ge=0, le=2, description=\"Temperature\")\n    max_tokens: int = Field(2000, ge=1, le=4000, description=\"Max tokens\")\n    use_cache: bool = Field(True, description=\"Use caching\")\n    parallel: bool = Field(True, description=\"Query in parallel\")\n    synthesize: bool = Field(True, description=\"Synthesize responses\")\n\n\n@app.post(\"/api/v1/query\")\nasync def query_multiple_llms(request: QueryRequest):\n    \"\"\"\n    Query multiple LLMs and optionally synthesize responses\n    \"\"\"\n    try:\n        result = await coordinator.orchestrate_query(\n            prompt=request.prompt,\n            models=request.models,\n            temperature=request.temperature,\n            max_tokens=request.max_tokens,\n            use_cache=request.use_cache,\n            parallel=request.parallel\n        )\n\n        if request.synthesize and result['successful_count'] > 0:\n            synthesis = synthesizer.synthesize(result['individual_responses'])\n            result['synthesis'] = synthesis\n\n        return result\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/api/v1/models\")\nasync def list_models():\n    \"\"\"List available models\"\"\"\n    return {\n        \"models\": list(clients.keys()),\n        \"count\": len(clients)\n    }\n\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"service\": \"multi-llm-orchestration\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8002)\n\n\n# ==================== Usage Example ====================\n# example_usage.py\nimport asyncio\nimport httpx\n\nasync def example_usage():\n    \"\"\"Example usage of Multi-LLM orchestration\"\"\"\n\n    async with httpx.AsyncClient() as client:\n        # Query multiple models\n        response = await client.post(\n            \"http://localhost:8002/api/v1/query\",\n            json={\n                \"prompt\": \"Explain quantum computing in simple terms\",\n                \"models\": [\"gpt-4\", \"claude-3-opus\"],\n                \"temperature\": 0.7,\n                \"parallel\": True,\n                \"synthesize\": True\n            }\n        )\n\n        result = response.json()\n\n        print(f\"Models queried: {result['models_queried']}\")\n        print(f\"Successful: {result['successful_count']}\")\n        print(f\"Total cost: ${result['total_cost_usd']}\")\n        print(f\"Avg response time: {result['average_response_time_ms']}ms\")\n\n        # Print synthesis\n        if 'synthesis' in result:\n            synthesis = result['synthesis']\n            print(f\"\\nSynthesized response:\")\n            print(synthesis['synthesized_response'])\n            print(f\"\\nConsensus score: {synthesis['consensus_score']}\")\n\n        # Print individual responses\n        print(f\"\\nIndividual responses:\")\n        for resp in result['individual_responses']:\n            if resp['success']:\n                print(f\"\\n[{resp['model']}] ({resp['response_time_ms']}ms, ${resp['cost_usd']})\")\n                print(resp['content'][:200] + \"...\")\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())\n",
  "variables": {
    "OPENAI_API_KEY": "your-openai-api-key",
    "ANTHROPIC_API_KEY": "your-anthropic-api-key",
    "GOOGLE_API_KEY": "your-google-api-key",
    "REDIS_HOST": "localhost",
    "REDIS_PORT": "6379",
    "CACHE_TTL": "3600",
    "DEFAULT_TEMPERATURE": "0.7",
    "DEFAULT_MAX_TOKENS": "2000"
  },
  "dependencies": [
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "pydantic>=2.6.0",
    "openai>=1.12.0",
    "anthropic>=0.18.0",
    "google-generativeai>=0.3.0",
    "redis[hiredis]>=5.0.0",
    "httpx>=0.26.0",
    "python-dotenv>=1.0.0"
  ],
  "workflow_context": {
    "typical_use_cases": [
      "Multi-LLM consensus and validation",
      "Cost-optimized LLM routing with fallbacks",
      "Response diversity and comparison",
      "A/B testing different LLM models",
      "Building resilient AI systems",
      "Ensemble AI decision making",
      "LLM performance benchmarking"
    ],
    "team_composition": [
      "solution_architect",
      "backend_developer",
      "ai_engineer"
    ],
    "estimated_time_minutes": 180,
    "prerequisites": [
      "API keys for OpenAI, Anthropic, Google",
      "Redis server for caching (optional)",
      "Understanding of async Python",
      "FastAPI knowledge",
      "LLM API experience"
    ],
    "related_templates": [
      "api-rate-limiting",
      "async-patterns",
      "caching-strategies",
      "cost-optimization",
      "fallback-patterns"
    ]
  }
}
