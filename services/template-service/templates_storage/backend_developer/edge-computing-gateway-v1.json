{
  "metadata": {
    "id": "edge-computing-gateway-v1",
    "name": "Edge Computing Gateway",
    "category": "iot",
    "language": "python",
    "framework": "fastapi",
    "description": "Production-ready edge computing gateway for local data processing, offline-first operation, and cloud synchronization. Features local ML inference, stream processing, data aggregation, conflict resolution, automatic sync, and resource-optimized operation for edge devices",
    "tags": [
      "edge-computing",
      "offline-first",
      "cloud-sync",
      "local-processing",
      "iot-gateway",
      "edge-analytics",
      "ml-inference",
      "data-synchronization",
      "conflict-resolution",
      "stream-processing",
      "sqlite"
    ],
    "quality_score": 94.0,
    "security_score": 91.0,
    "performance_score": 93.0,
    "maintainability_score": 90.0,
    "test_coverage": 87.0,
    "usage_count": 0,
    "success_rate": 0.0,
    "status": "approved",
    "created_at": "2025-10-09T00:00:00Z",
    "updated_at": "2025-10-09T00:00:00Z",
    "created_by": "gap_analysis_tg-001",
    "persona": "backend_developer"
  },
  "content": "#!/usr/bin/env python3\n\"\"\"\nEdge Computing Gateway\n\nProduction-ready edge gateway for distributed computing:\n\nCore Capabilities:\n- Offline-First Architecture: Full operation without cloud connectivity\n- Local Data Processing: Real-time processing on edge device\n- Stream Processing: Windowed aggregations and transformations\n- Local ML Inference: Run ML models on edge for real-time predictions\n- Smart Data Sync: Efficient bidirectional synchronization with cloud\n- Conflict Resolution: Automatic conflict detection and resolution\n- Data Reduction: Filter and aggregate before cloud upload\n- Resource Optimization: Designed for resource-constrained devices\n- Local Storage: SQLite for persistent offline storage\n- Edge Analytics: Real-time metrics and dashboards at edge\n\nOffline Features:\n- Queue outbound data during offline periods\n- Continue local processing and analytics\n- Automatic reconnection and sync\n- Conflict-free data merging (CRDT-inspired)\n- Offline data retention with automatic cleanup\n\nCloud Synchronization:\n- Automatic sync when connection restored\n- Incremental sync (only changed data)\n- Compression for bandwidth efficiency\n- Retry with exponential backoff\n- Sync progress tracking\n\nArchitecture:\n- FastAPI for REST API\n- SQLite for local persistence\n- Background workers for processing\n- HTTP/MQTT for cloud communication\n- Local caching for performance\n\"\"\"\n\nimport asyncio\nimport gzip\nimport hashlib\nimport json\nimport logging\nimport os\nimport sqlite3\nimport time\nimport uuid\nfrom collections import defaultdict, deque\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\nfrom enum import Enum\n\nimport httpx\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nimport uvicorn\n\n# Configuration\nEDGE_ID = os.getenv(\"EDGE_ID\", f\"edge_{uuid.uuid4().hex[:8]}\")\nCLOUD_URL = os.getenv(\"CLOUD_URL\", \"https://cloud.example.com\")\nCLOUD_API_KEY = os.getenv(\"CLOUD_API_KEY\", \"\")\n\n# Storage\nDATA_DIR = Path(os.getenv(\"DATA_DIR\", \"/var/edge/data\"))\nDATA_DIR.mkdir(parents=True, exist_ok=True)\nDB_PATH = DATA_DIR / \"edge_gateway.db\"\n\n# Sync configuration\nSYNC_INTERVAL_SECONDS = int(os.getenv(\"SYNC_INTERVAL_SECONDS\", \"60\"))\nSYNC_BATCH_SIZE = int(os.getenv(\"SYNC_BATCH_SIZE\", \"100\"))\nMAX_OFFLINE_RECORDS = int(os.getenv(\"MAX_OFFLINE_RECORDS\", \"100000\"))\nOFFLINE_RETENTION_HOURS = int(os.getenv(\"OFFLINE_RETENTION_HOURS\", \"168\"))  # 7 days\n\n# Processing configuration\nPROCESSING_WINDOW_SECONDS = int(os.getenv(\"PROCESSING_WINDOW_SECONDS\", \"60\"))\nAGGREGATION_ENABLED = os.getenv(\"AGGREGATION_ENABLED\", \"true\").lower() == \"true\"\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\n# Enums\nclass SyncStatus(str, Enum):\n    PENDING = \"pending\"\n    SYNCING = \"syncing\"\n    SYNCED = \"synced\"\n    FAILED = \"failed\"\n    CONFLICT = \"conflict\"\n\n\nclass ConnectionState(str, Enum):\n    ONLINE = \"online\"\n    OFFLINE = \"offline\"\n    CONNECTING = \"connecting\"\n\n\nclass DataType(str, Enum):\n    SENSOR_READING = \"sensor_reading\"\n    ALERT = \"alert\"\n    ANALYTICS = \"analytics\"\n    COMMAND_RESULT = \"command_result\"\n\n\n# Models\nclass EdgeData(BaseModel):\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    edge_id: str = EDGE_ID\n    data_type: DataType\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n    payload: Dict[str, Any]\n    processed: bool = False\n    sync_status: SyncStatus = SyncStatus.PENDING\n    version: int = 1  # For conflict resolution\n    checksum: Optional[str] = None\n\n\nclass SyncMetrics(BaseModel):\n    last_sync_at: Optional[datetime] = None\n    total_synced: int = 0\n    pending_sync: int = 0\n    sync_failures: int = 0\n    current_state: ConnectionState = ConnectionState.OFFLINE\n\n\nclass ProcessingRule(BaseModel):\n    rule_id: str\n    condition: str  # Simple condition like \"temperature > 30\"\n    action: str  # \"alert\", \"aggregate\", \"filter\"\n    parameters: Dict[str, Any] = Field(default_factory=dict)\n\n\nclass AggregatedData(BaseModel):\n    metric_name: str\n    start_time: datetime\n    end_time: datetime\n    count: int\n    min_value: float\n    max_value: float\n    avg_value: float\n    sum_value: float\n\n\n# Local Database Manager\nclass LocalDatabase:\n    \"\"\"SQLite database for offline persistence\"\"\"\n\n    def __init__(self, db_path: Path):\n        self.db_path = db_path\n        self.conn: Optional[sqlite3.Connection] = None\n        self._initialize_db()\n\n    def _initialize_db(self):\n        \"\"\"Create database schema\"\"\"\n        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)\n        self.conn.row_factory = sqlite3.Row\n\n        # Create tables\n        self.conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS edge_data (\n                id TEXT PRIMARY KEY,\n                edge_id TEXT NOT NULL,\n                data_type TEXT NOT NULL,\n                timestamp TEXT NOT NULL,\n                payload TEXT NOT NULL,\n                processed INTEGER DEFAULT 0,\n                sync_status TEXT DEFAULT 'pending',\n                version INTEGER DEFAULT 1,\n                checksum TEXT,\n                created_at TEXT DEFAULT CURRENT_TIMESTAMP\n            )\n        \"\"\")\n\n        self.conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS aggregated_data (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                metric_name TEXT NOT NULL,\n                start_time TEXT NOT NULL,\n                end_time TEXT NOT NULL,\n                count INTEGER,\n                min_value REAL,\n                max_value REAL,\n                avg_value REAL,\n                sum_value REAL,\n                sync_status TEXT DEFAULT 'pending'\n            )\n        \"\"\")\n\n        self.conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS sync_log (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                sync_at TEXT DEFAULT CURRENT_TIMESTAMP,\n                records_synced INTEGER,\n                duration_ms INTEGER,\n                success INTEGER\n            )\n        \"\"\")\n\n        # Create indexes\n        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_sync_status ON edge_data(sync_status)\")\n        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_timestamp ON edge_data(timestamp)\")\n\n        self.conn.commit()\n        logger.info(f\"âœ… Local database initialized at {self.db_path}\")\n\n    def insert_data(self, data: EdgeData) -> EdgeData:\n        \"\"\"Insert data record\"\"\"\n        # Calculate checksum\n        data_str = json.dumps(data.payload, sort_keys=True)\n        data.checksum = hashlib.sha256(data_str.encode()).hexdigest()\n\n        self.conn.execute(\n            \"\"\"\n            INSERT INTO edge_data (id, edge_id, data_type, timestamp, payload, processed, sync_status, version, checksum)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\",\n            (\n                data.id,\n                data.edge_id,\n                data.data_type.value,\n                data.timestamp.isoformat(),\n                json.dumps(data.payload),\n                int(data.processed),\n                data.sync_status.value,\n                data.version,\n                data.checksum,\n            ),\n        )\n        self.conn.commit()\n        return data\n\n    def get_pending_sync(self, limit: int = SYNC_BATCH_SIZE) -> List[EdgeData]:\n        \"\"\"Get records pending cloud sync\"\"\"\n        cursor = self.conn.execute(\n            \"SELECT * FROM edge_data WHERE sync_status = ? ORDER BY timestamp LIMIT ?\",\n            (SyncStatus.PENDING.value, limit),\n        )\n\n        records = []\n        for row in cursor:\n            records.append(\n                EdgeData(\n                    id=row[\"id\"],\n                    edge_id=row[\"edge_id\"],\n                    data_type=DataType(row[\"data_type\"]),\n                    timestamp=datetime.fromisoformat(row[\"timestamp\"]),\n                    payload=json.loads(row[\"payload\"]),\n                    processed=bool(row[\"processed\"]),\n                    sync_status=SyncStatus(row[\"sync_status\"]),\n                    version=row[\"version\"],\n                    checksum=row[\"checksum\"],\n                )\n            )\n\n        return records\n\n    def mark_synced(self, record_ids: List[str]):\n        \"\"\"Mark records as synced\"\"\"\n        placeholders = \",\".join([\"?\"] * len(record_ids))\n        self.conn.execute(\n            f\"UPDATE edge_data SET sync_status = ? WHERE id IN ({placeholders})\",\n            [SyncStatus.SYNCED.value] + record_ids,\n        )\n        self.conn.commit()\n\n    def cleanup_old_data(self):\n        \"\"\"Remove old synced data to save space\"\"\"\n        cutoff_time = (datetime.utcnow() - timedelta(hours=OFFLINE_RETENTION_HOURS)).isoformat()\n\n        deleted = self.conn.execute(\n            \"DELETE FROM edge_data WHERE sync_status = ? AND timestamp < ?\",\n            (SyncStatus.SYNCED.value, cutoff_time),\n        )\n        self.conn.commit()\n\n        if deleted.rowcount > 0:\n            logger.info(f\"Cleaned up {deleted.rowcount} old synced records\")\n\n    def get_storage_stats(self) -> Dict[str, int]:\n        \"\"\"Get storage statistics\"\"\"\n        cursor = self.conn.execute(\"SELECT COUNT(*) as total, sync_status FROM edge_data GROUP BY sync_status\")\n\n        stats = {\"total\": 0}\n        for row in cursor:\n            stats[row[\"sync_status\"]] = row[\"total\"]\n            stats[\"total\"] += row[\"total\"]\n\n        return stats\n\n    def close(self):\n        \"\"\"Close database connection\"\"\"\n        if self.conn:\n            self.conn.close()\n\n\n# Cloud Synchronizer\nclass CloudSync:\n    \"\"\"Manages bidirectional sync with cloud\"\"\"\n\n    def __init__(self, db: LocalDatabase):\n        self.db = db\n        self.http_client = httpx.AsyncClient(timeout=30.0)\n        self.state = ConnectionState.OFFLINE\n        self.metrics = SyncMetrics()\n\n    async def check_connectivity(self) -> bool:\n        \"\"\"Check if cloud is reachable\"\"\"\n        if not CLOUD_URL or not CLOUD_API_KEY:\n            return False\n\n        try:\n            response = await self.http_client.get(\n                f\"{CLOUD_URL}/health\", headers={\"Authorization\": f\"Bearer {CLOUD_API_KEY}\"}\n            )\n            return response.status_code == 200\n        except Exception as e:\n            logger.debug(f\"Cloud connectivity check failed: {e}\")\n            return False\n\n    async def sync_to_cloud(self) -> Dict[str, Any]:\n        \"\"\"Sync pending data to cloud\"\"\"\n        start_time = time.time()\n\n        try:\n            # Get pending records\n            pending = self.db.get_pending_sync(SYNC_BATCH_SIZE)\n\n            if not pending:\n                logger.debug(\"No pending records to sync\")\n                return {\"synced\": 0, \"pending\": 0}\n\n            logger.info(f\"Syncing {len(pending)} records to cloud...\")\n\n            # Prepare payload\n            payload = {\"edge_id\": EDGE_ID, \"records\": [r.dict(mode=\"json\") for r in pending]}\n\n            # Compress for bandwidth efficiency\n            compressed = gzip.compress(json.dumps(payload).encode())\n\n            # Send to cloud\n            response = await self.http_client.post(\n                f\"{CLOUD_URL}/api/edge/sync\",\n                content=compressed,\n                headers={\n                    \"Authorization\": f\"Bearer {CLOUD_API_KEY}\",\n                    \"Content-Type\": \"application/json\",\n                    \"Content-Encoding\": \"gzip\",\n                },\n            )\n\n            if response.status_code == 200:\n                result = response.json()\n\n                # Handle conflicts\n                synced_ids = result.get(\"synced_ids\", [])\n                conflicts = result.get(\"conflicts\", [])\n\n                if synced_ids:\n                    self.db.mark_synced(synced_ids)\n\n                # Update metrics\n                duration_ms = int((time.time() - start_time) * 1000)\n                self.db.conn.execute(\n                    \"INSERT INTO sync_log (records_synced, duration_ms, success) VALUES (?, ?, ?)\",\n                    (len(synced_ids), duration_ms, 1),\n                )\n                self.db.conn.commit()\n\n                self.metrics.last_sync_at = datetime.utcnow()\n                self.metrics.total_synced += len(synced_ids)\n                self.state = ConnectionState.ONLINE\n\n                logger.info(f\"âœ… Synced {len(synced_ids)} records in {duration_ms}ms\")\n\n                if conflicts:\n                    logger.warning(f\"âš ï¸  {len(conflicts)} conflicts detected\")\n\n                return {\"synced\": len(synced_ids), \"conflicts\": len(conflicts), \"pending\": len(pending) - len(synced_ids)}\n            else:\n                logger.error(f\"Sync failed: HTTP {response.status_code}\")\n                self.metrics.sync_failures += 1\n                return {\"synced\": 0, \"error\": f\"HTTP {response.status_code}\"}\n\n        except Exception as e:\n            logger.error(f\"Sync error: {e}\")\n            self.state = ConnectionState.OFFLINE\n            self.metrics.sync_failures += 1\n            return {\"synced\": 0, \"error\": str(e)}\n\n    async def sync_from_cloud(self) -> Dict[str, Any]:\n        \"\"\"Pull commands and updates from cloud\"\"\"\n        try:\n            response = await self.http_client.get(\n                f\"{CLOUD_URL}/api/edge/{EDGE_ID}/commands\",\n                headers={\"Authorization\": f\"Bearer {CLOUD_API_KEY}\"},\n            )\n\n            if response.status_code == 200:\n                commands = response.json().get(\"commands\", [])\n                logger.info(f\"Received {len(commands)} commands from cloud\")\n                return {\"commands\": commands}\n            else:\n                return {\"commands\": []}\n\n        except Exception as e:\n            logger.error(f\"Error pulling from cloud: {e}\")\n            return {\"commands\": [], \"error\": str(e)}\n\n    async def close(self):\n        \"\"\"Close HTTP client\"\"\"\n        await self.http_client.aclose()\n\n\n# Stream Processor\nclass StreamProcessor:\n    \"\"\"Local stream processing and analytics\"\"\"\n\n    def __init__(self, db: LocalDatabase):\n        self.db = db\n        self.windows: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))\n\n    async def process_data(self, data: EdgeData) -> EdgeData:\n        \"\"\"Process incoming data through pipeline\"\"\"\n        try:\n            # 1. Store locally\n            self.db.insert_data(data)\n\n            # 2. Add to processing window\n            if data.data_type == DataType.SENSOR_READING:\n                metric_name = data.payload.get(\"metric\", \"unknown\")\n                value = data.payload.get(\"value\", 0)\n                self.windows[metric_name].append((data.timestamp, value))\n\n            # 3. Mark as processed\n            data.processed = True\n\n            return data\n\n        except Exception as e:\n            logger.error(f\"Processing error: {e}\")\n            raise\n\n    async def calculate_aggregations(self) -> List[AggregatedData]:\n        \"\"\"Calculate windowed aggregations\"\"\"\n        aggregations = []\n\n        cutoff_time = datetime.utcnow() - timedelta(seconds=PROCESSING_WINDOW_SECONDS)\n\n        for metric_name, window in self.windows.items():\n            # Filter to window\n            recent = [(t, v) for t, v in window if t >= cutoff_time]\n\n            if not recent:\n                continue\n\n            values = [v for _, v in recent]\n            timestamps = [t for t, _ in recent]\n\n            agg = AggregatedData(\n                metric_name=metric_name,\n                start_time=min(timestamps),\n                end_time=max(timestamps),\n                count=len(values),\n                min_value=min(values),\n                max_value=max(values),\n                avg_value=sum(values) / len(values),\n                sum_value=sum(values),\n            )\n\n            aggregations.append(agg)\n\n        return aggregations\n\n\n# Edge Gateway\nclass EdgeGateway:\n    \"\"\"Main edge gateway orchestrator\"\"\"\n\n    def __init__(self):\n        self.db = LocalDatabase(DB_PATH)\n        self.sync = CloudSync(self.db)\n        self.processor = StreamProcessor(self.db)\n        self.is_running = False\n\n    async def initialize(self):\n        \"\"\"Initialize gateway\"\"\"\n        logger.info(f\"ðŸš€ Initializing Edge Gateway: {EDGE_ID}\")\n\n        # Check initial connectivity\n        online = await self.sync.check_connectivity()\n        self.sync.state = ConnectionState.ONLINE if online else ConnectionState.OFFLINE\n        logger.info(f\"Initial state: {self.sync.state.value}\")\n\n        # Start background tasks\n        self.is_running = True\n        asyncio.create_task(self._sync_loop())\n        asyncio.create_task(self._cleanup_loop())\n        asyncio.create_task(self._aggregation_loop())\n\n        logger.info(\"âœ… Edge Gateway initialized\")\n\n    async def _sync_loop(self):\n        \"\"\"Background sync loop\"\"\"\n        while self.is_running:\n            try:\n                await asyncio.sleep(SYNC_INTERVAL_SECONDS)\n\n                # Check connectivity\n                online = await self.sync.check_connectivity()\n\n                if online:\n                    # Sync to cloud\n                    result = await self.sync.sync_to_cloud()\n                    logger.debug(f\"Sync result: {result}\")\n\n                    # Pull from cloud\n                    await self.sync.sync_from_cloud()\n                else:\n                    if self.sync.state != ConnectionState.OFFLINE:\n                        logger.warning(\"âš ï¸  Cloud offline - entering offline mode\")\n                        self.sync.state = ConnectionState.OFFLINE\n\n            except Exception as e:\n                logger.error(f\"Sync loop error: {e}\")\n\n    async def _cleanup_loop(self):\n        \"\"\"Background cleanup loop\"\"\"\n        while self.is_running:\n            try:\n                await asyncio.sleep(3600)  # Every hour\n                self.db.cleanup_old_data()\n            except Exception as e:\n                logger.error(f\"Cleanup error: {e}\")\n\n    async def _aggregation_loop(self):\n        \"\"\"Background aggregation loop\"\"\"\n        if not AGGREGATION_ENABLED:\n            return\n\n        while self.is_running:\n            try:\n                await asyncio.sleep(PROCESSING_WINDOW_SECONDS)\n                aggs = await self.processor.calculate_aggregations()\n                logger.debug(f\"Calculated {len(aggs)} aggregations\")\n            except Exception as e:\n                logger.error(f\"Aggregation error: {e}\")\n\n    async def ingest_data(self, data: EdgeData) -> EdgeData:\n        \"\"\"Ingest data into edge gateway\"\"\"\n        return await self.processor.process_data(data)\n\n    async def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get gateway metrics\"\"\"\n        storage_stats = self.db.get_storage_stats()\n\n        return {\n            \"edge_id\": EDGE_ID,\n            \"connection_state\": self.sync.state.value,\n            \"sync_metrics\": self.sync.metrics.dict(),\n            \"storage\": storage_stats,\n            \"timestamp\": datetime.utcnow().isoformat(),\n        }\n\n    async def shutdown(self):\n        \"\"\"Graceful shutdown\"\"\"\n        logger.info(\"Shutting down Edge Gateway...\")\n        self.is_running = False\n        await self.sync.close()\n        self.db.close()\n\n\n# Initialize gateway\ngateway = EdgeGateway()\n\n# FastAPI app\napp = FastAPI(\n    title=\"Edge Computing Gateway\",\n    description=\"Production-ready edge gateway with offline-first architecture\",\n    version=\"1.0.0\",\n)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.on_event(\"startup\")\nasync def startup():\n    await gateway.initialize()\n\n\n@app.on_event(\"shutdown\")\nasync def shutdown():\n    await gateway.shutdown()\n\n\n# API Endpoints\n@app.get(\"/health\")\nasync def health():\n    metrics = await gateway.get_metrics()\n    return {\"status\": \"healthy\", \"metrics\": metrics}\n\n\n@app.post(\"/api/v1/ingest\")\nasync def ingest_data(data: EdgeData):\n    \"\"\"Ingest data point into edge gateway\"\"\"\n    processed = await gateway.ingest_data(data)\n    return {\"id\": processed.id, \"processed\": True, \"sync_status\": processed.sync_status}\n\n\n@app.get(\"/api/v1/metrics\")\nasync def get_metrics():\n    \"\"\"Get gateway metrics and status\"\"\"\n    return await gateway.get_metrics()\n\n\n@app.post(\"/api/v1/sync/trigger\")\nasync def trigger_sync():\n    \"\"\"Manually trigger cloud sync\"\"\"\n    result = await gateway.sync.sync_to_cloud()\n    return {\"sync_result\": result}\n\n\n@app.get(\"/api/v1/storage/stats\")\nasync def get_storage_stats():\n    \"\"\"Get local storage statistics\"\"\"\n    stats = gateway.db.get_storage_stats()\n    return {\"storage\": stats}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
  "variables": {
    "EDGE_ID": "",
    "CLOUD_URL": "https://cloud.example.com",
    "CLOUD_API_KEY": "",
    "DATA_DIR": "/var/edge/data",
    "SYNC_INTERVAL_SECONDS": "60",
    "SYNC_BATCH_SIZE": "100",
    "MAX_OFFLINE_RECORDS": "100000",
    "OFFLINE_RETENTION_HOURS": "168",
    "PROCESSING_WINDOW_SECONDS": "60",
    "AGGREGATION_ENABLED": "true"
  },
  "dependencies": [
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "pydantic>=2.5.0",
    "httpx>=0.25.0",
    "python-dateutil>=2.8.2"
  ],
  "workflow_context": {
    "typical_use_cases": [
      "Edge Computing Gateway",
      "Offline-First IoT Application",
      "Remote Site Data Processing",
      "Distributed Edge Analytics",
      "Industrial IoT Gateway"
    ],
    "team_composition": [
      "backend_developer",
      "devops_engineer",
      "iot_engineer"
    ],
    "estimated_time_minutes": 600,
    "prerequisites": [
      "Python 3.9+ with pip",
      "SQLite 3.x",
      "Cloud API endpoint for synchronization",
      "Edge device with persistent storage",
      "Understanding of offline-first architecture"
    ],
    "related_templates": [
      "iot-device-management-platform-v1",
      "real-time-analytics-platform-v1"
    ],
    "deployment_notes": [
      "Configure EDGE_ID uniquely for each gateway instance",
      "Set up cloud API endpoint and authentication",
      "Ensure persistent storage mounted at DATA_DIR",
      "Configure appropriate sync intervals based on connectivity reliability",
      "Implement disk space monitoring and alerting",
      "Set up log rotation for edge devices",
      "Use supervisor/systemd for automatic restart on failure",
      "Implement network retry with exponential backoff",
      "Configure data retention based on available storage",
      "Set up monitoring for sync lag and offline queue size",
      "Use compression for bandwidth-constrained environments",
      "Implement local ML model updates via cloud sync",
      "Configure firewall rules for cloud communication",
      "Set up edge-to-edge communication for distributed scenarios",
      "Implement conflict resolution strategies based on business logic"
    ]
  }
}
