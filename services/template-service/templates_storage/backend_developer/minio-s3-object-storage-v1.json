{
  "template_id": "minio-s3-object-storage-v1",
  "name": "MinIO S3-Compatible Object Storage",
  "version": "1.0.0",
  "description": "Production-ready S3-compatible object storage integration using MinIO. Supports upload/download, versioning, presigned URLs, lifecycle policies, and multipart uploads.",
  "persona": "backend_developer",
  "category": "storage",
  "complexity": "moderate",
  "tags": [
    "minio",
    "s3",
    "object-storage",
    "aws",
    "storage",
    "boto3",
    "python"
  ],
  "use_cases": [
    "ML model storage",
    "File upload/download",
    "Data lake",
    "Backup and archival"
  ],
  "quality_score": 92,
  "security_score": 93,
  "performance_score": 90,
  "maintainability_score": 91,
  "extracted_from": "Conductor ML Platform (Production)",
  "implementation": {
    "s3_connector.py": "\"\"\"\nAWS S3 Storage Connector for Maestro ML Platform\n\nFeatures:\n- Upload/download artifacts\n- List objects\n- Versioning support\n- Multipart uploads\n- Presigned URLs\n- Lifecycle management\n\"\"\"\n\nimport os\nimport boto3\nfrom botocore.exceptions import ClientError\nfrom typing import Optional, List, Dict, Any, BinaryIO\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n\nlogger = logging.getLogger(__name__)\n\n\nclass S3Connector:\n    \"\"\"\n    AWS S3 storage connector\n\n    Usage:\n        connector = S3Connector(\n            bucket_name=\"maestro-ml-artifacts\",\n            region=\"us-east-1\"\n        )\n\n        # Upload file\n        connector.upload_file(\"model.pkl\", \"models/model_v1.pkl\")\n\n        # Download file\n        connector.download_file(\"models/model_v1.pkl\", \"local_model.pkl\")\n\n        # List objects\n        objects = connector.list_objects(prefix=\"models/\")\n    \"\"\"\n\n    def __init__(\n        self,\n        bucket_name: str,\n        region: Optional[str] = None,\n        aws_access_key_id: Optional[str] = None,\n        aws_secret_access_key: Optional[str] = None,\n        endpoint_url: Optional[str] = None,\n        create_bucket: bool = False\n    ):\n        \"\"\"\n        Initialize S3 connector\n\n        Args:\n            bucket_name: S3 bucket name\n            region: AWS region\n            aws_access_key_id: AWS access key (or use environment variable)\n            aws_secret_access_key: AWS secret key (or use environment variable)\n            endpoint_url: Custom endpoint URL (for S3-compatible services)\n            create_bucket: Create bucket if it doesn't exist\n        \"\"\"\n        self.bucket_name = bucket_name\n        self.region = region or os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n\n        # Initialize S3 client\n        session_kwargs = {}\n        if aws_access_key_id:\n            session_kwargs[\"aws_access_key_id\"] = aws_access_key_id\n        if aws_secret_access_key:\n            session_kwargs[\"aws_secret_access_key\"] = aws_secret_access_key\n        if region:\n            session_kwargs[\"region_name\"] = region\n\n        self.s3_client = boto3.client(\"s3\", **session_kwargs, endpoint_url=endpoint_url)\n        self.s3_resource = boto3.resource(\"s3\", **session_kwargs, endpoint_url=endpoint_url)\n\n        # Create bucket if requested\n        if create_bucket:\n            self._create_bucket_if_not_exists()\n\n        logger.info(f\"S3 connector initialized: {bucket_name} ({self.region})\")\n\n    def _create_bucket_if_not_exists(self):\n        \"\"\"Create bucket if it doesn't exist\"\"\"\n        try:\n            self.s3_client.head_bucket(Bucket=self.bucket_name)\n            logger.info(f\"Bucket exists: {self.bucket_name}\")\n        except ClientError:\n            # Bucket doesn't exist, create it\n            try:\n                if self.region == \"us-east-1\":\n                    self.s3_client.create_bucket(Bucket=self.bucket_name)\n                else:\n                    self.s3_client.create_bucket(\n                        Bucket=self.bucket_name,\n                        CreateBucketConfiguration={\"LocationConstraint\": self.region}\n                    )\n                logger.info(f\"Created bucket: {self.bucket_name}\")\n            except ClientError as e:\n                logger.error(f\"Failed to create bucket: {e}\")\n                raise\n\n    def upload_file(\n        self,\n        local_path: str,\n        s3_key: str,\n        metadata: Optional[Dict[str, str]] = None,\n        storage_class: str = \"STANDARD\",\n        acl: str = \"private\"\n    ) -> bool:\n        \"\"\"\n        Upload file to S3\n\n        Args:\n            local_path: Local file path\n            s3_key: S3 object key\n            metadata: Object metadata\n            storage_class: Storage class (STANDARD, INTELLIGENT_TIERING, etc.)\n            acl: Access control list\n\n        Returns:\n            Success boolean\n        \"\"\"\n        try:\n            extra_args = {\n                \"StorageClass\": storage_class,\n                \"ACL\": acl\n            }\n\n            if metadata:\n                extra_args[\"Metadata\"] = metadata\n\n            self.s3_client.upload_file(\n                local_path,\n                self.bucket_name,\n                s3_key,\n                ExtraArgs=extra_args\n            )\n\n            logger.info(f\"Uploaded: {local_path} -> s3://{self.bucket_name}/{s3_key}\")\n            return True\n\n        except ClientError as e:\n            logger.error(f\"Upload failed: {e}\")\n            return False\n\n    def upload_fileobj(\n        self,\n        file_obj: BinaryIO,\n        s3_key: str,\n        metadata: Optional[Dict[str, str]] = None\n    ) -> bool:\n        \"\"\"\n        Upload file-like object to S3\n\n        Args:\n            file_obj: File-like object\n            s3_key: S3 object key\n            metadata: Object metadata\n\n        Returns:\n            Success boolean\n        \"\"\"\n        try:\n            extra_args = {}\n            if metadata:\n                extra_args[\"Metadata\"] = metadata\n\n            self.s3_client.upload_fileobj(\n                file_obj,\n                self.bucket_name,\n                s3_key,\n                ExtraArgs=extra_args\n            )\n\n            logger.info(f\"Uploaded file object -> s3://{self.bucket_name}/{s3_key}\")\n            return True\n\n        except ClientError as e:\n            logger.error(f\"Upload failed: {e}\")\n            return False\n\n    def download_file(\n        self,\n        s3_key: str,\n        local_path: str,\n        version_id: Optional[str] = None\n    ) -> bool:\n        \"\"\"\n        Download file from S3\n\n        Args:\n            s3_key: S3 object key\n            local_path: Local destination path\n            version_id: Specific version to download\n\n        Returns:\n            Success boolean\n        \"\"\"\n        try:\n            # Create directory if needed\n            Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n\n            extra_args = {}\n            if version_id:\n                extra_args[\"VersionId\"] = version_id\n\n            self.s3_client.download_file(\n                self.bucket_name,\n                s3_key,\n                local_path,\n                ExtraArgs=extra_args\n            )\n\n            logger.info(f\"Downloaded: s3://{self.bucket_name}/{s3_key} -> {local_path}\")\n            return True\n\n        except ClientError as e:\n            logger.error(f\"Download failed: {e}\")\n            return False\n\n    def download_fileobj(\n        self,\n        s3_key: str,\n        file_obj: BinaryIO,\n        version_id: Optional[str] = None\n    ) -> bool:\n        \"\"\"\n        Download file to file-like object\n\n        Args:\n            s3_key: S3 object key\n            file_obj: File-like object to write to\n            version_id: Specific version to download\n\n        Returns:\n            Success boolean\n        \"\"\"\n        try:\n            extra_args = {}\n            if version_id:\n                extra_args[\"VersionId\"] = version_id\n\n            self.s3_client.download_fileobj(\n                self.bucket_name,\n                s3_key,\n                file_obj,\n                ExtraArgs=extra_args\n            )\n\n            logger.info(f\"Downloaded: s3://{self.bucket_name}/{s3_key} to file object\")\n            return True\n\n        except ClientError as e:\n            logger.error(f\"Download failed: {e}\")\n            return False\n\n    def list_objects(\n        self,\n        prefix: str = \"\",\n        max_keys: int = 1000,\n        delimiter: str = \"\"\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        List objects in bucket\n\n        Args:\n            prefix: Object key prefix filter\n            max_keys: Maximum number of objects to return\n            delimiter: Delimiter for grouping\n\n        Returns:\n            List of object metadata dictionaries\n        \"\"\"\n        try:\n            response = self.s3_client.list_objects_v2(\n                Bucket=self.bucket_name,\n                Prefix=prefix,\n                MaxKeys=max_keys,\n                Delimiter=delimiter\n            )\n\n            objects = []\n            if \"Contents\" in response:\n                for obj in response[\"Contents\"]:\n                    objects.append({\n                        \"key\": obj[\"Key\"],\n                        \"size\": obj[\"Size\"],\n                        \"last_modified\": obj[\"LastModified\"],\n                        \"etag\": obj[\"ETag\"]\n                    })\n\n            logger.info(f\"Listed {len(objects)} objects with prefix: {prefix}\")\n            return objects\n\n        except ClientError as e:\n            logger.error(f\"List failed: {e}\")\n            return []\n\n    def delete_object(self, s3_key: str, version_id: Optional[str] = None) -> bool:\n        \"\"\"\n        Delete object from S3\n\n        Args:\n            s3_key: S3 object key\n            version_id: Specific version to delete\n\n        Returns:\n            Success boolean\n        \"\"\"\n        try:\n            delete_kwargs = {\n                \"Bucket\": self.bucket_name,\n                \"Key\": s3_key\n            }\n\n            if version_id:\n                delete_kwargs[\"VersionId\"] = version_id\n\n            self.s3_client.delete_object(**delete_kwargs)\n\n            logger.info(f\"Deleted: s3://{self.bucket_name}/{s3_key}\")\n            return True\n\n        except ClientError as e:\n            logger.error(f\"Delete failed: {e}\")\n            return False\n\n    def object_exists(self, s3_key: str) -> bool:\n        \"\"\"\n        Check if object exists\n\n        Args:\n            s3_key: S3 object key\n\n        Returns:\n            Exists boolean\n        \"\"\"\n        try:\n            self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)\n            return True\n        except ClientError:\n            return False\n\n    def get_object_metadata(self, s3_key: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Get object metadata\n\n        Args:\n            s3_key: S3 object key\n\n        Returns:\n            Metadata dictionary or None\n        \"\"\"\n        try:\n            response = self.s3_client.head_object(\n                Bucket=self.bucket_name,\n                Key=s3_key\n            )\n\n            return {\n                \"size\": response[\"ContentLength\"],\n                \"last_modified\": response[\"LastModified\"],\n                \"content_type\": response.get(\"ContentType\"),\n                \"metadata\": response.get(\"Metadata\", {}),\n                \"etag\": response[\"ETag\"],\n                \"version_id\": response.get(\"VersionId\")\n            }\n\n        except ClientError as e:\n            logger.error(f\"Get metadata failed: {e}\")\n            return None\n\n    def generate_presigned_url(\n        self,\n        s3_key: str,\n        expiration: int = 3600,\n        method: str = \"get_object\"\n    ) -> Optional[str]:\n        \"\"\"\n        Generate presigned URL for temporary access\n\n        Args:\n            s3_key: S3 object key\n            expiration: URL expiration in seconds\n            method: S3 method (get_object, put_object)\n\n        Returns:\n            Presigned URL or None\n        \"\"\"\n        try:\n            url = self.s3_client.generate_presigned_url(\n                method,\n                Params={\n                    \"Bucket\": self.bucket_name,\n                    \"Key\": s3_key\n                },\n                ExpiresIn=expiration\n            )\n\n            logger.info(f\"Generated presigned URL for: {s3_key}\")\n            return url\n\n        except ClientError as e:\n            logger.error(f\"Generate presigned URL failed: {e}\")\n            return None\n\n    def copy_object(\n        self,\n        source_key: str,\n        destination_key: str,\n        source_bucket: Optional[str] = None\n    ) -> bool:\n        \"\"\"\n        Copy object within or between buckets\n\n        Args:\n            source_key: Source object key\n            destination_key: Destination object key\n            source_bucket: Source bucket (defaults to same bucket)\n\n        Returns:\n            Success boolean\n        \"\"\"\n        try:\n            source_bucket = source_bucket or self.bucket_name\n            copy_source = {\n                \"Bucket\": source_bucket,\n                \"Key\": source_key\n            }\n\n            self.s3_client.copy_object(\n                CopySource=copy_source,\n                Bucket=self.bucket_name,\n                Key=destination_key\n            )\n\n            logger.info(f\"Copied: {source_key} -> {destination_key}\")\n            return True\n\n        except ClientError as e:\n            logger.error(f\"Copy failed: {e}\")\n            return False\n\n    def enable_versioning(self) -> bool:\n        \"\"\"\n        Enable versioning on bucket\n\n        Returns:\n            Success boolean\n        \"\"\"\n        try:\n            self.s3_client.put_bucket_versioning(\n                Bucket=self.bucket_name,\n                VersioningConfiguration={\"Status\": \"Enabled\"}\n            )\n\n            logger.info(f\"Enabled versioning on: {self.bucket_name}\")\n            return True\n\n        except ClientError as e:\n            logger.error(f\"Enable versioning failed: {e}\")\n            return False\n\n    def set_lifecycle_policy(\n        self,\n        transition_days: int = 90,\n        expiration_days: int = 365\n    ) -> bool:\n        \"\"\"\n        Set lifecycle policy for automatic archival and deletion\n\n        Args:\n            transition_days: Days until transition to cheaper storage\n            expiration_days: Days until deletion\n\n        Returns:\n            Success boolean\n        \"\"\"\n        try:\n            lifecycle_policy = {\n                \"Rules\": [\n                    {\n                        \"Id\": \"maestro-ml-lifecycle\",\n                        \"Status\": \"Enabled\",\n                        \"Filter\": {\"Prefix\": \"\"},\n                        \"Transitions\": [\n                            {\n                                \"Days\": transition_days,\n                                \"StorageClass\": \"INTELLIGENT_TIERING\"\n                            }\n                        ],\n                        \"Expiration\": {\n                            \"Days\": expiration_days\n                        }\n                    }\n                ]\n            }\n\n            self.s3_client.put_bucket_lifecycle_configuration(\n                Bucket=self.bucket_name,\n                LifecycleConfiguration=lifecycle_policy\n            )\n\n            logger.info(f\"Set lifecycle policy on: {self.bucket_name}\")\n            return True\n\n        except ClientError as e:\n            logger.error(f\"Set lifecycle policy failed: {e}\")\n            return False\n",
    "README.md": "# MinIO S3-Compatible Storage\n\nS3-compatible object storage with MinIO or AWS S3.\n\n```python\nfrom s3_connector import S3Connector\n\nconnector = S3Connector(\n    bucket_name='ml-artifacts',\n    endpoint_url='http://minio:9000'\n)\n\nconnector.upload_file('model.pkl', 's3://bucket/model.pkl')\n```\n"
  },
  "dependencies": {
    "python": ">=3.8",
    "runtime": [
      "boto3>=1.28.0"
    ],
    "dev": []
  },
  "metadata": {
    "created_at": "2025-10-13T00:00:00Z",
    "curated": true,
    "production_ready": true
  }
}