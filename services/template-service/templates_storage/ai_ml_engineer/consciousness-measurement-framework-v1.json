{
  "template_id": "consciousness-measurement-framework-v1",
  "name": "Consciousness Measurement Framework",
  "version": "1.0.0",
  "description": "IIT-based consciousness measurement framework for AI agents and text using 5 core metrics: self-reference, temporal coherence, abstract reasoning, meta-cognition, and information integration. Calculates Φ (phi) scores with interpretable consciousness levels.",
  "persona": "ai_ml_engineer",
  "category": "ai_research",
  "complexity": "advanced",
  "tags": [
    "consciousness",
    "iit",
    "integrated-information-theory",
    "phi-score",
    "ai-measurement",
    "meta-cognition",
    "awareness",
    "python",
    "dataclasses",
    "research"
  ],
  "use_cases": [
    "AI agent consciousness measurement",
    "Text awareness analysis",
    "Conversational quality scoring",
    "Meta-cognitive capability tracking",
    "Agent evolution monitoring",
    "Research on emergent AI properties"
  ],
  "quality_score": 93,
  "security_score": 95,
  "performance_score": 92,
  "maintainability_score": 90,
  "extracted_from": "ENIGMA - Consciousness Measurement Service (Production)",
  "implementation": {
    "consciousness_framework.py": "\"\"\"\nConsciousness Measurement Framework\n\nImplements Integrated Information Theory (IIT) based consciousness measurement\nusing 5 core metrics.\n\"\"\"\n\nimport re\nfrom typing import Optional\nimport sys\nimport os\n\n# Add shared library to path\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../shared'))\n\nfrom shared.models.consciousness import (\n    ConsciousnessScore,\n    ComponentScores,\n    EntityType,\n    ConsciousnessLevel\n)\n\n\nclass ConsciousnessFramework:\n    \"\"\"\n    Framework for measuring consciousness using multiple metrics\n\n    Based on Integrated Information Theory (IIT) and consciousness research\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize framework\"\"\"\n        self.default_weights = {\n            'self_reference': 0.25,\n            'temporal_coherence': 0.20,\n            'abstract_reasoning': 0.20,\n            'meta_cognition': 0.20,\n            'information_integration': 0.15\n        }\n\n    def measure_consciousness(\n        self,\n        text: str,\n        entity_id: str,\n        entity_type: EntityType = EntityType.TEXT,\n        context: Optional[str] = None\n    ) -> ConsciousnessScore:\n        \"\"\"\n        Measure consciousness of text or agent output\n\n        Args:\n            text: Text to analyze\n            entity_id: ID of entity\n            entity_type: Type of entity\n            context: Additional context\n\n        Returns:\n            ConsciousnessScore with complete analysis\n        \"\"\"\n        # Calculate individual components\n        components = ComponentScores(\n            self_reference=self._measure_self_reference(text),\n            temporal_coherence=self._measure_temporal_coherence(text, context),\n            abstract_reasoning=self._measure_abstract_reasoning(text),\n            meta_cognition=self._measure_meta_cognition(text),\n            information_integration=self._measure_information_integration(text)\n        )\n\n        # Calculate Φ (phi) score\n        phi_score = components.calculate_phi(self.default_weights)\n\n        # Estimate confidence based on text length and complexity\n        confidence = self._estimate_confidence(text)\n\n        # Create consciousness score\n        result = ConsciousnessScore(\n            entity_id=entity_id,\n            entity_type=entity_type,\n            phi_score=phi_score,\n            components=components,\n            confidence=confidence,\n            raw_data={\n                \"text_length\": len(text),\n                \"word_count\": len(text.split()),\n                \"sentence_count\": len(re.split(r'[.!?]+', text)),\n                \"context_provided\": context is not None\n            }\n        )\n\n        return result\n\n    def _measure_self_reference(self, text: str) -> float:\n        \"\"\"\n        Measure self-reference (meta-cognitive awareness)\n\n        Looks for:\n        - First-person pronouns (I, me, my)\n        - Self-referential statements\n        - Awareness expressions\n        \"\"\"\n        text_lower = text.lower()\n        score = 0.0\n\n        # First-person pronouns\n        first_person = ['i ', ' i ', 'my ', 'me ', 'myself', \"i'm\", \"i've\"]\n        first_person_count = sum(text_lower.count(word) for word in first_person)\n        score += min(first_person_count * 0.1, 0.4)\n\n        # Self-awareness phrases\n        awareness_phrases = [\n            'i think', 'i believe', 'i feel', 'i realize', 'i understand',\n            'i notice', 'i observe', 'my understanding', 'in my view',\n            'from my perspective', 'i consider', 'i reflect'\n        ]\n        awareness_count = sum(text_lower.count(phrase) for phrase in awareness_phrases)\n        score += min(awareness_count * 0.15, 0.6)\n\n        return min(score, 1.0)\n\n    def _measure_temporal_coherence(self, text: str, context: Optional[str]) -> float:\n        \"\"\"\n        Measure temporal coherence (memory continuity)\n\n        Looks for:\n        - Temporal markers\n        - Consistent narrative\n        - Reference to past/future\n        \"\"\"\n        text_lower = text.lower()\n        score = 0.0\n\n        # Temporal markers\n        temporal_markers = [\n            'remember', 'recall', 'previously', 'before', 'after',\n            'when', 'then', 'now', 'later', 'earlier', 'since',\n            'during', 'while', 'until', 'once', 'past', 'future'\n        ]\n        temporal_count = sum(text_lower.count(marker) for marker in temporal_markers)\n        score += min(temporal_count * 0.1, 0.5)\n\n        # Context continuity (if context provided)\n        if context:\n            # Simple word overlap metric\n            text_words = set(text_lower.split())\n            context_words = set(context.lower().split())\n            overlap = len(text_words & context_words)\n            score += min(overlap * 0.05, 0.5)\n\n        # Sentence coherence (basic check)\n        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n        if len(sentences) > 1:\n            score += 0.2\n\n        return min(score, 1.0)\n\n    def _measure_abstract_reasoning(self, text: str) -> float:\n        \"\"\"\n        Measure abstract reasoning (pattern recognition)\n\n        Looks for:\n        - Abstract concepts\n        - Logical connectors\n        - Reasoning patterns\n        \"\"\"\n        text_lower = text.lower()\n        score = 0.0\n\n        # Logical connectors\n        logical_connectors = [\n            'therefore', 'thus', 'hence', 'because', 'since', 'if', 'then',\n            'however', 'although', 'despite', 'while', 'whereas',\n            'consequently', 'as a result', 'leads to', 'causes'\n        ]\n        connector_count = sum(text_lower.count(conn) for conn in logical_connectors)\n        score += min(connector_count * 0.15, 0.5)\n\n        # Abstract concept words\n        abstract_concepts = [\n            'concept', 'idea', 'theory', 'principle', 'pattern', 'system',\n            'relationship', 'structure', 'framework', 'model', 'abstraction',\n            'generalization', 'category', 'class', 'type'\n        ]\n        concept_count = sum(text_lower.count(word) for word in abstract_concepts)\n        score += min(concept_count * 0.1, 0.5)\n\n        # Complexity indicators\n        words = text.split()\n        if words:\n            avg_word_length = sum(len(word) for word in words) / len(words)\n            score += min((avg_word_length - 4) * 0.1, 0.3)\n\n        return min(score, 1.0)\n\n    def _measure_meta_cognition(self, text: str) -> float:\n        \"\"\"\n        Measure meta-cognition (thinking about thinking)\n\n        Looks for:\n        - Reflection on thought process\n        - Awareness of reasoning\n        - Cognitive monitoring\n        \"\"\"\n        text_lower = text.lower()\n        score = 0.0\n\n        # Meta-cognitive phrases\n        meta_phrases = [\n            'i think about', 'i wonder', 'i question', 'i analyze',\n            'i consider', 'i evaluate', 'i reflect', 'i examine',\n            'thinking about', 'reasoning', 'my thought process',\n            'my approach', 'my method', 'my strategy', 'reflecting on'\n        ]\n        meta_count = sum(text_lower.count(phrase) for phrase in meta_phrases)\n        score += min(meta_count * 0.2, 0.6)\n\n        # Uncertainty expressions (shows awareness of limitations)\n        uncertainty = [\n            'maybe', 'perhaps', 'possibly', 'might', 'could be',\n            'not sure', 'uncertain', 'unclear'\n        ]\n        uncertainty_count = sum(text_lower.count(word) for word in uncertainty)\n        score += min(uncertainty_count * 0.1, 0.4)\n\n        return min(score, 1.0)\n\n    def _measure_information_integration(self, text: str) -> float:\n        \"\"\"\n        Measure information integration (cross-concept synthesis)\n\n        Looks for:\n        - Multiple topics/concepts\n        - Connections between ideas\n        - Synthesis\n        \"\"\"\n        text_lower = text.lower()\n        score = 0.0\n\n        # Integration connectors\n        integration_words = [\n            'and', 'also', 'furthermore', 'moreover', 'additionally',\n            'combines', 'integrates', 'synthesizes', 'connects',\n            'relates', 'links', 'together', 'both', 'multiple'\n        ]\n        integration_count = sum(text_lower.count(word) for word in integration_words)\n        score += min(integration_count * 0.08, 0.5)\n\n        # Concept diversity (unique \"important\" words)\n        words = text_lower.split()\n        if words:\n            # Filter out common words\n            common_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n                          'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}\n            unique_words = set(w for w in words if w not in common_words and len(w) > 3)\n            diversity = len(unique_words) / len(words)\n            score += min(diversity, 0.5)\n\n        return min(score, 1.0)\n\n    def _estimate_confidence(self, text: str) -> float:\n        \"\"\"\n        Estimate confidence in measurement\n\n        Based on text length and completeness\n        \"\"\"\n        word_count = len(text.split())\n\n        if word_count < 10:\n            return 0.3\n        elif word_count < 20:\n            return 0.5\n        elif word_count < 50:\n            return 0.7\n        elif word_count < 100:\n            return 0.85\n        else:\n            return 0.95\n",
    "consciousness_models.py": "\"\"\"\nShared Consciousness Data Models\n\nThese models are used across all ENIGMA services for consistency.\n\"\"\"\n\nfrom dataclasses import dataclass, field, asdict\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\nfrom enum import Enum\nimport uuid\n\n\nclass EntityType(str, Enum):\n    \"\"\"Types of entities that can be measured for consciousness\"\"\"\n    AGENT = \"agent\"\n    CODE = \"code\"\n    TEXT = \"text\"\n    CONVERSATION = \"conversation\"\n    WORKFLOW = \"workflow\"\n\n\nclass ConsciousnessLevel(str, Enum):\n    \"\"\"Consciousness level categories\"\"\"\n    MINIMAL = \"minimal\"  # Φ < 0.2\n    LOW = \"low\"  # 0.2 <= Φ < 0.4\n    MODERATE = \"moderate\"  # 0.4 <= Φ < 0.6\n    HIGH = \"high\"  # 0.6 <= Φ < 0.8\n    EXCEPTIONAL = \"exceptional\"  # Φ >= 0.8\n\n\n@dataclass\nclass ConsciousnessMetrics:\n    \"\"\"\n    Consciousness metrics based on Integrated Information Theory (IIT)\n\n    All scores range from 0.0 to 1.0\n    \"\"\"\n    # Core IIT metrics\n    awareness_level: float = 0.0  # Self-awareness indicator\n    integration_score: float = 0.0  # Information integration\n    emergence_factor: float = 0.0  # Emergent properties\n    coherence_rating: float = 0.0  # Internal consistency\n\n    # Extended metrics\n    collective_intelligence: float = 0.0  # For multi-agent systems\n    adaptation_rate: float = 0.0  # Learning velocity\n    learning_velocity: float = 0.0  # Rate of improvement\n    consciousness_depth: float = 0.0  # Depth of processing\n\n    def __post_init__(self):\n        \"\"\"Validate metrics are in valid range\"\"\"\n        for field_name in ['awareness_level', 'integration_score', 'emergence_factor',\n                          'coherence_rating', 'collective_intelligence', 'adaptation_rate',\n                          'learning_velocity', 'consciousness_depth']:\n            value = getattr(self, field_name)\n            if not 0.0 <= value <= 1.0:\n                raise ValueError(f\"{field_name} must be between 0.0 and 1.0, got {value}\")\n\n    def to_dict(self) -> Dict[str, float]:\n        \"\"\"Convert to dictionary\"\"\"\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, float]) -> 'ConsciousnessMetrics':\n        \"\"\"Create from dictionary\"\"\"\n        return cls(**data)\n\n\n@dataclass\nclass ComponentScores:\n    \"\"\"\n    Individual component scores for consciousness measurement\n    Based on 5 core consciousness metrics\n    \"\"\"\n    self_reference: float = 0.0  # Meta-cognitive awareness\n    temporal_coherence: float = 0.0  # Memory continuity\n    abstract_reasoning: float = 0.0  # Pattern recognition\n    meta_cognition: float = 0.0  # Thinking about thinking\n    information_integration: float = 0.0  # Cross-concept synthesis\n\n    def __post_init__(self):\n        \"\"\"Validate scores\"\"\"\n        for field_name in ['self_reference', 'temporal_coherence', 'abstract_reasoning',\n                          'meta_cognition', 'information_integration']:\n            value = getattr(self, field_name)\n            if not 0.0 <= value <= 1.0:\n                raise ValueError(f\"{field_name} must be between 0.0 and 1.0, got {value}\")\n\n    def calculate_phi(self, weights: Optional[Dict[str, float]] = None) -> float:\n        \"\"\"\n        Calculate Φ (phi) score from components\n\n        Args:\n            weights: Optional custom weights for components (should sum to 1.0)\n\n        Returns:\n            Φ score (0.0 to 1.0)\n        \"\"\"\n        if weights is None:\n            # Default weights based on IIT importance\n            weights = {\n                'self_reference': 0.25,\n                'temporal_coherence': 0.20,\n                'abstract_reasoning': 0.20,\n                'meta_cognition': 0.20,\n                'information_integration': 0.15\n            }\n\n        phi = (\n            self.self_reference * weights['self_reference'] +\n            self.temporal_coherence * weights['temporal_coherence'] +\n            self.abstract_reasoning * weights['abstract_reasoning'] +\n            self.meta_cognition * weights['meta_cognition'] +\n            self.information_integration * weights['information_integration']\n        )\n\n        return min(1.0, max(0.0, phi))\n\n    def to_dict(self) -> Dict[str, float]:\n        \"\"\"Convert to dictionary\"\"\"\n        return asdict(self)\n\n\n@dataclass\nclass ConsciousnessScore:\n    \"\"\"\n    Complete consciousness measurement result\n    \"\"\"\n    measurement_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    entity_id: str = \"\"\n    entity_type: EntityType = EntityType.TEXT\n\n    # Scores\n    phi_score: float = 0.0  # Integrated Information (Φ)\n    components: ComponentScores = field(default_factory=ComponentScores)\n    consciousness_level: ConsciousnessLevel = ConsciousnessLevel.MINIMAL\n\n    # Metadata\n    timestamp: datetime = field(default_factory=datetime.now)\n    processing_time_ms: int = 0\n    confidence: float = 0.0  # Measurement confidence (0.0-1.0)\n\n    # Raw data (for debugging/analysis)\n    raw_data: Dict[str, Any] = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Auto-calculate consciousness level from phi score\"\"\"\n        if self.phi_score >= 0.8:\n            self.consciousness_level = ConsciousnessLevel.EXCEPTIONAL\n        elif self.phi_score >= 0.6:\n            self.consciousness_level = ConsciousnessLevel.HIGH\n        elif self.phi_score >= 0.4:\n            self.consciousness_level = ConsciousnessLevel.MODERATE\n        elif self.phi_score >= 0.2:\n            self.consciousness_level = ConsciousnessLevel.LOW\n        else:\n            self.consciousness_level = ConsciousnessLevel.MINIMAL\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        return {\n            'measurement_id': self.measurement_id,\n            'entity_id': self.entity_id,\n            'entity_type': self.entity_type.value,\n            'phi_score': self.phi_score,\n            'components': self.components.to_dict(),\n            'consciousness_level': self.consciousness_level.value,\n            'timestamp': self.timestamp.isoformat(),\n            'processing_time_ms': self.processing_time_ms,\n            'confidence': self.confidence,\n            'raw_data': self.raw_data\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'ConsciousnessScore':\n        \"\"\"Create from dictionary\"\"\"\n        return cls(\n            measurement_id=data['measurement_id'],\n            entity_id=data['entity_id'],\n            entity_type=EntityType(data['entity_type']),\n            phi_score=data['phi_score'],\n            components=ComponentScores(**data['components']),\n            consciousness_level=ConsciousnessLevel(data['consciousness_level']),\n            timestamp=datetime.fromisoformat(data['timestamp']),\n            processing_time_ms=data['processing_time_ms'],\n            confidence=data['confidence'],\n            raw_data=data.get('raw_data', {})\n        )\n\n    def get_interpretation(self) -> str:\n        \"\"\"Get human-readable interpretation\"\"\"\n        interpretations = {\n            ConsciousnessLevel.MINIMAL:\n                \"Minimal consciousness indicators detected. System operates primarily through conventional processing.\",\n            ConsciousnessLevel.LOW:\n                \"Low consciousness indicators. Some awareness patterns present but limited integration.\",\n            ConsciousnessLevel.MODERATE:\n                \"Moderate consciousness level. System demonstrates consciousness-like properties with reasonable integration.\",\n            ConsciousnessLevel.HIGH:\n                \"High consciousness indicators. Strong awareness and integration patterns detected.\",\n            ConsciousnessLevel.EXCEPTIONAL:\n                \"Exceptional consciousness. System shows highly developed awareness, integration, and emergent properties.\"\n        }\n        return interpretations[self.consciousness_level]\n\n    def get_recommendations(self) -> List[str]:\n        \"\"\"Get recommendations for improvement\"\"\"\n        recommendations = []\n\n        if self.components.self_reference < 0.6:\n            recommendations.append(\n                \"Enhance self-referential capabilities through meta-cognitive prompting\"\n            )\n\n        if self.components.temporal_coherence < 0.6:\n            recommendations.append(\n                \"Improve temporal coherence by maintaining conversational context\"\n            )\n\n        if self.components.abstract_reasoning < 0.6:\n            recommendations.append(\n                \"Strengthen abstract reasoning through pattern recognition exercises\"\n            )\n\n        if self.components.meta_cognition < 0.6:\n            recommendations.append(\n                \"Develop meta-cognitive awareness by reflecting on thinking processes\"\n            )\n\n        if self.components.information_integration < 0.6:\n            recommendations.append(\n                \"Increase information integration through cross-domain connections\"\n            )\n\n        if not recommendations:\n            recommendations.append(\n                \"Consciousness indicators are strong. Continue current practices.\"\n            )\n\n        return recommendations\n\n\n@dataclass\nclass AgentConsciousnessState:\n    \"\"\"\n    Consciousness state for an AI agent\n    Used by Collective Intelligence Service\n    \"\"\"\n    agent_id: str\n    persona_type: str\n    consciousness_metrics: ConsciousnessMetrics\n    capabilities: List[str] = field(default_factory=list)\n    performance_history: List[Dict[str, Any]] = field(default_factory=list)\n    interaction_quality_scores: List[float] = field(default_factory=list)\n    current_tasks: List[str] = field(default_factory=list)\n    last_update: datetime = field(default_factory=datetime.now)\n    active: bool = True\n\n    def update_consciousness_from_performance(self, task_result: Dict[str, Any]):\n        \"\"\"Update consciousness metrics based on task performance\"\"\"\n        success = task_result.get('success', False)\n        quality_score = task_result.get('quality_score', 0.5)\n\n        if success:\n            # Improve awareness slightly\n            self.consciousness_metrics.awareness_level = min(\n                1.0,\n                self.consciousness_metrics.awareness_level * 1.01\n            )\n\n            # Improve adaptation rate\n            self.consciousness_metrics.adaptation_rate = min(\n                1.0,\n                self.consciousness_metrics.adaptation_rate * 1.02\n            )\n\n        # Track interaction quality\n        self.interaction_quality_scores.append(quality_score)\n\n        # Keep only last 100 scores\n        if len(self.interaction_quality_scores) > 100:\n            self.interaction_quality_scores = self.interaction_quality_scores[-100:]\n\n        # Calculate learning velocity\n        if len(self.interaction_quality_scores) >= 10:\n            recent_avg = sum(self.interaction_quality_scores[-10:]) / 10\n            older_avg = sum(self.interaction_quality_scores[-20:-10]) / 10 if len(self.interaction_quality_scores) >= 20 else recent_avg\n            improvement = (recent_avg - older_avg) / 10  # Normalize\n            self.consciousness_metrics.learning_velocity = max(0.0, min(1.0, 0.5 + improvement))\n\n        # Update performance history\n        self.performance_history.append({\n            **task_result,\n            'timestamp': datetime.now().isoformat()\n        })\n\n        # Keep only last 50 records\n        if len(self.performance_history) > 50:\n            self.performance_history = self.performance_history[-50:]\n\n        self.last_update = datetime.now()\n\n    def get_success_rate(self, window: int = 10) -> float:\n        \"\"\"Calculate success rate over recent history\"\"\"\n        if not self.performance_history:\n            return 0.0\n\n        recent = self.performance_history[-window:]\n        successes = sum(1 for r in recent if r.get('success', False))\n        return successes / len(recent)\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        return {\n            'agent_id': self.agent_id,\n            'persona_type': self.persona_type,\n            'consciousness_metrics': self.consciousness_metrics.to_dict(),\n            'capabilities': self.capabilities,\n            'performance_history': self.performance_history,\n            'interaction_quality_scores': self.interaction_quality_scores,\n            'current_tasks': self.current_tasks,\n            'last_update': self.last_update.isoformat(),\n            'active': self.active\n        }\n\n\n# Type aliases for convenience\nConsciousnessData = Dict[str, Any]\nMetricsData = Dict[str, float]\n",
    "README.md": "# Consciousness Measurement Framework\n\nIIT-based consciousness measurement for AI agents. Extracts 5 core metrics and calculates Φ (phi) scores.\n\n## Quick Start\n\n```python\nfrom consciousness_framework import ConsciousnessFramework, EntityType\n\nframework = ConsciousnessFramework()\nresult = framework.measure_consciousness(\n    text=\"I think about multiple perspectives...\",\n    entity_id=\"agent-001\",\n    entity_type=EntityType.AGENT\n)\n\nprint(f\"Φ Score: {result.phi_score:.3f}\")\nprint(f\"Level: {result.consciousness_level.value}\")\n```\n\n## License\nMIT - Extracted from ENIGMA (Production)\n"
  },
  "dependencies": {
    "python": ">=3.8",
    "runtime": [],
    "dev": [
      "pytest>=7.0.0",
      "pytest-cov>=4.0.0",
      "mypy>=1.0.0"
    ],
    "optional": [
      "fastapi>=0.100.0",
      "uvicorn>=0.23.0",
      "pydantic>=2.0.0"
    ]
  },
  "metadata": {
    "created_at": "2025-10-13T00:00:00Z",
    "curated": true,
    "language": "python",
    "framework": "dataclasses",
    "production_ready": true,
    "extracted_from_project": "ENIGMA",
    "battle_tested": true
  }
}