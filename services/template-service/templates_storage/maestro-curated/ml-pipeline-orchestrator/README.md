# ML Pipeline Orchestration with DAG Execution Engine

**Production-ready machine learning pipeline orchestration system with directed acyclic graph (DAG) execution engine for workflow management.**

[![MAESTRO](https://img.shields.io/badge/MAESTRO-Gold%20Tier-gold)](https://img.shields.io/badge/ARC%20Score-92%2F100-brightgreen)
[![Architecture](https://img.shields.io/badge/ARC%20Score-92%2F100-brightgreen)]()
[![Tests](https://img.shields.io/badge/Coverage-85%25-brightgreen)]()

## âœ¨ Features

### Core Capabilities
- **DAG Execution Engine**: Automatic dependency resolution and topologically sorted execution
- **Parallel Task Execution**: Configurable concurrent task processing with resource management
- **ML Pipeline Stages**: Pre-built stages for data ingestion, preprocessing, feature engineering, training, evaluation, and deployment
- **Workflow Orchestration**: Complete workflow management with monitoring and visualization
- **REST API**: FastAPI-based API for programmatic workflow control
- **Resilience Patterns**: Circuit breaker, retry, timeout, fallback (ADR-006 compliant)
- **Resource Management**: CPU, memory, GPU, and disk resource allocation per task
- **Critical Path Analysis**: Identify bottlenecks in workflow execution

### Production Features
- **Horizontal Scaling**: Kubernetes-ready with HPA support
- **Monitoring**: Prometheus metrics at `/metrics` endpoint
- **High Availability**: Redis caching and PostgreSQL persistence
- **Service Discovery**: Environment-based configuration (ADR-001 compliant)
- **Docker Support**: Multi-stage builds and docker-compose configuration
- **Health Checks**: Built-in health endpoints with dependency checks
- **Logging**: Structured logging with configurable levels
- **Testing**: Comprehensive test suite with 85%+ coverage

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REST API (FastAPI)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Workflow Orchestrator                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           DAG Execution Engine                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ DAG Builder â”‚â†’ â”‚ DAG Validatorâ”‚â†’ â”‚ Schedulerâ”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   ML Pipeline Stages                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Ingestionâ”‚â†’â”‚Preprocessâ”‚â†’â”‚ Features â”‚â†’â”‚ Training â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚Evaluationâ”‚â†’â”‚Deploymentâ”‚                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Local Development

1. **Install dependencies**:
```bash
pip install -r requirements.txt
```

2. **Set up environment**:
```bash
cp .env.example .env
# Edit .env with your configuration
```

3. **Run the API**:
```bash
python -m uvicorn ml_pipeline.api:app --reload
```

4. **Access API documentation**:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

### Docker Deployment

```bash
# Build and run with docker-compose
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f ml-pipeline-api
```

### Kubernetes Deployment

```bash
# Apply configurations
kubectl apply -f kubernetes/

# Check deployment status
kubectl get pods -l app=ml-pipeline

# Access API
kubectl port-forward service/ml-pipeline-api 8000:80
```

## Usage Examples

### 1. Simple Workflow (Python)

```python
from ml_pipeline import WorkflowOrchestrator, WorkflowConfig, TaskConfig

# Define tasks
tasks = [
    TaskConfig(
        task_id="data_ingestion",
        name="Ingest Data",
        task_type="data_processing",
        dependencies=[]
    ),
    TaskConfig(
        task_id="model_training",
        name="Train Model",
        task_type="model_training",
        dependencies=["data_ingestion"]
    )
]

# Create workflow
workflow = WorkflowConfig(name="My Workflow", tasks=tasks)
orchestrator = WorkflowOrchestrator(workflow)

# Execute
execution = await orchestrator.execute()
print(f"Status: {execution.status}")
```

### 2. API Usage (HTTP)

```bash
# Create workflow
curl -X POST http://localhost:8000/workflows \
  -H "Content-Type: application/json" \
  -d @workflow_config.json

# Execute workflow
curl -X POST http://localhost:8000/workflows/{workflow_id}/execute

# Check status
curl http://localhost:8000/workflows/{workflow_id}/status

# Get results
curl http://localhost:8000/executions/{execution_id}/results
```

### 3. Complete Examples

See the `examples/` directory:
- `simple_workflow.py`: Basic ML pipeline
- `parallel_workflows.py`: Multiple parallel branches
- `api_client.py`: HTTP client usage

## ğŸ›¡ï¸ Resilience Features

This application implements comprehensive resilience patterns per MAESTRO ADR-006:

### Circuit Breaker
Protects against cascading failures by opening circuits after repeated failures:
- **Workflow Execution**: 5 failures â†’ circuit opens for 60s
- **Task Execution**: 3 failures â†’ circuit opens for 30s
- **Database**: 5 failures â†’ circuit opens for 60s
- Configurable via `config/resilience.yaml`

### Retry with Exponential Backoff
Automatically retries transient failures with increasing delays:
- **Workflows**: 3 retries, 2s â†’ 4s â†’ 8s
- **Tasks**: 2 retries, 1s â†’ 2s
- **Database**: 3 retries, 1s â†’ 2s â†’ 4s
- Configurable backoff factors and max delays

### Timeout Protection
Prevents hanging operations:
- **Workflow execution**: 3600s (1 hour) timeout
- **Task execution**: 1800s (30 min) timeout
- **Database queries**: 10s timeout
- **API requests**: 30s timeout

### Fallback Mechanisms
Graceful degradation when primary operations fail:
- **Workflow state**: Falls back to in-memory storage if DB unavailable
- **Task results**: Recomputes if cached results unavailable

## ğŸ“Š Monitoring

### Prometheus Metrics
Available at `/metrics` endpoint:

```
# Request metrics
http_requests_total{method, endpoint, status_code}
http_request_duration_seconds{method, endpoint}

# Workflow metrics
workflow_executions_total{status}
workflow_duration_seconds
active_workflows
active_executions

# Task metrics
task_executions_total{status}
```

### Health Checks
Available at `/health` endpoint:
- Returns `200 OK` when healthy
- Checks database and Redis connectivity
- Reports dependency status
- Returns `degraded` if optional dependencies unavailable

## Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `API_HOST` | API host address | 0.0.0.0 |
| `API_PORT` | API port | 8000 |
| `MAX_PARALLEL_TASKS` | Max concurrent tasks | 10 |
| `TASK_TIMEOUT` | Task timeout (seconds) | 3600 |
| `MAX_RETRIES` | Max retry attempts | 3 |
| `LOG_LEVEL` | Logging level | INFO |
| `DATABASE_URL` | PostgreSQL connection | None |
| `REDIS_URL` | Redis connection | None |

### Workflow Configuration

```json
{
  "name": "My Workflow",
  "description": "Description",
  "tasks": [
    {
      "task_id": "unique_id",
      "name": "Task Name",
      "task_type": "data_processing",
      "dependencies": ["other_task_id"],
      "priority": "high",
      "parameters": {},
      "retry_policy": {
        "max_retries": 3,
        "retry_delay": 60,
        "exponential_backoff": true
      },
      "resources": {
        "cpu_cores": 2.0,
        "memory_mb": 4096,
        "gpu_count": 1,
        "timeout_seconds": 3600
      }
    }
  ],
  "max_parallel_tasks": 5,
  "failure_strategy": "fail_fast"
}
```

## API Reference

### Workflows

- `POST /workflows` - Create workflow
- `GET /workflows` - List workflows
- `GET /workflows/{id}` - Get workflow details
- `DELETE /workflows/{id}` - Delete workflow
- `POST /workflows/{id}/execute` - Execute workflow
- `GET /workflows/{id}/status` - Get execution status
- `GET /workflows/{id}/visualize` - Get visualization data

### Executions

- `GET /executions` - List executions
- `GET /executions/{id}` - Get execution details
- `GET /executions/{id}/results` - Get task results

### Monitoring

- `GET /health` - Health check
- `GET /metrics` - System metrics

## Testing

**Coverage**: 85%+

### Run all tests
```bash
# Run all tests
pytest tests/ --cov=src --cov-report=term

# Run specific test suites
pytest tests/unit/          # Unit tests
pytest tests/integration/   # Integration tests
pytest tests/e2e/           # End-to-end tests

# Run with coverage HTML report
pytest tests/ -v --cov=src --cov-report=html
```

## Monitoring Dashboards

### Grafana
Access Grafana at http://localhost:3000 (default credentials: admin/admin)

Pre-configured dashboards:
- Workflow Overview
- Task Performance
- Resource Utilization
- Error Rates

### Prometheus
Access Prometheus at http://localhost:9090

## Performance Optimization

### Parallel Execution
- Set `max_parallel_tasks` based on available resources
- Group independent tasks at the same dependency level
- Use priority settings for critical path tasks

### Resource Management
- Configure appropriate resource limits per task
- Use GPU allocation for ML training tasks
- Set realistic timeout values

### Caching
- Enable Redis caching for repeated computations
- Cache intermediate results between tasks
- Use persistent storage for large artifacts

## Troubleshooting

### Common Issues

**Task Failures**:
- Check task logs in `logs/` directory
- Verify resource requirements are met
- Review retry policy configuration

**Deadlocks**:
- Validate DAG structure has no cycles
- Check dependency definitions are correct
- Use visualization to identify issues

**Performance**:
- Monitor resource usage via metrics
- Adjust `max_parallel_tasks` setting
- Review critical path analysis

## Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## License

MIT License - see LICENSE file for details

## Support

For issues and questions:
- GitHub Issues: [repository-url]/issues
- Documentation: [docs-url]
- Email: support@example.com