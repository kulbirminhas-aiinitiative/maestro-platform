"""
EU AI Act Article 11 - AI Risk Assessment Documentation.

EPIC: MD-2159 - [Compliance] Technical Documentation
AC-5: Create AI risk assessment document

This module provides tools for conducting and documenting
AI risk assessments per EU AI Act requirements.

AI DISCLOSURE: This code was generated by AI (Claude) for EU AI Act compliance purposes.
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Any
import json


class RiskCategory(Enum):
    """EU AI Act risk categories."""
    UNACCEPTABLE = "unacceptable"  # Banned
    HIGH = "high"                   # Requires conformity assessment
    LIMITED = "limited"             # Transparency obligations
    MINIMAL = "minimal"             # Voluntary codes of conduct


class RiskDomain(Enum):
    """Domains of potential risk."""
    SAFETY = "safety"
    FUNDAMENTAL_RIGHTS = "fundamental_rights"
    PRIVACY = "privacy"
    DISCRIMINATION = "discrimination"
    MANIPULATION = "manipulation"
    TRANSPARENCY = "transparency"
    ACCOUNTABILITY = "accountability"
    ENVIRONMENTAL = "environmental"


class Likelihood(Enum):
    """Likelihood of risk occurrence."""
    RARE = "rare"
    UNLIKELY = "unlikely"
    POSSIBLE = "possible"
    LIKELY = "likely"
    ALMOST_CERTAIN = "almost_certain"


class Impact(Enum):
    """Impact severity of risk."""
    NEGLIGIBLE = "negligible"
    MINOR = "minor"
    MODERATE = "moderate"
    MAJOR = "major"
    CATASTROPHIC = "catastrophic"


@dataclass
class RiskIdentification:
    """Identified risk in the AI system."""
    id: str
    name: str
    description: str
    domain: RiskDomain
    affected_stakeholders: List[str]
    ai_component: str
    data_involved: List[str]
    identified_by: str
    identified_date: datetime


@dataclass
class RiskAnalysis:
    """Analysis of an identified risk."""
    risk_id: str
    likelihood: Likelihood
    impact: Impact
    inherent_risk_score: float
    contributing_factors: List[str]
    existing_controls: List[str]
    residual_risk_score: float
    analysis_date: datetime
    analyst: str


@dataclass
class RiskMitigation:
    """Mitigation measures for a risk."""
    risk_id: str
    measure: str
    description: str
    implementation_status: str
    responsible_party: str
    target_date: datetime
    effectiveness_rating: str
    verification_method: str


@dataclass
class RiskAssessment:
    """Complete risk assessment document."""
    id: str
    name: str
    version: str
    system_name: str
    assessment_scope: str
    risk_category: RiskCategory
    identifications: List[RiskIdentification]
    analyses: List[RiskAnalysis]
    mitigations: List[RiskMitigation]
    overall_risk_rating: str
    compliance_statement: str
    assessor: str
    reviewer: str
    created_at: datetime
    updated_at: datetime
    next_review_date: datetime


class AIRiskAssessmentSystem:
    """
    System for conducting AI risk assessments per EU AI Act.

    EU AI Act Article 9 Requirements:
    - Risk management throughout AI system lifecycle
    - Identification and analysis of known and foreseeable risks
    - Estimation and evaluation of risks
    - Risk mitigation measures
    """

    def __init__(self, system_name: str):
        self.system_name = system_name
        self.risks: Dict[str, RiskIdentification] = {}
        self.analyses: Dict[str, RiskAnalysis] = {}
        self.mitigations: Dict[str, List[RiskMitigation]] = {}
        self.overall_category: RiskCategory = RiskCategory.MINIMAL
        self.version = "1.0.0"
        self.created_at = datetime.utcnow()

    def identify_risk(self, risk: RiskIdentification) -> bool:
        """Add an identified risk to the assessment."""
        if risk.id in self.risks:
            return False
        self.risks[risk.id] = risk
        return True

    def get_risk(self, risk_id: str) -> Optional[RiskIdentification]:
        """Retrieve a risk by ID."""
        return self.risks.get(risk_id)

    def analyze_risk(self, analysis: RiskAnalysis) -> bool:
        """Add a risk analysis."""
        if analysis.risk_id not in self.risks:
            return False
        self.analyses[analysis.risk_id] = analysis
        return True

    def add_mitigation(self, mitigation: RiskMitigation) -> bool:
        """Add a mitigation measure for a risk."""
        if mitigation.risk_id not in self.risks:
            return False
        if mitigation.risk_id not in self.mitigations:
            self.mitigations[mitigation.risk_id] = []
        self.mitigations[mitigation.risk_id].append(mitigation)
        return True

    def list_risks(self, domain: Optional[RiskDomain] = None) -> List[RiskIdentification]:
        """List all risks, optionally filtered by domain."""
        risks = list(self.risks.values())
        if domain:
            risks = [r for r in risks if r.domain == domain]
        return risks

    def get_high_impact_risks(self) -> List[RiskIdentification]:
        """Get risks with major or catastrophic impact."""
        high_impact_ids = [
            rid for rid, analysis in self.analyses.items()
            if analysis.impact in [Impact.MAJOR, Impact.CATASTROPHIC]
        ]
        return [r for r in self.risks.values() if r.id in high_impact_ids]

    def get_unmitigated_risks(self) -> List[RiskIdentification]:
        """Get risks without mitigation measures."""
        mitigated_ids = set(self.mitigations.keys())
        return [r for r in self.risks.values() if r.id not in mitigated_ids]

    def calculate_risk_score(self, likelihood: Likelihood, impact: Impact) -> float:
        """Calculate risk score based on likelihood and impact."""
        likelihood_scores = {
            Likelihood.RARE: 1,
            Likelihood.UNLIKELY: 2,
            Likelihood.POSSIBLE: 3,
            Likelihood.LIKELY: 4,
            Likelihood.ALMOST_CERTAIN: 5
        }
        impact_scores = {
            Impact.NEGLIGIBLE: 1,
            Impact.MINOR: 2,
            Impact.MODERATE: 3,
            Impact.MAJOR: 4,
            Impact.CATASTROPHIC: 5
        }
        return likelihood_scores[likelihood] * impact_scores[impact]

    def determine_risk_category(self) -> RiskCategory:
        """Determine overall risk category based on analyses."""
        if not self.analyses:
            return RiskCategory.MINIMAL

        max_score = 0
        for analysis in self.analyses.values():
            score = self.calculate_risk_score(analysis.likelihood, analysis.impact)
            max_score = max(max_score, score)

        # Categorization thresholds
        if max_score >= 20:
            return RiskCategory.UNACCEPTABLE
        elif max_score >= 12:
            return RiskCategory.HIGH
        elif max_score >= 6:
            return RiskCategory.LIMITED
        else:
            return RiskCategory.MINIMAL

    def validate_assessment_completeness(self) -> Dict[str, Any]:
        """Validate the assessment meets EU AI Act requirements."""
        issues = []

        # All risks must have analysis
        unanalyzed = [r.id for r in self.risks.values() if r.id not in self.analyses]
        if unanalyzed:
            issues.append(f"Risks without analysis: {unanalyzed}")

        # High-impact risks must have mitigations
        high_impact = self.get_high_impact_risks()
        for risk in high_impact:
            if risk.id not in self.mitigations or len(self.mitigations[risk.id]) == 0:
                issues.append(f"High-impact risk {risk.id} lacks mitigation measures")

        # Check all risk domains are addressed
        addressed_domains = {r.domain for r in self.risks.values()}
        critical_domains = {RiskDomain.SAFETY, RiskDomain.FUNDAMENTAL_RIGHTS, RiskDomain.PRIVACY}
        missing_domains = critical_domains - addressed_domains
        if missing_domains:
            issues.append(f"Critical domains not assessed: {[d.value for d in missing_domains]}")

        return {
            "valid": len(issues) == 0,
            "total_risks": len(self.risks),
            "analyzed_risks": len(self.analyses),
            "mitigated_risks": len(self.mitigations),
            "issues": issues
        }

    def generate_risk_matrix(self) -> Dict[str, List[str]]:
        """Generate a risk matrix showing likelihood vs impact."""
        matrix = {}
        for analysis in self.analyses.values():
            key = f"{analysis.likelihood.value}_{analysis.impact.value}"
            if key not in matrix:
                matrix[key] = []
            matrix[key].append(analysis.risk_id)
        return matrix

    def generate_assessment(self) -> RiskAssessment:
        """Generate the complete risk assessment document."""
        validation = self.validate_assessment_completeness()
        self.overall_category = self.determine_risk_category()

        all_mitigations = []
        for mitigation_list in self.mitigations.values():
            all_mitigations.extend(mitigation_list)

        return RiskAssessment(
            id=f"ra-{self.system_name.lower().replace(' ', '-')}",
            name=f"{self.system_name} Risk Assessment",
            version=self.version,
            system_name=self.system_name,
            assessment_scope="Full AI system lifecycle",
            risk_category=self.overall_category,
            identifications=list(self.risks.values()),
            analyses=list(self.analyses.values()),
            mitigations=all_mitigations,
            overall_risk_rating=self.overall_category.value,
            compliance_statement=self._generate_compliance_statement(validation),
            assessor="system",
            reviewer="pending",
            created_at=self.created_at,
            updated_at=datetime.utcnow(),
            next_review_date=datetime.utcnow()
        )

    def _generate_compliance_statement(self, validation: Dict[str, Any]) -> str:
        """Generate compliance statement."""
        if validation["valid"]:
            return f"This AI system is classified as {self.overall_category.value} risk per EU AI Act. All required assessments have been completed."
        else:
            return f"This AI system is classified as {self.overall_category.value} risk per EU AI Act. Assessment incomplete: {len(validation['issues'])} issues found."

    def export_to_json(self) -> str:
        """Export the risk assessment to JSON."""
        assessment = self.generate_assessment()
        return json.dumps({
            "id": assessment.id,
            "name": assessment.name,
            "version": assessment.version,
            "risk_category": assessment.risk_category.value,
            "total_risks": len(assessment.identifications),
            "mitigations_count": len(assessment.mitigations),
            "overall_rating": assessment.overall_risk_rating,
            "compliance_statement": assessment.compliance_statement,
            "risk_matrix": self.generate_risk_matrix(),
            "validation": self.validate_assessment_completeness(),
            "created_at": assessment.created_at.isoformat(),
            "updated_at": assessment.updated_at.isoformat()
        }, indent=2)


def create_risk_assessment_system(system_name: str = "Maestro AI Platform") -> AIRiskAssessmentSystem:
    """Factory function to create an AIRiskAssessmentSystem."""
    return AIRiskAssessmentSystem(system_name)
