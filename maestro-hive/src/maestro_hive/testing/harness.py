#!/usr/bin/env python3
"""
Maestro TaaS Harness (Testing as a Service)
The Execution Platform for Maestro Quality Assurance.

Philosophy:
- TaaS is the RUNTIME, not the AUTHOR.
- TaaS receives a "Test Scope" (defined by DDE/ACC) and executes it.
- TaaS provides the infrastructure (Sandbox, Reporting, Metrics).

Architecture:
[DDE/ACC] -> (defines) -> [Test Scope] -> (inputs to) -> [TaaS Harness] -> (outputs) -> [Quality Verdict]
"""

import asyncio
import json
import logging
import subprocess
import time
from dataclasses import dataclass, field, asdict
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Union

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("MaestroTaaS")

class TestType(str, Enum):
    UNIT = "unit"
    INTEGRATION = "integration"
    E2E = "e2e"
    CONTRACT = "contract"
    PERFORMANCE = "performance"
    SECURITY = "security"

class TestStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"

@dataclass
class TestEnvironment:
    """Definition of the execution environment (The Jail)"""
    type: str = "local"  # "local" or "docker"
    image: Optional[str] = None
    dependencies: List[str] = field(default_factory=list)

@dataclass
class TestLifecycle:
    """Setup and Teardown hooks"""
    setup: List[str] = field(default_factory=list)
    teardown: List[str] = field(default_factory=list)

@dataclass
class TestExecution:
    """Detailed execution instructions"""
    runner: Optional[str] = None  # e.g., "pytest", "jest"
    target: Optional[str] = None  # e.g., "tests/test_core.py"
    command: Optional[str] = None # Raw command (legacy/integration)

@dataclass
class TestScenario:
    """
    A specific test scenario defined by the Upstream (DDE/ACC).
    """
    id: str
    name: str
    description: str
    test_type: TestType
    target_component: str
    
    # Schema 2.0: Explicit Execution Definition
    execution: Optional[TestExecution] = None
    
    # Legacy support (to be deprecated)
    execution_command: Optional[str] = None
    execution_params: Dict[str, Any] = field(default_factory=dict)
    
    expected_result: Optional[Dict[str, Any]] = None
    tags: List[str] = field(default_factory=list)
    timeout_seconds: int = 60

    def __post_init__(self):
        # Auto-migrate legacy command to execution object
        if self.execution_command and not self.execution:
            self.execution = TestExecution(command=self.execution_command)
        # Handle dict input for execution
        if isinstance(self.execution, dict):
            self.execution = TestExecution(**self.execution)

@dataclass
class TestScope:
    """
    The complete Test Scope defined by the Architect (DDE/ACC).
    This is the "Contract" between DDE and TaaS.
    """
    project_id: str
    scope_id: str
    scenarios: List[TestScenario]
    
    # Schema 2.0: Environment and Lifecycle
    environment: TestEnvironment = field(default_factory=TestEnvironment)
    lifecycle: TestLifecycle = field(default_factory=TestLifecycle)
    
    # Legacy config
    environment_config: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        # Handle dict inputs
        if isinstance(self.environment, dict):
            self.environment = TestEnvironment(**self.environment)
        if isinstance(self.lifecycle, dict):
            self.lifecycle = TestLifecycle(**self.lifecycle)
        # Convert scenarios dicts to objects if needed
        self.scenarios = [
            s if isinstance(s, TestScenario) else TestScenario(**s)
            for s in self.scenarios
        ]

    def to_json(self) -> str:
        return json.dumps(asdict(self), indent=2)

@dataclass
class TestResult:
    """
    The result of a single scenario execution.
    """
    scenario_id: str
    status: TestStatus
    duration_ms: float
    output_log: str
    error_message: Optional[str] = None
    artifacts: List[str] = field(default_factory=list)

@dataclass
class TaaSReport:
    """
    The final report generated by the TaaS Platform.
    """
    scope_id: str
    timestamp: str
    total_tests: int
    passed: int
    failed: int
    skipped: int
    results: List[TestResult]
    overall_status: TestStatus

class TaaSHarness:
    """
    The Execution Engine.
    """
    
    def __init__(self, workspace_root: str):
        self.workspace_root = Path(workspace_root)
        self.results: List[TestResult] = []

    async def execute_scope(self, scope: TestScope) -> TaaSReport:
        """
        Execute the provided Test Scope.
        """
        logger.info(f"üöÄ TaaS Starting Execution for Scope: {scope.scope_id}")
        logger.info(f"üìã Project: {scope.project_id} | Scenarios: {len(scope.scenarios)}")
        
        # 1. Setup Lifecycle
        if scope.lifecycle.setup:
            logger.info("üîß Running Setup Hooks...")
            for cmd in scope.lifecycle.setup:
                await self._run_hook(cmd)

        start_time = time.time()
        
        # In a real implementation, this would parallelize based on dependencies
        for scenario in scope.scenarios:
            result = await self._run_scenario(scenario, scope.environment)
            self.results.append(result)
            
        end_time = time.time()

        # 2. Teardown Lifecycle
        if scope.lifecycle.teardown:
            logger.info("üßπ Running Teardown Hooks...")
            for cmd in scope.lifecycle.teardown:
                await self._run_hook(cmd)
        
        # Calculate stats
        passed = sum(1 for r in self.results if r.status == TestStatus.PASSED)
        failed = sum(1 for r in self.results if r.status == TestStatus.FAILED)
        skipped = sum(1 for r in self.results if r.status == TestStatus.SKIPPED)
        
        overall = TestStatus.PASSED if failed == 0 else TestStatus.FAILED
        
        report = TaaSReport(
            scope_id=scope.scope_id,
            timestamp=datetime.now().isoformat(),
            total_tests=len(self.results),
            passed=passed,
            failed=failed,
            skipped=skipped,
            results=self.results,
            overall_status=overall
        )
        
        self._print_summary(report)
        return report

    async def _run_hook(self, command: str):
        """Run a lifecycle hook"""
        logger.info(f"  ‚öôÔ∏è Hook: {command}")
        # TODO: Run in sandbox
        # For now, just log
        pass

    async def _run_scenario(self, scenario: TestScenario, env: TestEnvironment) -> TestResult:
        """
        Run a single test scenario.
        """
        logger.info(f"  ‚ñ∂Ô∏è Running [{scenario.test_type.value}] {scenario.name}...")
        start = time.time()
        
        try:
            # 1. Setup Environment (Mock)
            if env.type == "docker":
                logger.info(f"    üê≥ Using Docker Image: {env.image}")
            
            # 2. Execute Command
            cmd = None
            if scenario.execution:
                if scenario.execution.runner == "pytest":
                    cmd = f"pytest {scenario.execution.target}"
                elif scenario.execution.command:
                    cmd = scenario.execution.command
            
            if cmd:
                # Security: In prod, validate command or use a safe runner
                # cmd = cmd.format(**env_config) 
                
                process = await asyncio.create_subprocess_shell(
                    cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd=str(self.workspace_root)
                )
                
                try:
                    stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=scenario.timeout_seconds)
                    output = stdout.decode() + stderr.decode()
                    
                    if process.returncode == 0:
                        status = TestStatus.PASSED
                        error = None
                    else:
                        status = TestStatus.FAILED
                        error = f"Command failed with exit code {process.returncode}"
                        
                except asyncio.TimeoutError:
                    process.kill()
                    status = TestStatus.FAILED
                    output = "Timeout exceeded"
                    error = "Timeout"
            
            else:
                # Simulation mode for demo
                await asyncio.sleep(0.5)
                status = TestStatus.PASSED
                output = "Simulated execution successful"
                error = None

            duration = (time.time() - start) * 1000
            
            logger.info(f"    {'‚úÖ' if status == TestStatus.PASSED else '‚ùå'} {status.value.upper()} ({duration:.0f}ms)")
            
            return TestResult(
                scenario_id=scenario.id,
                status=status,
                duration_ms=duration,
                output_log=output,
                error_message=error
            )

        except Exception as e:
            logger.error(f"    üî• System Error: {e}")
            return TestResult(
                scenario_id=scenario.id,
                status=TestStatus.ERROR,
                duration_ms=(time.time() - start) * 1000,
                output_log=str(e),
                error_message=str(e)
            )

    def _print_summary(self, report: TaaSReport):
        print("\n" + "="*60)
        print(f"üìä TaaS Execution Report: {report.scope_id}")
        print("="*60)
        print(f"Status: {report.overall_status.value.upper()}")
        print(f"Total:  {report.total_tests}")
        print(f"Passed: {report.passed} üü¢")
        print(f"Failed: {report.failed} üî¥")
        print("-" * 60)
        for res in report.results:
            icon = "‚úÖ" if res.status == TestStatus.PASSED else "‚ùå"
            print(f"{icon} {res.scenario_id}: {res.status.value} ({res.duration_ms:.0f}ms)")
            if res.error_message:
                print(f"   Error: {res.error_message}")
        print("="*60 + "\n")

# ============================================================================
# CLI Entry Point
# ============================================================================
if __name__ == "__main__":
    import argparse
    from datetime import datetime
    
    parser = argparse.ArgumentParser(description="Maestro TaaS Harness")
    parser.add_argument("--scope-file", help="Path to JSON file containing TestScope")
    parser.add_argument("--demo", action="store_true", help="Run a demo scope")
    
    args = parser.parse_args()
    
    harness = TaaSHarness(workspace_root=".")
    
    if args.demo:
        # Create a demo scope to illustrate the concept
        demo_scope = TestScope(
            project_id="pilot-001-api-gateway",
            scope_id="scope-2025-12-07-001",
            environment=TestEnvironment(
                type="docker",
                image="node:18-alpine"
            ),
            lifecycle=TestLifecycle(
                setup=["npm install"],
                teardown=["rm -rf node_modules"]
            ),
            scenarios=[
                TestScenario(
                    id="TEST-001",
                    name="Verify Health Endpoint",
                    description="Ensure /health returns 200 OK",
                    test_type=TestType.INTEGRATION,
                    target_component="api-gateway",
                    execution=TestExecution(command="echo 'Checking health...' && exit 0")
                ),
                TestScenario(
                    id="TEST-002",
                    name="Check Rate Limiting",
                    description="Ensure 429 is returned after limit",
                    test_type=TestType.E2E,
                    target_component="api-gateway",
                    execution=TestExecution(command="echo 'Simulating load...' && exit 0")
                ),
                TestScenario(
                    id="TEST-003",
                    name="Validate Auth Token",
                    description="Ensure invalid token returns 401",
                    test_type=TestType.SECURITY,
                    target_component="auth-service",
                    execution=TestExecution(command="echo 'Testing auth...' && exit 0")
                )
            ]
        )
        asyncio.run(harness.execute_scope(demo_scope))
    
    elif args.scope_file:
        with open(args.scope_file, 'r') as f:
            data = json.load(f)
            # In a real app, we'd deserialize properly
            # For now, just a placeholder
            print(f"Loading scope from {args.scope_file}...")
