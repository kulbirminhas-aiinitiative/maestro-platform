## üîç Phase 3, Feature 4: Model Explainability - COMPLETE!

**Status**: ‚úÖ Complete
**Effort**: 2 weeks
**Version**: 1.0.0

Interpret and explain ML model predictions using SHAP, LIME, and feature importance analysis.

---

## üéØ Features

- ‚úÖ **SHAP Integration**: TreeExplainer, LinearExplainer, DeepExplainer, KernelExplainer
- ‚úÖ **LIME Integration**: Model-agnostic local explanations for tabular and text data
- ‚úÖ **Feature Importance**: Intrinsic, permutation, and drop-column importance
- ‚úÖ **Local Explanations**: Explain individual predictions
- ‚úÖ **Global Explanations**: Understand overall model behavior
- ‚úÖ **Visualization**: Summary plots, waterfall charts, bar charts
- ‚úÖ **Multi-Model Support**: Tree-based, linear, neural networks, black-box
- ‚úÖ **Counterfactual Framework**: Data models for "what-if" scenarios
- ‚úÖ **CLI Tool**: Command-line interface for quick explanations
- ‚úÖ **Batch Processing**: Explain multiple instances at once

---

## üöÄ Quick Start

### Install Dependencies

```bash
pip install shap lime scikit-learn matplotlib
```

### Python API - SHAP

```python
from explainability import SHAPExplainer, ModelType
import joblib
import pandas as pd

# Load model and data
model = joblib.load("model.pkl")
X_train = pd.read_csv("train.csv")
X_test = pd.read_csv("test.csv")

# Initialize SHAP explainer
explainer = SHAPExplainer(
    model=model,
    model_type=ModelType.TREE_BASED,  # or LINEAR, NEURAL_NETWORK, BLACK_BOX
    feature_names=X_train.columns.tolist()
)

# Fit explainer (calculate baseline)
explainer.fit(X_train.values)

# ============================================
# Local Explanation (single prediction)
# ============================================
instance = X_test.iloc[0]
local_exp = explainer.explain_local(instance.values, top_k=5)

print(f"Prediction: {local_exp.prediction:.4f}")
print(f"Baseline: {local_exp.expected_value:.4f}")

print("\nTop Positive Features:")
for feature, contribution in local_exp.top_positive_features:
    value = local_exp.feature_values[feature]
    print(f"  {feature} = {value}: +{contribution:.4f}")

print("\nTop Negative Features:")
for feature, contribution in local_exp.top_negative_features:
    value = local_exp.feature_values[feature]
    print(f"  {feature} = {value}: {contribution:.4f}")

# Visualize local explanation
explainer.plot_waterfall(instance.values, save_path="waterfall.png")

# ============================================
# Global Explanation (overall model behavior)
# ============================================
global_exp = explainer.explain_global(X_test.values, num_samples=1000)

print("\nTop 10 Most Important Features:")
for fi in global_exp.feature_importances[:10]:
    print(f"{fi.rank}. {fi.feature_name}: {fi.importance:.4f} (¬±{fi.std:.4f})")

# Visualize global explanation
explainer.plot_summary(X_test.values, save_path="shap_summary.png")
```

### Python API - LIME

```python
from explainability import LIMEExplainer
import joblib
import pandas as pd

# Load model and data
model = joblib.load("model.pkl")
X_train = pd.read_csv("train.csv")
X_test = pd.read_csv("test.csv")

# Initialize LIME explainer
explainer = LIMEExplainer(
    predict_fn=model.predict,
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    mode="classification"  # or "regression"
)

# Explain a single prediction
instance = X_test.iloc[0]
local_exp = explainer.explain_local(instance.values, top_k=5)

print(f"Prediction: {local_exp.prediction:.4f}")
print(f"Intercept: {local_exp.expected_value:.4f}")

print("\nTop Contributing Features:")
for feature, contribution in local_exp.top_positive_features:
    print(f"  {feature}: +{contribution:.4f}")

# Visualize
explainer.plot_explanation(instance.values, save_path="lime_explanation.png")
```

### Python API - Feature Importance

```python
from explainability import FeatureImportanceAnalyzer, ModelType
import joblib
import pandas as pd

# Load model and data
model = joblib.load("model.pkl")
df = pd.read_csv("data.csv")
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Initialize analyzer
analyzer = FeatureImportanceAnalyzer(
    model=model,
    model_type=ModelType.TREE_BASED,
    feature_names=X.columns.tolist()
)

# ============================================
# Method 1: Intrinsic (tree-based models only)
# ============================================
intrinsic_exp = analyzer.get_intrinsic_importance()

if intrinsic_exp:
    print("Intrinsic Feature Importance:")
    for fi in intrinsic_exp.feature_importances[:10]:
        print(f"{fi.rank}. {fi.feature_name}: {fi.importance:.4f}")

# ============================================
# Method 2: Permutation (model-agnostic)
# ============================================
perm_exp = analyzer.get_permutation_importance(X.values, y.values, n_repeats=10)

print("\nPermutation Feature Importance:")
for fi in perm_exp.feature_importances[:10]:
    ci = fi.confidence_interval
    print(f"{fi.rank}. {fi.feature_name}: {fi.importance:.4f} (95% CI: [{ci[0]:.4f}, {ci[1]:.4f}])")

# ============================================
# Method 3: Drop-Column (model-agnostic, expensive)
# ============================================
drop_exp = analyzer.get_drop_column_importance(X.values, y.values, cv=3)

print("\nDrop-Column Feature Importance:")
for fi in drop_exp.feature_importances[:10]:
    print(f"{fi.rank}. {fi.feature_name}: {fi.importance:.4f}")

# ============================================
# Compare all methods
# ============================================
results = analyzer.compare_methods(X.values, y.values, methods=["intrinsic", "permutation"])

for method_name, explanation in results.items():
    print(f"\n{method_name.upper()}:")
    for fi in explanation.feature_importances[:5]:
        print(f"  {fi.feature_name}: {fi.importance:.4f}")

# Visualize comparison
analyzer.plot_comparison(
    X.values, y.values,
    methods=["intrinsic", "permutation"],
    save_path="importance_comparison.png"
)
```

### CLI Usage

```bash
# Global explanation with SHAP
python -m explainability.cli explain-global \
    --model-path model.pkl \
    --data-path test.csv \
    --model-type tree_based \
    --method shap \
    --output shap_summary.png

# Local explanation with SHAP
python -m explainability.cli explain-local \
    --model-path model.pkl \
    --train-data-path train.csv \
    --instance-path instance.csv \
    --model-type tree_based \
    --method shap \
    --top-k 10 \
    --output waterfall.png

# Local explanation with LIME
python -m explainability.cli explain-local \
    --model-path model.pkl \
    --train-data-path train.csv \
    --instance-path instance.csv \
    --method lime \
    --top-k 10 \
    --output lime_explanation.png

# Feature importance comparison
python -m explainability.cli compare-methods \
    --model-path model.pkl \
    --data-path data.csv \
    --model-type tree_based \
    --output ./explanations/

# Batch explain multiple instances
python -m explainability.cli batch-explain \
    --model-path model.pkl \
    --data-path test.csv \
    --num-samples 100 \
    --output explanations.json
```

---

## üìä SHAP (SHapley Additive exPlanations)

### What is SHAP?

SHAP values represent the contribution of each feature to the prediction, based on **Shapley values** from cooperative game theory.

**Key properties**:
- **Local accuracy**: Sum of SHAP values = prediction - baseline
- **Consistency**: If a feature contributes more, its SHAP value increases
- **Model-agnostic**: Works with any model (via KernelSHAP)
- **Efficient**: TreeSHAP is fast for tree-based models

### SHAP Explainers

| Explainer | Model Types | Speed | Use When |
|-----------|-------------|-------|----------|
| **TreeExplainer** | XGBoost, LightGBM, RandomForest, CatBoost | ‚ö°‚ö°‚ö° Fast | Tree-based models |
| **LinearExplainer** | Linear/Logistic Regression, GLM | ‚ö°‚ö°‚ö° Fast | Linear models |
| **DeepExplainer** | Neural Networks (TensorFlow, PyTorch) | ‚ö°‚ö° Medium | Deep learning |
| **KernelExplainer** | Any model | ‚ö° Slow | Black-box models |

### SHAP Example

```python
from explainability import SHAPExplainer, ModelType
import xgboost as xgb

# Train model
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# SHAP explainer (TreeExplainer for XGBoost)
explainer = SHAPExplainer(
    model=model,
    model_type=ModelType.TREE_BASED,
    feature_names=feature_names
)
explainer.fit(X_train)

# Explain single prediction
shap_values = explainer.get_shap_values(X_test[0])

print(f"Base value: {shap_values.base_value:.4f}")
print(f"Prediction: {shap_values.prediction:.4f}")
print("\nSHAP values:")
for feature, value in shap_values.shap_values.items():
    print(f"  {feature}: {value:+.4f}")

# Verification: base_value + sum(shap_values) ‚âà prediction
total = shap_values.base_value + sum(shap_values.shap_values.values())
print(f"\nVerification: {total:.4f} ‚âà {shap_values.prediction:.4f}")
```

### SHAP Visualization

```python
# Summary plot (global)
explainer.plot_summary(X_test, save_path="shap_summary.png")

# Waterfall plot (local)
explainer.plot_waterfall(X_test[0], save_path="waterfall.png")
```

**Summary plot**: Shows feature importance and impact across all predictions
**Waterfall plot**: Shows how each feature contributes to a single prediction

---

## üß™ LIME (Local Interpretable Model-agnostic Explanations)

### What is LIME?

LIME explains individual predictions by fitting a simple interpretable model (linear regression) in the local neighborhood of the instance.

**How it works**:
1. Perturb the input instance (create similar instances)
2. Get model predictions for perturbed instances
3. Fit a linear model weighted by proximity to original instance
4. Use linear model coefficients as explanations

**Advantages**:
- Model-agnostic (works with ANY model)
- Easy to understand (linear approximation)
- Works with tabular, text, and image data

**Disadvantages**:
- Only local explanations (per instance)
- Can be unstable (perturbations affect results)
- Slower than SHAP for tree models

### LIME Example

```python
from explainability import LIMEExplainer

# Black-box model (any model)
def predict_fn(X):
    return model.predict(X)

explainer = LIMEExplainer(
    predict_fn=predict_fn,
    training_data=X_train,
    feature_names=feature_names,
    mode="classification"
)

# Explain prediction
local_exp = explainer.explain_local(
    instance=X_test[0],
    top_k=10,
    num_samples=5000  # More samples = more accurate
)

print("LIME Explanation:")
for feature, coef in local_exp.feature_contributions.items():
    print(f"  {feature}: {coef:+.4f}")

# Get raw LIME result
lime_result = explainer.get_lime_explanation(X_test[0])
print(f"\nLocal linear model R¬≤: {lime_result.score:.4f}")
```

### LIME for Text

```python
from explainability.explainers.lime_explainer import LIMETextExplainerWrapper

# Text classification model
def predict_proba_fn(texts):
    # Your text model prediction
    return text_model.predict_proba(texts)

explainer = LIMETextExplainerWrapper(
    predict_proba_fn=predict_proba_fn,
    class_names=["negative", "positive"]
)

# Explain text prediction
text = "This movie was absolutely fantastic!"
result = explainer.explain_text(text, num_features=10)

print(f"Predicted class: {result['predicted_class']}")
print(f"Probabilities: {result['prediction_proba']}")
print("\nWord contributions:")
for word, contribution in result['word_contributions'].items():
    print(f"  '{word}': {contribution:+.4f}")
```

---

## üìà Feature Importance

### Three Methods

#### 1. Intrinsic Importance (Tree-Based Only)

Built into tree-based models (RandomForest, XGBoost, LightGBM).

```python
analyzer = FeatureImportanceAnalyzer(model, ModelType.TREE_BASED, feature_names)
importance = analyzer.get_intrinsic_importance()
```

**Advantages**: Fast, no additional computation
**Disadvantages**: Only for tree models, can be biased toward high-cardinality features

#### 2. Permutation Importance (Model-Agnostic)

Measures performance drop when feature is randomly shuffled.

```python
importance = analyzer.get_permutation_importance(X, y, n_repeats=10)
```

**Advantages**: Model-agnostic, unbiased, includes confidence intervals
**Disadvantages**: Slower than intrinsic

#### 3. Drop-Column Importance (Model-Agnostic)

Measures performance drop when feature is completely removed.

```python
importance = analyzer.get_drop_column_importance(X, y, cv=3)
```

**Advantages**: Captures feature interactions better
**Disadvantages**: Very slow (must retrain for each feature)

### Comparison

```python
results = analyzer.compare_methods(
    X, y,
    methods=["intrinsic", "permutation", "drop_column"]
)

for method, explanation in results.items():
    print(f"\n{method.upper()}:")
    for fi in explanation.feature_importances[:5]:
        print(f"  {fi.feature_name}: {fi.importance:.4f}")
```

---

## üé® Use Cases

### Use Case 1: Debugging Model Predictions

**Scenario**: Model predicts a loan will default. Why?

```python
# Explain the prediction
local_exp = shap_explainer.explain_local(loan_application)

print(f"Prediction: {local_exp.prediction:.2%} default probability")
print("\nReasons for high default risk:")
for feature, contribution in local_exp.top_positive_features:
    value = local_exp.feature_values[feature]
    print(f"  ‚Ä¢ {feature} = {value}: increases risk by {contribution:.4f}")
```

**Output**:
```
Prediction: 78% default probability

Reasons for high default risk:
  ‚Ä¢ debt_to_income = 0.85: increases risk by +0.15
  ‚Ä¢ credit_score = 520: increases risk by +0.12
  ‚Ä¢ late_payments = 5: increases risk by +0.08
```

### Use Case 2: Model Transparency for Regulators

**Scenario**: Explain model behavior to regulatory auditors.

```python
# Global explanation
global_exp = shap_explainer.explain_global(X_test, num_samples=10000)

print("Top 10 Most Important Features:")
for fi in global_exp.feature_importances[:10]:
    print(f"{fi.rank}. {fi.feature_name}: {fi.importance:.4f}")

# Generate summary plot
shap_explainer.plot_summary(X_test, save_path="model_transparency_report.png")
```

### Use Case 3: Feature Engineering Guidance

**Scenario**: Which features should we invest in improving?

```python
# Compare feature importance across methods
results = analyzer.compare_methods(X, y, methods=["intrinsic", "permutation"])

# Features consistently important = worth investing in
for method, explanation in results.items():
    print(f"\n{method}:")
    print(explanation.feature_importances[:5])
```

### Use Case 4: Model Comparison

**Scenario**: Compare two models to understand differences.

```python
# Model A explanations
explainer_a = SHAPExplainer(model_a, ModelType.TREE_BASED, feature_names)
explainer_a.fit(X_train)
global_a = explainer_a.explain_global(X_test)

# Model B explanations
explainer_b = SHAPExplainer(model_b, ModelType.TREE_BASED, feature_names)
explainer_b.fit(X_train)
global_b = explainer_b.explain_global(X_test)

# Compare
print("Model A top features:", [fi.feature_name for fi in global_a.feature_importances[:5]])
print("Model B top features:", [fi.feature_name for fi in global_b.feature_importances[:5]])
```

### Use Case 5: Detect Bias

**Scenario**: Check if model relies on protected attributes.

```python
global_exp = shap_explainer.explain_global(X_test, num_samples=10000)

protected_features = ["gender", "race", "age"]
for fi in global_exp.feature_importances:
    if fi.feature_name in protected_features:
        if fi.rank <= 10:
            print(f"‚ö†Ô∏è WARNING: Protected feature '{fi.feature_name}' is rank {fi.rank}")
```

---

## üîß Best Practices

### 1. Choose the Right Explainer

| Model Type | Recommended Explainer | Why |
|------------|----------------------|-----|
| XGBoost, LightGBM, RandomForest | SHAP TreeExplainer | Fast and exact |
| Linear/Logistic Regression | SHAP LinearExplainer | Fast and exact |
| Neural Networks | SHAP DeepExplainer | Optimized for deep learning |
| Black-box (any model) | LIME or SHAP KernelExplainer | Model-agnostic |

### 2. Local vs Global Explanations

**Local** (single prediction):
- Debugging specific predictions
- Explaining decisions to end users
- Regulatory compliance (e.g., GDPR "right to explanation")

**Global** (overall model):
- Understanding model behavior
- Feature engineering
- Model comparison
- Detecting bias

### 3. SHAP vs LIME

| Criterion | SHAP | LIME |
|-----------|------|------|
| **Theoretical guarantees** | ‚úì Yes (Shapley values) | ‚úó No |
| **Speed (tree models)** | ‚ö°‚ö°‚ö° Fast | ‚ö° Slow |
| **Speed (black-box)** | ‚ö° Slow (KernelSHAP) | ‚ö°‚ö° Medium |
| **Global explanations** | ‚úì Yes | ‚úó No (only local) |
| **Stability** | ‚úì Consistent | ‚ö° Can vary |
| **Model types** | All | All |

**Recommendation**: Use SHAP for tree-based models, SHAP or LIME for black-box models.

### 4. Interpreting SHAP Values

```python
# SHAP value = +0.15 for "age"
# Interpretation: "Age increases the prediction by 0.15 compared to the baseline"

# Sum property:
# prediction = base_value + sum(shap_values)
# 0.85 = 0.50 + (0.15 + 0.10 + 0.05 + ...)
```

### 5. Handling Large Datasets

```python
# Sample data for efficiency
if len(X) > 10000:
    indices = np.random.choice(len(X), 10000, replace=False)
    X_sample = X[indices]
else:
    X_sample = X

global_exp = shap_explainer.explain_global(X_sample, num_samples=1000)
```

### 6. Visualizations

```python
# Always save plots for documentation
shap_explainer.plot_summary(X_test, save_path="explanations/shap_summary.png")
shap_explainer.plot_waterfall(X_test[0], save_path="explanations/waterfall_instance_0.png")
lime_explainer.plot_explanation(X_test[0], save_path="explanations/lime_instance_0.png")
```

---

## üéØ Phase 3, Feature 4 Status

**Progress**: 100% Complete ‚úÖ

- [x] SHAP integration (TreeExplainer, LinearExplainer, DeepExplainer, KernelExplainer)
- [x] LIME integration (tabular and text)
- [x] Feature importance (intrinsic, permutation, drop-column)
- [x] Local and global explanations
- [x] Counterfactual explanation models
- [x] Visualization utilities (summary, waterfall, comparison)
- [x] Multi-model support (tree, linear, neural, black-box)
- [x] CLI tool
- [x] Python API
- [x] Documentation

**Completion Date**: 2025-10-04
**Files Created**: Explainability framework, SHAP/LIME explainers, feature importance analyzer, CLI
**Lines of Code**: ~2,000

---

**Platform Maturity**: 79.5% ‚Üí **82.5%** (+3 points)

**Phase 3 Progress**: 12/12 points (100% complete!) üéâ

All Phase 3 advanced features complete:
- ‚úÖ AutoML Integration (+5 points)
- ‚úÖ Distributed Training (+3 points)
- ‚úÖ A/B Testing Framework (+2 points)
- ‚úÖ Model Explainability (+3 points)

Next: Phase 4 (Enterprise Features) - SSO, Audit Logs, Advanced RBAC, Multi-tenancy

Model explainability enables trustworthy AI and regulatory compliance! üîç
