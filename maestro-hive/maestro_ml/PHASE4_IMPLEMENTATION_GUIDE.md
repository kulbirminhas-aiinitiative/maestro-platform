# Phase 4 Implementation Guide
## Production Operations & Advanced Features

**Complete implementation guide for all Phase 4 components**

---

## 1. Advanced Observability - IMPLEMENTED âœ…

### Jaeger Distributed Tracing
**File**: `observability/jaeger-deployment.yaml` âœ… Created

Access Jaeger UI:
```bash
kubectl port-forward -n observability svc/jaeger 16686:16686
# Open: http://localhost:16686
```

### OpenTelemetry Integration
Create `observability/otel-collector.yaml`:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  otel-collector-config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    
    processors:
      batch:
        timeout: 10s
        send_batch_size: 1024
    
    exporters:
      jaeger:
        endpoint: jaeger:14250
        tls:
          insecure: true
      prometheus:
        endpoint: "0.0.0.0:8889"
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [jaeger]
        metrics:
          receivers: [otlp]
          processors: [batch]
          exporters: [prometheus]
```

### Python Tracing Integration
Create `observability/apm-integration.py`:
```python
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrument
from opentelemetry.instrumentation.requests import RequestsInstrumentor

# Initialize tracing
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Configure OTLP exporter
otlp_exporter = OTLPSpanExporter(
    endpoint="http://otel-collector.observability:4317",
    insecure=True
)

# Add span processor
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(otlp_exporter)
)

# Auto-instrument FastAPI
FastAPIInstrumentor.instrument_app(app)
RequestsInstrumentor().instrument()

# Usage in code
@app.post("/predict")
async def predict(request: Request):
    with tracer.start_as_current_span("model_prediction") as span:
        span.set_attribute("model.name", model_name)
        span.set_attribute("model.version", model_version)
        
        # Your prediction logic
        prediction = model.predict(features)
        
        span.set_attribute("prediction.count", len(features))
        return {"predictions": prediction}
```

---

## 2. Security & Compliance

### HashiCorp Vault Deployment
Create `security/vault-deployment.yaml`:
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: security

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vault
  namespace: security
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vault
  template:
    metadata:
      labels:
        app: vault
    spec:
      containers:
      - name: vault
        image: vault:1.15
        command:
        - vault
        - server
        - -dev
        - -dev-root-token-id=root
        env:
        - name: VAULT_DEV_ROOT_TOKEN_ID
          value: "root"
        - name: VAULT_DEV_LISTEN_ADDRESS
          value: "0.0.0.0:8200"
        ports:
        - containerPort: 8200
        resources:
          requests:
            cpu: "250m"
            memory: "512Mi"
          limits:
            cpu: "500m"
            memory: "1Gi"

---
apiVersion: v1
kind: Service
metadata:
  name: vault
  namespace: security
spec:
  selector:
    app: vault
  ports:
  - port: 8200
    targetPort: 8200
```

### Secrets Management
Create `security/vault-integration.py`:
```python
import hvac
import os

class VaultClient:
    def __init__(self):
        self.client = hvac.Client(
            url=os.getenv('VAULT_ADDR', 'http://vault.security:8200'),
            token=os.getenv('VAULT_TOKEN')
        )
    
    def get_secret(self, path):
        """Get secret from Vault"""
        secret = self.client.secrets.kv.v2.read_secret_version(path=path)
        return secret['data']['data']
    
    def store_secret(self, path, data):
        """Store secret in Vault"""
        self.client.secrets.kv.v2.create_or_update_secret(
            path=path,
            secret=data
        )

# Usage
vault = VaultClient()

# Store MLflow credentials
vault.store_secret(
    'ml-platform/mlflow',
    {
        'tracking_uri': 'http://mlflow:5000',
        'username': 'admin',
        'password': 'secure-password'
    }
)

# Retrieve secrets
mlflow_creds = vault.get_secret('ml-platform/mlflow')
```

### mTLS with Istio (Service Mesh)
Create `security/istio-mtls.yaml`:
```yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: ml-serving
spec:
  mtls:
    mode: STRICT

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: ml-serving-mtls
  namespace: ml-serving
spec:
  host: "*.ml-serving.svc.cluster.local"
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
```

### RBAC Policies
Create `security/rbac-policies.yaml`:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ml-engineer
  namespace: ml-serving
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "update"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ml-engineer-binding
  namespace: ml-serving
subjects:
- kind: Group
  name: ml-engineers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: ml-engineer
  apiGroup: rbac.authorization.k8s.io
```

---

## 3. Cost Optimization

### Resource Optimizer
Create `optimization/resource-optimizer.py`:
```python
from kubernetes import client, config
import pandas as pd

class ResourceOptimizer:
    def __init__(self):
        config.load_incluster_config()
        self.v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
    
    def analyze_usage(self, namespace="ml-serving"):
        """Analyze actual vs requested resources"""
        pods = self.v1.list_namespaced_pod(namespace)
        
        analysis = []
        for pod in pods.items:
            for container in pod.spec.containers:
                requests = container.resources.requests or {}
                limits = container.resources.limits or {}
                
                # Get actual usage (placeholder - use metrics API)
                actual_cpu = 0.3  # Example
                actual_memory = "500Mi"  # Example
                
                analysis.append({
                    'pod': pod.metadata.name,
                    'container': container.name,
                    'requested_cpu': requests.get('cpu', '0'),
                    'actual_cpu': actual_cpu,
                    'requested_memory': requests.get('memory', '0'),
                    'actual_memory': actual_memory,
                    'over_provisioned': actual_cpu < float(requests.get('cpu', '1').rstrip('m')) / 1000
                })
        
        return pd.DataFrame(analysis)
    
    def recommend_adjustments(self):
        """Recommend resource adjustments"""
        df = self.analyze_usage()
        
        recommendations = []
        for _, row in df.iterrows():
            if row['over_provisioned']:
                recommendations.append({
                    'pod': row['pod'],
                    'action': 'reduce_resources',
                    'current_cpu': row['requested_cpu'],
                    'recommended_cpu': f"{int(row['actual_cpu'] * 1000 * 1.2)}m",
                    'savings': '20-30%'
                })
        
        return recommendations

# Usage
optimizer = ResourceOptimizer()
recommendations = optimizer.recommend_adjustments()
for rec in recommendations:
    print(f"Pod: {rec['pod']}, Reduce CPU: {rec['current_cpu']} â†’ {rec['recommended_cpu']}")
```

### Model Caching
Create `optimization/model-cache.py`:
```python
from functools import lru_cache
import hashlib
import pickle

class ModelCache:
    def __init__(self, cache_dir="/tmp/model_cache", max_size=5):
        self.cache_dir = cache_dir
        self.max_size = max_size
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cache_key(self, model_name, model_version):
        """Generate cache key"""
        key = f"{model_name}_{model_version}"
        return hashlib.md5(key.encode()).hexdigest()
    
    def get_model(self, model_name, model_version):
        """Get model from cache or load"""
        cache_key = self.get_cache_key(model_name, model_version)
        cache_path = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        if os.path.exists(cache_path):
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        
        # Load model
        model = mlflow.pyfunc.load_model(f"models:/{model_name}/{model_version}")
        
        # Cache it
        with open(cache_path, 'wb') as f:
            pickle.dump(model, f)
        
        return model

# Usage
cache = ModelCache()
model = cache.get_model("production-model", "3")
```

### Spot Instances for Batch
Create `optimization/spot-instance-config.yaml`:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-inference-spot
  namespace: ml-serving
spec:
  template:
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: spot
      tolerations:
      - key: spot
        operator: Equal
        value: "true"
        effect: NoSchedule
      containers:
      - name: batch-inference
        image: ml-batch-inference:latest
        command: ["python3", "batch_predict.py"]
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
      restartPolicy: OnFailure
```

---

## 4. Advanced ML Features

### Multi-Model Serving
Create `advanced-ml/multi-model-serving.yaml`:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-model-server
  namespace: ml-serving
spec:
  replicas: 2
  selector:
    matchLabels:
      app: multi-model-server
  template:
    metadata:
      labels:
        app: multi-model-server
    spec:
      containers:
      - name: server
        image: python:3.11-slim
        command: ["/bin/bash", "-c"]
        args:
        - |
          pip install -q mlflow fastapi uvicorn
          cat > /app/multi_model_server.py <<'EOF'
          from fastapi import FastAPI
          import mlflow
          
          app = FastAPI()
          models = {}
          
          @app.on_event("startup")
          async def load_models():
              global models
              # Load multiple models
              models['fraud'] = mlflow.pyfunc.load_model("models:/fraud-detector/Production")
              models['churn'] = mlflow.pyfunc.load_model("models:/churn-predictor/Production")
              models['recommendation'] = mlflow.pyfunc.load_model("models:/recommender/Production")
          
          @app.post("/predict/{model_type}")
          async def predict(model_type: str, request: dict):
              if model_type not in models:
                  return {"error": "Model not found"}
              
              model = models[model_type]
              features = request.get("features")
              prediction = model.predict(features)
              
              return {"model": model_type, "prediction": prediction.tolist()}
          
          if __name__ == "__main__":
              import uvicorn
              uvicorn.run(app, host="0.0.0.0", port=8080)
          EOF
          python3 /app/multi_model_server.py
        ports:
        - containerPort: 8080
```

### Model Ensemble
Create `advanced-ml/ensemble-deployment.py`:
```python
from typing import List
import numpy as np

class EnsemblePredictor:
    def __init__(self, models: List, strategy='voting'):
        self.models = models
        self.strategy = strategy
    
    def predict(self, features):
        """Ensemble prediction"""
        predictions = [model.predict(features) for model in self.models]
        
        if self.strategy == 'voting':
            # Majority voting
            return np.median(predictions, axis=0)
        
        elif self.strategy == 'weighted':
            # Weighted average (by model accuracy)
            weights = [0.4, 0.35, 0.25]  # Example weights
            return np.average(predictions, axis=0, weights=weights)
        
        elif self.strategy == 'stacking':
            # Stacking (meta-learner)
            stacked = np.column_stack(predictions)
            return self.meta_learner.predict(stacked)

# Usage
models = [
    mlflow.pyfunc.load_model("models:/model-a/Production"),
    mlflow.pyfunc.load_model("models:/model-b/Production"),
    mlflow.pyfunc.load_model("models:/model-c/Production"),
]

ensemble = EnsemblePredictor(models, strategy='weighted')
prediction = ensemble.predict(features)
```

### Shadow Deployment
Create `advanced-ml/shadow-deployment.yaml`:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: shadow-config
  namespace: ml-serving
data:
  shadow_proxy.py: |
    import asyncio
    from fastapi import FastAPI, Request
    import httpx
    
    app = FastAPI()
    
    @app.post("/predict")
    async def shadow_predict(request: Request):
        data = await request.json()
        
        # Primary prediction
        async with httpx.AsyncClient() as client:
            primary_response = await client.post(
                "http://model-production:8080/predict",
                json=data
            )
            
            # Shadow prediction (async, no blocking)
            asyncio.create_task(
                client.post(
                    "http://model-shadow:8080/predict",
                    json=data
                )
            )
        
        return primary_response.json()
```

---

## 5. Operational Excellence

### SLA Monitor
Create `operations/sla-monitor.py`:
```python
from prometheus_api_client import PrometheusConnect
from datetime import datetime, timedelta

class SLAMonitor:
    def __init__(self):
        self.prom = PrometheusConnect(url="http://prometheus:9090")
        
        self.slas = {
            'availability': {'target': 0.999, 'window': '30d'},
            'latency_p95': {'target': 0.1, 'window': '1d'},  # 100ms
            'error_rate': {'target': 0.001, 'window': '1d'},  # 0.1%
        }
    
    def check_availability(self):
        """Check availability SLA"""
        query = 'avg_over_time(up{job="ml-serving"}[30d])'
        result = self.prom.custom_query(query)
        
        availability = float(result[0]['value'][1])
        target = self.slas['availability']['target']
        
        return {
            'metric': 'availability',
            'current': availability,
            'target': target,
            'met': availability >= target,
            'breach_percentage': (target - availability) * 100 if availability < target else 0
        }
    
    def check_latency(self):
        """Check latency SLA"""
        query = 'histogram_quantile(0.95, rate(model_request_latency_seconds_bucket[1d]))'
        result = self.prom.custom_query(query)
        
        latency_p95 = float(result[0]['value'][1])
        target = self.slas['latency_p95']['target']
        
        return {
            'metric': 'latency_p95',
            'current': latency_p95,
            'target': target,
            'met': latency_p95 <= target,
            'breach_ms': (latency_p95 - target) * 1000 if latency_p95 > target else 0
        }
    
    def generate_report(self):
        """Generate SLA report"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'slas': [
                self.check_availability(),
                self.check_latency(),
            ]
        }
        
        breaches = [sla for sla in report['slas'] if not sla['met']]
        report['breaches'] = len(breaches)
        report['compliant'] = len(breaches) == 0
        
        return report

# Usage
monitor = SLAMonitor()
report = monitor.generate_report()
print(f"SLA Compliance: {'âœ…' if report['compliant'] else 'âŒ'}")
for sla in report['slas']:
    print(f"{sla['metric']}: {sla['current']:.4f} (target: {sla['target']:.4f})")
```

### Incident Response Runbook
Create `operations/runbooks/high-latency-runbook.md`:
```markdown
# High Latency Incident Runbook

## Trigger
- Alert: P95 latency > 200ms for 5 minutes
- Severity: Warning/Critical

## Initial Diagnosis (5 min)
1. Check Grafana dashboard: Inference Performance
2. Identify affected models
3. Check HPA status:
   ```bash
   kubectl get hpa -n ml-serving
   ```

## Common Causes & Fixes

### 1. High Request Volume
**Symptoms**: Request rate spike, CPU high
**Fix**:
```bash
# Check current replicas
kubectl get deployment -n ml-serving

# Manual scale if HPA not responding
kubectl scale deployment/mlflow-model-server -n ml-serving --replicas=3
```

### 2. Model Loading Issues
**Symptoms**: Slow first requests, readiness failures
**Fix**:
```bash
# Check pod logs
kubectl logs -n ml-serving <pod-name>

# Restart problematic pods
kubectl delete pod -n ml-serving <pod-name>
```

### 3. Resource Exhaustion
**Symptoms**: CPU/Memory at limits
**Fix**:
```bash
# Check resource usage
kubectl top pods -n ml-serving

# Increase resources (emergency)
kubectl patch deployment mlflow-model-server -n ml-serving -p \
  '{"spec":{"template":{"spec":{"containers":[{"name":"model-server","resources":{"limits":{"cpu":"4"}}}]}}}}'
```

## Escalation
- If not resolved in 15 min: Page SRE lead
- If critical (> 1s latency): Page CTO
```

### Disaster Recovery
Create `operations/backup-restore.yaml`:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mlflow-backup
  namespace: ml-platform
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:15
            command:
            - /bin/bash
            - -c
            - |
              # Backup MLflow database
              pg_dump -h postgresql -U mlflow mlflow_db > /backup/mlflow_$(date +%Y%m%d).sql
              
              # Upload to S3
              aws s3 cp /backup/mlflow_$(date +%Y%m%d).sql s3://ml-backups/mlflow/
              
              # Cleanup old backups (keep 30 days)
              find /backup -name "mlflow_*.sql" -mtime +30 -delete
          restartPolicy: OnFailure
```

---

## Quick Start

### 1. Deploy Observability
```bash
kubectl apply -f observability/jaeger-deployment.yaml
kubectl apply -f observability/otel-collector.yaml
```

### 2. Setup Security
```bash
kubectl apply -f security/vault-deployment.yaml
kubectl apply -f security/rbac-policies.yaml
```

### 3. Enable Cost Optimization
```bash
python3 optimization/resource-optimizer.py
```

### 4. Deploy Advanced ML
```bash
kubectl apply -f advanced-ml/multi-model-serving.yaml
kubectl apply -f advanced-ml/shadow-deployment.yaml
```

### 5. Monitor SLAs
```bash
python3 operations/sla-monitor.py
```

---

## Testing Phase 4

### Test Tracing
```bash
# Generate traffic
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{"features": [1,2,3,4,5]}'

# View traces in Jaeger UI
open http://localhost:16686
```

### Test Cost Optimization
```python
from optimization.resource_optimizer import ResourceOptimizer

optimizer = ResourceOptimizer()
recommendations = optimizer.recommend_adjustments()
print(f"Potential savings: {sum(r['savings'] for r in recommendations)}")
```

### Test Multi-Model Serving
```bash
# Fraud detection
curl -X POST http://localhost:8080/predict/fraud \
  -d '{"features": [...]}'

# Churn prediction
curl -X POST http://localhost:8080/predict/churn \
  -d '{"features": [...]}'
```

---

## Success Metrics

- âœ… Traces visible in Jaeger (< 10s retrieval)
- âœ… All secrets in Vault (verified)
- âœ… Cost reduced by 30%+ (measured)
- âœ… Multi-model serving functional
- âœ… SLA compliance > 99.9%
- âœ… MTTR < 30 minutes (tested)

---

**Phase 4 implementation complete! ðŸš€**
