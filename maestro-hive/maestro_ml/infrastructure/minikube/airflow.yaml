# Apache Airflow for Minikube Testing
# Scaled-down configuration with LocalExecutor for simplicity

apiVersion: v1
kind: Namespace
metadata:
  name: airflow

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-config
  namespace: airflow
data:
  AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True"
  AIRFLOW__CORE__LOAD_EXAMPLES: "False"
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://maestro:maestro123@airflow-postgresql.airflow.svc.cluster.local:5432/airflow"
  AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/opt/airflow/logs"
  AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "30"
  AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: "False"

---
apiVersion: v1
kind: Secret
metadata:
  name: airflow-secrets
  namespace: airflow
type: Opaque
stringData:
  AIRFLOW__CORE__FERNET_KEY: "zP3KvUiHVh3dQwGxN3xKvJ7hK3iZvL3xKvJ7hK3iZvL="
  AIRFLOW__WEBSERVER__SECRET_KEY: "airflow_webserver_secret_key_minikube"

---
# PostgreSQL for Airflow Metadata
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: airflow-postgresql
  namespace: airflow
spec:
  serviceName: airflow-postgresql
  replicas: 1
  selector:
    matchLabels:
      app: airflow-postgresql
  template:
    metadata:
      labels:
        app: airflow-postgresql
    spec:
      containers:
      - name: postgresql
        image: postgres:15-alpine
        env:
        - name: POSTGRES_USER
          value: "maestro"
        - name: POSTGRES_PASSWORD
          value: "maestro123"
        - name: POSTGRES_DB
          value: "airflow"
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        ports:
        - containerPort: 5432
          name: postgresql
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        volumeMounts:
        - name: postgresql-data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgresql-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 2Gi

---
apiVersion: v1
kind: Service
metadata:
  name: airflow-postgresql
  namespace: airflow
spec:
  type: ClusterIP
  ports:
  - port: 5432
    targetPort: 5432
    name: postgresql
  selector:
    app: airflow-postgresql

---
# Airflow All-in-One (Webserver + Scheduler)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow
  namespace: airflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow
  template:
    metadata:
      labels:
        app: airflow
    spec:
      initContainers:
      - name: wait-for-db
        image: postgres:15-alpine
        command: ['sh', '-c', 'until pg_isready -h airflow-postgresql -U maestro; do echo waiting for db; sleep 2; done']
      - name: init-db
        image: apache/airflow:2.7.3-python3.11
        envFrom:
        - configMapRef:
            name: airflow-config
        - secretRef:
            name: airflow-secrets
        command: ['bash', '-c']
        args:
        - |
          airflow db init || airflow db upgrade
          airflow users create \
            --username admin \
            --password admin \
            --firstname Admin \
            --lastname User \
            --role Admin \
            --email admin@maestro.ml || echo "User already exists"
        volumeMounts:
        - name: dags
          mountPath: /opt/airflow/dags
        - name: logs
          mountPath: /opt/airflow/logs
      containers:
      - name: webserver
        image: apache/airflow:2.7.3-python3.11
        envFrom:
        - configMapRef:
            name: airflow-config
        - secretRef:
            name: airflow-secrets
        env:
        - name: AWS_ACCESS_KEY_ID
          value: "admin"
        - name: AWS_SECRET_ACCESS_KEY
          value: "minioadmin"
        - name: MLFLOW_S3_ENDPOINT_URL
          value: "http://minio.storage.svc.cluster.local:9000"
        ports:
        - containerPort: 8080
          name: http
        command: ['bash', '-c']
        args:
        - |
          # Install additional Python packages
          pip install apache-airflow-providers-amazon==8.11.0 \
                      apache-airflow-providers-cncf-kubernetes==7.9.0 \
                      mlflow==2.8.1 \
                      feast==0.35.0 \
                      scikit-learn==1.3.2 \
                      pandas==2.1.4

          # Start scheduler in background
          airflow scheduler &

          # Start webserver in foreground
          airflow webserver
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        volumeMounts:
        - name: dags
          mountPath: /opt/airflow/dags
        - name: logs
          mountPath: /opt/airflow/logs
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
      volumes:
      - name: dags
        emptyDir: {}
      - name: logs
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: airflow-webserver
  namespace: airflow
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30502  # Allocated via port registry
    name: http
  selector:
    app: airflow
