# Horizontal Pod Autoscaler with Custom Metrics
# Scales based on request rate and latency

---
# HPA for MLflow Model Server
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mlflow-model-server-hpa
  namespace: ml-serving
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mlflow-model-server
  minReplicas: 1
  maxReplicas: 3
  metrics:
  # CPU-based scaling
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # Memory-based scaling
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  # Request rate (custom metric from Prometheus)
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"

  # Request latency (custom metric)
  - type: Pods
    pods:
      metric:
        name: http_request_duration_seconds
      target:
        type: AverageValue
        averageValue: "0.2"  # 200ms

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 30
      selectPolicy: Max

    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 1
        periodSeconds: 60
      selectPolicy: Min

---
# PrometheusRule for custom metrics
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: model-serving-metrics
  namespace: ml-serving
spec:
  groups:
  - name: model_serving
    interval: 30s
    rules:
    # Request rate metric
    - record: http_requests_per_second
      expr: |
        rate(model_requests_total{namespace="ml-serving"}[1m])

    # Average latency metric
    - record: http_request_duration_seconds
      expr: |
        histogram_quantile(0.95,
          rate(model_request_latency_seconds_bucket{namespace="ml-serving"}[1m])
        )

    # Error rate metric
    - record: http_request_error_rate
      expr: |
        rate(model_requests_total{namespace="ml-serving",status="error"}[5m]) /
        rate(model_requests_total{namespace="ml-serving"}[5m])

---
# ServiceMonitor to export custom metrics
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-adapter-config
  namespace: ml-serving
data:
  config.yaml: |
    rules:
    - seriesQuery: 'model_requests_total{namespace="ml-serving"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^(.*)_total"
        as: "http_requests_per_second"
      metricsQuery: 'rate(<<.Series>>{<<.LabelMatchers>>}[1m])'

    - seriesQuery: 'model_request_latency_seconds_bucket{namespace="ml-serving"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^(.*)_bucket"
        as: "http_request_duration_seconds"
      metricsQuery: |
        histogram_quantile(0.95,
          rate(<<.Series>>{<<.LabelMatchers>>}[1m])
        )
