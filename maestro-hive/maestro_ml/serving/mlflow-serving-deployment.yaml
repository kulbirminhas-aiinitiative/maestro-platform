# MLflow Model Serving Deployment
# Serves models directly from MLflow Model Registry
# Optimized for current server capacity

---
# Namespace for serving
apiVersion: v1
kind: Namespace
metadata:
  name: ml-serving
  labels:
    app: ml-platform
    component: serving

---
# MLflow Model Server Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlflow-model-server
  namespace: ml-serving
  labels:
    app: mlflow-serving
    version: v1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mlflow-serving
  template:
    metadata:
      labels:
        app: mlflow-serving
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: model-server
        image: python:3.11-slim
        command:
        - /bin/bash
        - -c
        - |
          pip install -q mlflow scikit-learn pandas numpy prometheus-client
          python3 /app/serve.py
        env:
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow.ml-platform.svc.cluster.local:5000"
        - name: MODEL_NAME
          value: "production-model"
        - name: MODEL_STAGE
          value: "Production"
        - name: SERVER_PORT
          value: "8080"
        - name: WORKERS
          value: "2"
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
        volumeMounts:
        - name: serving-code
          mountPath: /app
      volumes:
      - name: serving-code
        configMap:
          name: mlflow-serving-code

---
# ConfigMap with serving code
apiVersion: v1
kind: ConfigMap
metadata:
  name: mlflow-serving-code
  namespace: ml-serving
data:
  serve.py: |
    import os
    import json
    import time
    from datetime import datetime
    from typing import Dict, List
    import mlflow
    from mlflow.tracking import MlflowClient
    from fastapi import FastAPI, HTTPException, Request
    from fastapi.responses import JSONResponse
    import uvicorn
    from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
    from starlette.responses import Response
    import numpy as np

    # Prometheus metrics
    REQUEST_COUNT = Counter('model_requests_total', 'Total inference requests', ['model', 'status'])
    REQUEST_LATENCY = Histogram('model_request_latency_seconds', 'Request latency', ['model'])
    PREDICTION_COUNT = Counter('model_predictions_total', 'Total predictions', ['model'])
    MODEL_VERSION = Gauge('model_version_info', 'Model version', ['model', 'version'])

    app = FastAPI(title="MLflow Model Serving API")

    # Global model holder
    MODEL = None
    MODEL_INFO = {}

    def load_model():
        """Load model from MLflow registry"""
        global MODEL, MODEL_INFO

        mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
        model_name = os.environ.get('MODEL_NAME', 'production-model')
        model_stage = os.environ.get('MODEL_STAGE', 'Production')

        print(f"Loading model: {model_name} (stage: {model_stage})")

        try:
            client = MlflowClient()

            # Get latest version in stage
            versions = client.get_latest_versions(model_name, stages=[model_stage])
            if not versions:
                raise ValueError(f"No model found for {model_name} in stage {model_stage}")

            latest_version = versions[0]
            model_uri = f"models:/{model_name}/{model_stage}"

            # Load model
            MODEL = mlflow.pyfunc.load_model(model_uri)

            MODEL_INFO = {
                'name': model_name,
                'version': latest_version.version,
                'stage': model_stage,
                'run_id': latest_version.run_id,
                'loaded_at': datetime.now().isoformat()
            }

            # Update Prometheus metric
            MODEL_VERSION.labels(model=model_name, version=latest_version.version).set(1)

            print(f"Model loaded successfully: v{latest_version.version}")
            return True

        except Exception as e:
            print(f"Error loading model: {e}")
            return False

    @app.on_event("startup")
    async def startup_event():
        """Load model on startup"""
        success = load_model()
        if not success:
            print("WARNING: Failed to load model on startup")

    @app.get("/health")
    async def health():
        """Health check endpoint"""
        return {"status": "healthy", "model_loaded": MODEL is not None}

    @app.get("/ready")
    async def ready():
        """Readiness check endpoint"""
        if MODEL is None:
            raise HTTPException(status_code=503, detail="Model not loaded")
        return {"status": "ready", "model": MODEL_INFO}

    @app.get("/model/info")
    async def model_info():
        """Get model information"""
        if MODEL is None:
            raise HTTPException(status_code=503, detail="Model not loaded")
        return MODEL_INFO

    @app.post("/predict")
    async def predict(request: Request):
        """Inference endpoint"""
        if MODEL is None:
            REQUEST_COUNT.labels(model=MODEL_INFO.get('name', 'unknown'), status='error').inc()
            raise HTTPException(status_code=503, detail="Model not loaded")

        start_time = time.time()
        model_name = MODEL_INFO.get('name', 'unknown')

        try:
            # Parse request
            data = await request.json()

            # Extract features
            if 'instances' in data:
                features = data['instances']
            elif 'features' in data:
                features = [data['features']]
            else:
                raise ValueError("Request must contain 'instances' or 'features'")

            # Make prediction
            predictions = MODEL.predict(features)

            # Convert to list if numpy array
            if isinstance(predictions, np.ndarray):
                predictions = predictions.tolist()

            # Record metrics
            latency = time.time() - start_time
            REQUEST_LATENCY.labels(model=model_name).observe(latency)
            REQUEST_COUNT.labels(model=model_name, status='success').inc()
            PREDICTION_COUNT.labels(model=model_name).inc(len(features))

            return {
                'predictions': predictions,
                'model': MODEL_INFO,
                'latency_ms': round(latency * 1000, 2)
            }

        except Exception as e:
            REQUEST_COUNT.labels(model=model_name, status='error').inc()
            raise HTTPException(status_code=500, detail=str(e))

    @app.post("/predict/batch")
    async def predict_batch(request: Request):
        """Batch prediction endpoint"""
        if MODEL is None:
            raise HTTPException(status_code=503, detail="Model not loaded")

        start_time = time.time()
        model_name = MODEL_INFO.get('name', 'unknown')

        try:
            data = await request.json()
            features = data.get('instances', [])

            if not features:
                raise ValueError("No instances provided")

            # Make predictions
            predictions = MODEL.predict(features)

            if isinstance(predictions, np.ndarray):
                predictions = predictions.tolist()

            # Record metrics
            latency = time.time() - start_time
            REQUEST_LATENCY.labels(model=model_name).observe(latency)
            REQUEST_COUNT.labels(model=model_name, status='success').inc()
            PREDICTION_COUNT.labels(model=model_name).inc(len(features))

            return {
                'predictions': predictions,
                'count': len(predictions),
                'model': MODEL_INFO,
                'latency_ms': round(latency * 1000, 2)
            }

        except Exception as e:
            REQUEST_COUNT.labels(model=model_name, status='error').inc()
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/metrics")
    async def metrics():
        """Prometheus metrics endpoint"""
        return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

    if __name__ == "__main__":
        port = int(os.environ.get('SERVER_PORT', 8080))
        workers = int(os.environ.get('WORKERS', 2))

        uvicorn.run(
            app,
            host="0.0.0.0",
            port=port,
            workers=workers,
            log_level="info"
        )

---
# Service
apiVersion: v1
kind: Service
metadata:
  name: mlflow-model-server
  namespace: ml-serving
  labels:
    app: mlflow-serving
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: mlflow-serving

---
# ServiceMonitor for Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: mlflow-model-server
  namespace: ml-serving
  labels:
    app: mlflow-serving
spec:
  selector:
    matchLabels:
      app: mlflow-serving
  endpoints:
  - port: http
    interval: 30s
    path: /metrics

---
# Ingress (optional)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mlflow-model-server
  namespace: ml-serving
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: models.ml-platform.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: mlflow-model-server
            port:
              number: 80
