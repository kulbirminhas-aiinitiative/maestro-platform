# Prometheus Alert Rules for ML Inference
# Alerts for latency, error rate, and performance issues

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ml-inference-alerts
  namespace: ml-serving
  labels:
    prometheus: kube-prometheus
spec:
  groups:
  - name: inference_performance
    interval: 30s
    rules:
    # High latency alert
    - alert: HighInferenceLatency
      expr: |
        histogram_quantile(0.95, 
          rate(model_request_latency_seconds_bucket{namespace="ml-serving"}[5m])
        ) > 0.2
      for: 5m
      labels:
        severity: warning
        component: ml-inference
      annotations:
        summary: "High inference latency detected"
        description: "Model {{ $labels.model }} has p95 latency {{ $value }}s (threshold: 0.2s)"

    # Very high latency alert
    - alert: CriticalInferenceLatency
      expr: |
        histogram_quantile(0.95,
          rate(model_request_latency_seconds_bucket{namespace="ml-serving"}[5m])
        ) > 1.0
      for: 2m
      labels:
        severity: critical
        component: ml-inference
      annotations:
        summary: "Critical inference latency"
        description: "Model {{ $labels.model }} has p95 latency {{ $value }}s (threshold: 1.0s)"

    # High error rate alert
    - alert: HighErrorRate
      expr: |
        rate(model_requests_total{namespace="ml-serving",status="error"}[5m]) /
        rate(model_requests_total{namespace="ml-serving"}[5m]) > 0.01
      for: 5m
      labels:
        severity: warning
        component: ml-inference
      annotations:
        summary: "High error rate detected"
        description: "Model {{ $labels.model }} has error rate {{ $value | humanizePercentage }} (threshold: 1%)"

    # Critical error rate alert
    - alert: CriticalErrorRate
      expr: |
        rate(model_requests_total{namespace="ml-serving",status="error"}[5m]) /
        rate(model_requests_total{namespace="ml-serving"}[5m]) > 0.05
      for: 2m
      labels:
        severity: critical
        component: ml-inference
      annotations:
        summary: "Critical error rate"
        description: "Model {{ $labels.model }} has error rate {{ $value | humanizePercentage }} (threshold: 5%)"

    # Low throughput alert
    - alert: LowThroughput
      expr: |
        rate(model_predictions_total{namespace="ml-serving"}[5m]) < 10
      for: 10m
      labels:
        severity: info
        component: ml-inference
      annotations:
        summary: "Low prediction throughput"
        description: "Model {{ $labels.model }} has throughput {{ $value }} predictions/sec (threshold: 10)"

    # Model not responding
    - alert: ModelNotResponding
      expr: |
        rate(model_requests_total{namespace="ml-serving"}[5m]) == 0
      for: 5m
      labels:
        severity: critical
        component: ml-inference
      annotations:
        summary: "Model not responding"
        description: "Model {{ $labels.model }} has received no requests in 5 minutes"

  - name: resource_alerts
    interval: 30s
    rules:
    # High CPU usage
    - alert: HighCPUUsage
      expr: |
        sum(rate(container_cpu_usage_seconds_total{namespace="ml-serving",container="model-server"}[5m]))
        by (pod) > 0.9
      for: 5m
      labels:
        severity: warning
        component: ml-infrastructure
      annotations:
        summary: "High CPU usage"
        description: "Pod {{ $labels.pod }} has CPU usage {{ $value | humanizePercentage }} (threshold: 90%)"

    # High memory usage
    - alert: HighMemoryUsage
      expr: |
        sum(container_memory_working_set_bytes{namespace="ml-serving",container="model-server"})
        by (pod) /
        sum(container_spec_memory_limit_bytes{namespace="ml-serving",container="model-server"})
        by (pod) > 0.85
      for: 5m
      labels:
        severity: warning
        component: ml-infrastructure
      annotations:
        summary: "High memory usage"
        description: "Pod {{ $labels.pod }} has memory usage {{ $value | humanizePercentage }} (threshold: 85%)"

    # Pod not ready
    - alert: PodNotReady
      expr: |
        kube_pod_status_phase{namespace="ml-serving",phase!="Running"} == 1
      for: 5m
      labels:
        severity: critical
        component: ml-infrastructure
      annotations:
        summary: "Pod not ready"
        description: "Pod {{ $labels.pod }} is in {{ $labels.phase }} state"

    # HPA at max replicas
    - alert: HPAAtMaxReplicas
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas{namespace="ml-serving"} ==
        kube_horizontalpodautoscaler_spec_max_replicas{namespace="ml-serving"}
      for: 10m
      labels:
        severity: warning
        component: ml-infrastructure
      annotations:
        summary: "HPA at maximum replicas"
        description: "HPA {{ $labels.horizontalpodautoscaler }} is at max replicas for 10 minutes"

  - name: model_quality
    interval: 1m
    rules:
    # Data drift detected
    - alert: DataDriftDetected
      expr: |
        mlflow_drift_score{model="production-model"} > 0.05
      for: 5m
      labels:
        severity: warning
        component: ml-quality
      annotations:
        summary: "Data drift detected"
        description: "Feature {{ $labels.feature }} has drift score {{ $value }} (threshold: 0.05)"

    # Low prediction confidence
    - alert: LowPredictionConfidence
      expr: |
        avg(model_confidence_score{namespace="ml-serving"}) < 0.7
      for: 10m
      labels:
        severity: info
        component: ml-quality
      annotations:
        summary: "Low prediction confidence"
        description: "Model {{ $labels.model }} has average confidence {{ $value }} (threshold: 0.7)"

    # Model version mismatch
    - alert: ModelVersionMismatch
      expr: |
        count(count by (version) (model_version_info{namespace="ml-serving"})) > 1
      for: 5m
      labels:
        severity: warning
        component: ml-quality
      annotations:
        summary: "Multiple model versions running"
        description: "Multiple versions of the same model are serving traffic"
