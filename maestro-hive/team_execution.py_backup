#!/usr/bin/env python3
"""
Autonomous SDLC Engine V3.1 - Persona-Level Intelligent Reuse + Resumable Sessions

Revolutionary Integration:
- V4.1 Persona-Level Reuse: Analyze each persona independently for artifact reuse
- Resumable Sessions: Continue work across multiple runs
- RAG Integration: Get templates and best practices
- Quality Review: Validate outputs with Quality Fabric
- Template Creation: High-quality outputs become reusable templates

Complete Workflow:
  Frontend (WebSocket/REST)
      â†“
  BFF Service (unified_bff_service.py)
      â†“
  Autonomous SDLC Engine V3.1 (THIS FILE)
      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ NEW: Persona-Level Reuse Analysis (V4.1)    â”‚
  â”‚ - Check similar projects                    â”‚
  â”‚ - Build PersonaReuseMap                     â”‚
  â”‚ - Decide per-persona: reuse vs execute      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ For Personas to REUSE (85%+ match):         â”‚
  â”‚ - Fetch artifacts from similar projects     â”‚
  â”‚ - Skip execution (0 minutes)                â”‚
  â”‚ - Integrate into current session            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ For Personas to EXECUTE (<85% match):       â”‚
  â”‚ - RAG Integration (templates/best practices)â”‚
  â”‚ - Persona Execution (Claude Code SDK)       â”‚
  â”‚ - Quality Review (quality_service.py)       â”‚
  â”‚ - Template Validation & Creation            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
  Response to Frontend (quality scores + reuse stats)

Key Innovation:
  Even if overall project is 50% similar (V3 would execute all),
  V3.1 identifies specific personas with 90%+ matches and reuses them!

  Example:
    Overall: 52% similar
    - system_architect: 100% â†’ REUSE âš¡
    - frontend_developer: 90% â†’ REUSE âš¡
    - backend_developer: 35% â†’ EXECUTE ğŸ”¨
    Result: 50% time savings (V3: 0% savings)
"""

import asyncio
import sys
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
import httpx
import json

# Add parent directories to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
sys.path.insert(0, str(Path(__file__).parent))

from personas import SDLCPersonas
from team_organization import get_deliverables_for_persona
from config import CLAUDE_CONFIG, OUTPUT_CONFIG
from session_manager import SessionManager, SDLCSession
from validation_utils import (
    validate_persona_deliverables,
    detect_stubs_and_placeholders,
    detect_project_type,
    analyze_implementation_quality
)

# Claude Code SDK
try:
    from claude_code_sdk import query, ClaudeCodeOptions
    from claude_code_sdk._internal.transport.subprocess_cli import SubprocessCLITransport
    CLAUDE_SDK_AVAILABLE = True
except ImportError:
    CLAUDE_SDK_AVAILABLE = False
    logging.error("âŒ claude_code_sdk not available")

logger = logging.getLogger(__name__)

# Find Claude CLI path (needed for SubprocessCLITransport in poetry virtualenv)
def find_claude_cli():
    """Find Claude CLI in common locations since poetry subprocess may not have it in PATH."""
    # Try which() first
    cli_path = shutil.which("claude")
    if cli_path:
        return cli_path

    # Check common NVM locations
    import os
    home = os.path.expanduser("~")
    common_paths = [
        f"{home}/.nvm/versions/node/v22.19.0/bin/claude",
        f"{home}/.nvm/versions/node/v20.12.0/bin/claude",
        f"{home}/.nvm/versions/node/v18.18.0/bin/claude",
        "/usr/local/bin/claude",
        "/usr/bin/claude",
        f"{home}/.local/bin/claude",
        f"{home}/node_modules/.bin/claude",
    ]

    for path in common_paths:
        if Path(path).exists():
            return path

    return None

CLAUDE_CLI_PATH = find_claude_cli()
if not CLAUDE_CLI_PATH:
    logger.warning("âš ï¸  Claude CLI not found - persona execution may fail")
else:
    logger.info(f"âœ… Found Claude CLI at: {CLAUDE_CLI_PATH}")

# ============================================================================
# V4.1 DATA STRUCTURES
# ============================================================================

class PersonaReuseDecision:
    """Reuse decision for a single persona"""
    def __init__(
        self,
        persona_id: str,
        similarity_score: float,
        should_reuse: bool,
        source_project_id: Optional[str],
        rationale: str,
        match_details: Dict[str, Any]
    ):
        self.persona_id = persona_id
        self.similarity_score = similarity_score
        self.should_reuse = should_reuse
        self.source_project_id = source_project_id
        self.rationale = rationale
        self.match_details = match_details
        self.source_artifacts = []


class PersonaReuseMap:
    """Complete persona-level reuse map"""
    def __init__(
        self,
        overall_similarity: float,
        persona_decisions: Dict[str, PersonaReuseDecision],
        personas_to_reuse: List[str],
        personas_to_execute: List[str],
        time_savings_percent: float,
        cost_savings_dollars: float
    ):
        self.overall_similarity = overall_similarity
        self.persona_decisions = persona_decisions
        self.personas_to_reuse = personas_to_reuse
        self.personas_to_execute = personas_to_execute
        self.time_savings_percent = time_savings_percent
        self.cost_savings_dollars = cost_savings_dollars


# ============================================================================
# PERSONA EXECUTION CONTEXT
# ============================================================================

class PersonaExecutionContext:
    """Context for a single persona's execution"""

    def __init__(self, persona_id: str, requirement: str, output_dir: Path):
        self.persona_id = persona_id
        self.requirement = requirement
        self.output_dir = output_dir
        self.files_created = []
        self.deliverables = {}
        self.start_time = datetime.now()
        self.end_time = None
        self.success = False
        self.error = None
        self.reused = False  # NEW: Track if this was reused
        self.reuse_source = None  # NEW: Source project if reused
        self.quality_gate = None  # NEW: Quality gate validation results
        self.quality_issues = []  # NEW: List of quality issues found

    def mark_complete(self, success: bool = True, error: str = None):
        self.end_time = datetime.now()
        self.success = success
        self.error = error

    def mark_reused(self, source_project_id: str):
        """Mark this persona as reused (not executed)"""
        self.reused = True
        self.reuse_source = source_project_id
        self.success = True
        self.end_time = datetime.now()

    def duration(self) -> float:
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0

    def add_file(self, file_path: str):
        self.files_created.append(file_path)


# ============================================================================
# V4.1 PERSONA-LEVEL REUSE CLIENT
# ============================================================================

class PersonaReuseClient:
    """Client for ML Phase 3.1 persona-level reuse APIs"""

    def __init__(self, maestro_ml_url: str = "http://localhost:8001"):
        self.base_url = maestro_ml_url

    async def build_persona_reuse_map(
        self,
        new_requirements: str,
        existing_requirements: str,
        persona_ids: List[str]
    ) -> Optional[PersonaReuseMap]:
        """
        Build persona-level reuse map by calling ML Phase 3.1 API
        """
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/v1/ml/persona/build-reuse-map",
                    json={
                        "new_project_requirements": new_requirements,
                        "existing_project_requirements": existing_requirements,
                        "persona_ids": persona_ids
                    }
                )

                if response.status_code != 200:
                    logger.warning(f"Persona reuse map failed: {response.text}")
                    return None

                data = response.json()

                # Build PersonaReuseDecision objects
                persona_decisions = {}
                for persona_id, match_data in data["persona_matches"].items():
                    decision = PersonaReuseDecision(
                        persona_id=persona_id,
                        similarity_score=match_data["similarity_score"],
                        should_reuse=match_data["should_reuse"],
                        source_project_id=match_data.get("source_project_id"),
                        rationale=match_data["rationale"],
                        match_details=match_data["match_details"]
                    )
                    persona_decisions[persona_id] = decision

                return PersonaReuseMap(
                    overall_similarity=data["overall_similarity"],
                    persona_decisions=persona_decisions,
                    personas_to_reuse=data["personas_to_reuse"],
                    personas_to_execute=data["personas_to_execute"],
                    time_savings_percent=data["estimated_time_savings_percent"],
                    cost_savings_dollars=data.get("summary", {}).get("reuse_count", 0) * 22
                )

        except Exception as e:
            logger.error(f"Error building persona reuse map: {e}")
            return None

    async def find_similar_project(self, specs: Dict) -> Optional[Dict]:
        """Find most similar project"""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/v1/ml/find-similar-projects",
                    json={
                        "specs": specs,
                        "min_similarity": 0.50,  # Lower threshold for persona-level
                        "limit": 1
                    }
                )

                if response.status_code == 200:
                    data = response.json()
                    if data:
                        return data[0]  # Most similar

        except Exception as e:
            logger.error(f"Error finding similar project: {e}")

        return None

    async def fetch_persona_artifacts(
        self,
        source_project_id: str,
        persona_id: str
    ) -> List[str]:
        """Fetch artifacts for a specific persona from source project"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"{self.base_url}/api/v1/projects/{source_project_id}/artifacts",
                    params={"persona": persona_id}
                )

                if response.status_code == 200:
                    data = response.json()
                    return data.get("artifacts", [])

        except Exception as e:
            logger.warning(f"Could not fetch artifacts for {persona_id}: {e}")

        return []


# ============================================================================
# AUTONOMOUS SDLC ENGINE V3.1 (Persona-Level Reuse + Resumable)
# ============================================================================

class AutonomousSDLCEngineV3_1_Resumable:
    """
    V3.1 Engine with Persona-Level Intelligent Reuse + Resumable Sessions

    Key Features:
    1. Persona-level reuse analysis (V4.1)
    2. Session persistence and resume
    3. RAG integration for templates
    4. Quality review and validation
    5. Template creation from high-quality outputs
    """

    def __init__(
        self,
        selected_personas: List[str],
        output_dir: str = None,
        session_manager: SessionManager = None,
        maestro_ml_url: str = "http://localhost:8001",
        enable_persona_reuse: bool = True,
        force_rerun: bool = False
    ):
        if not CLAUDE_SDK_AVAILABLE:
            raise RuntimeError("claude_code_sdk is required")

        self.selected_personas = selected_personas
        self.output_dir = Path(output_dir or OUTPUT_CONFIG["default_output_dir"])
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Session manager
        self.session_manager = session_manager or SessionManager()

        # V4.1: Persona reuse client
        self.enable_persona_reuse = enable_persona_reuse

        # Force re-run of completed personas (for iterative improvements)
        self.force_rerun = force_rerun
        self.persona_reuse_client = PersonaReuseClient(maestro_ml_url)

        # Load persona configurations (will be loaded async in ensure_initialized)
        self.all_personas = {}
        self.persona_configs = {}
        self._initialized = False

        # Stats
        self.reuse_stats = {
            "personas_reused": 0,
            "personas_executed": 0,
            "time_saved_seconds": 0,
            "cost_saved_dollars": 0
        }

        # Project context for validation
        self.project_context = None

    async def ensure_initialized(self):
        """Ensure personas are loaded asynchronously"""
        if self._initialized:
            return

        # Load personas asynchronously
        try:
            from src.personas import get_adapter
            adapter = get_adapter()
            await adapter.ensure_loaded()
        except Exception as e:
            logger.warning(f"Could not load personas via adapter: {e}")

        self.all_personas = SDLCPersonas.get_all_personas()
        self.persona_configs = {
            pid: self.all_personas[pid]
            for pid in self.selected_personas
            if pid in self.all_personas
        }

        if not self.persona_configs:
            raise ValueError(f"No valid personas found in: {self.selected_personas}")

        self._initialized = True

    def _determine_execution_order(self, personas: List[str]) -> List[str]:
        """Determine optimal execution order"""
        priority_tiers = {
            "requirement_analyst": 1,
            "solution_architect": 2,
            "security_specialist": 3,
            "backend_developer": 4,
            "database_specialist": 4,
            "frontend_developer": 5,
            "ui_ux_designer": 5,
            "unit_tester": 6,
            "integration_tester": 7,
            "devops_engineer": 8,
            "technical_writer": 9
        }

        return sorted(personas, key=lambda p: priority_tiers.get(p, 999))

    async def execute(
        self,
        requirement: str,
        session_id: Optional[str] = None,
        resume_session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Execute SDLC workflow with persona-level reuse and session persistence

        NEW V3.1 Flow:
        1. Load/create session
        2. Check for similar projects (V4.1)
        3. Build persona-level reuse map
        4. For each persona:
           - If should_reuse: Fetch artifacts from similar project
           - If should_execute: Run persona with RAG + Quality review
        5. Save session and return results
        """

        # Ensure personas are loaded
        await self.ensure_initialized()

        # ===================================================================
        # STEP 1: Load or create session
        # ===================================================================
        if resume_session_id:
            session = self.session_manager.load_session(resume_session_id)
            if not session:
                raise ValueError(f"Session not found: {resume_session_id}")

            requirement = session.requirement
            self.output_dir = session.output_dir
            logger.info(f"ğŸ“‚ Resuming session: {session.session_id}")
            logger.info(f"âœ… Completed personas: {', '.join(session.completed_personas)}")

        else:
            session = self.session_manager.create_session(
                requirement=requirement,
                output_dir=self.output_dir,
                session_id=session_id
            )
            logger.info(f"ğŸ†• Created new session: {session.session_id}")

        # ===================================================================
        # STEP 2: Determine pending personas
        # ===================================================================
        if self.force_rerun:
            # Force mode: Re-run all selected personas regardless of completion status
            pending_personas = self.selected_personas
            logger.info(f"ğŸ”„ Force mode enabled - will re-run {len(pending_personas)} persona(s) even if completed")
        else:
            # Normal mode: Only run personas that haven't been completed
            pending_personas = [
                p for p in self.selected_personas
                if p not in session.completed_personas
            ]

        if not pending_personas:
            logger.info("âœ… All requested personas already completed!")
            return self._build_result(session, [], start_time=datetime.now())

        execution_order = self._determine_execution_order(pending_personas)

        # ===================================================================
        # STEP 3: NEW V3.1 - Persona-Level Reuse Analysis
        # ===================================================================
        reuse_map = None

        if self.enable_persona_reuse and "requirement_analyst" not in session.completed_personas:
            # First run - need to create requirements first
            logger.info("â„¹ï¸  First run - requirement_analyst will run first before reuse analysis")

        elif self.enable_persona_reuse:
            logger.info("\n" + "="*80)
            logger.info("ğŸ” V3.1: PERSONA-LEVEL REUSE ANALYSIS")
            logger.info("="*80)

            reuse_map = await self._analyze_persona_reuse(requirement, execution_order)

            if reuse_map:
                logger.info(f"ğŸ“Š Overall similarity: {reuse_map.overall_similarity:.1%}")
                logger.info(f"âš¡ Personas to REUSE: {len(reuse_map.personas_to_reuse)}")
                logger.info(f"ğŸ”¨ Personas to EXECUTE: {len(reuse_map.personas_to_execute)}")
                logger.info(f"â±ï¸  Time savings: {reuse_map.time_savings_percent:.1f}%")
                logger.info(f"ğŸ’° Cost savings: ${reuse_map.cost_savings_dollars:.2f}")

                # Log per-persona decisions
                for persona_id, decision in reuse_map.persona_decisions.items():
                    emoji = "âš¡ REUSE" if decision.should_reuse else "ğŸ”¨ EXECUTE"
                    logger.info(f"  {emoji} {persona_id}: {decision.similarity_score:.0%} - {decision.rationale[:80]}")

                logger.info("="*80 + "\n")

        # ===================================================================
        # STEP 4: Execute SDLC Workflow
        # ===================================================================
        logger.info("="*80)
        logger.info("ğŸš€ AUTONOMOUS SDLC ENGINE V3.1 - PERSONA-LEVEL REUSE")
        logger.info("="*80)
        logger.info(f"ğŸ“ Requirement: {requirement[:100]}...")
        logger.info(f"ğŸ†” Session: {session.session_id}")
        logger.info(f"â³ To process: {', '.join(execution_order)}")
        logger.info(f"ğŸ“ Output: {self.output_dir}")
        logger.info("="*80)

        start_time = datetime.now()

        for i, persona_id in enumerate(execution_order, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ¤– [{i}/{len(execution_order)}] Processing: {persona_id}")
            logger.info(f"{'='*80}")

            # Check if this persona should be reused
            should_reuse = (
                reuse_map and
                persona_id in reuse_map.personas_to_reuse and
                reuse_map.persona_decisions[persona_id].should_reuse
            )

            if should_reuse:
                # V3.1: REUSE artifacts from similar project
                persona_context = await self._reuse_persona_artifacts(
                    persona_id,
                    requirement,
                    session,
                    reuse_map.persona_decisions[persona_id]
                )
                self.reuse_stats["personas_reused"] += 1
                self.reuse_stats["cost_saved_dollars"] += 22  # $22 per persona

            else:
                # V3: EXECUTE persona normally (with RAG + Quality)
                persona_context = await self._execute_persona(
                    persona_id,
                    requirement,
                    session
                )
                self.reuse_stats["personas_executed"] += 1

            # ===================================================================
            # NEW V3.2: RUN QUALITY GATE (for executed personas only)
            # ===================================================================
            if persona_context.success and not persona_context.reused:
                quality_gate_result = await self._run_quality_gate(
                    persona_id,
                    persona_context
                )

                # Store quality gate results
                persona_context.quality_gate = quality_gate_result

                # Save validation report to disk
                self._save_validation_report(persona_id, quality_gate_result, persona_context)

                # If quality gate failed, mark persona as failed
                if not quality_gate_result["passed"]:
                    logger.warning(
                        f"âš ï¸  {persona_id} failed quality gate but continuing "
                        f"(can be retried later)"
                    )
                    # Store recommendations for later review
                    persona_context.quality_issues = quality_gate_result["recommendations"]

                    # NOTE: We don't mark as failed to avoid blocking,
                    # but we record the quality issues for visibility
                    # Option to enable strict mode: persona_context.success = False

            # Save persona execution to session
            session.add_persona_execution(
                persona_id=persona_id,
                files_created=persona_context.files_created,
                deliverables=persona_context.deliverables,
                duration=persona_context.duration(),
                success=persona_context.success
            )

            # Persist session after each persona
            self.session_manager.save_session(session)

            if persona_context.success:
                status = "âš¡ REUSED" if persona_context.reused else "âœ… EXECUTED"
                quality_suffix = ""
                if persona_context.quality_gate:
                    if persona_context.quality_gate["passed"]:
                        quality_suffix = " [Quality: âœ…]"
                    else:
                        quality_suffix = " [Quality: âš ï¸ Issues found]"
                logger.info(f"{status} {persona_id}: {len(persona_context.files_created)} files{quality_suffix}")
            else:
                logger.error(f"âŒ {persona_id} failed: {persona_context.error}")

        # ===================================================================
        # STEP 5: Generate final quality summary report
        # ===================================================================
        self._generate_final_quality_report(session, execution_order)

        # ===================================================================
        # STEP 6: Build and return result
        # ===================================================================
        return self._build_result(session, execution_order, start_time, reuse_map)

    async def _analyze_persona_reuse(
        self,
        requirement: str,
        pending_personas: List[str]
    ) -> Optional[PersonaReuseMap]:
        """
        NEW V3.1: Analyze persona-level reuse opportunities
        """
        try:
            # Read REQUIREMENTS.md from current session
            requirements_path = self.output_dir / "REQUIREMENTS.md"
            if not requirements_path.exists():
                logger.info("â„¹ï¸  No REQUIREMENTS.md found, skipping reuse analysis")
                return None

            new_requirements = requirements_path.read_text()

            # Find similar project
            logger.info("ğŸ” Searching for similar projects...")
            # For now, use a placeholder similar project
            # In production, this would call ML API to find similar project
            # and fetch its requirements

            # Placeholder: Assume we found a similar project
            # In reality, you'd call: similar_project = await self.persona_reuse_client.find_similar_project(specs)

            # For demo, return None if no similar project found
            # Real implementation would fetch existing project's requirements
            existing_requirements = "# Placeholder existing requirements"

            # Build persona reuse map
            reuse_map = await self.persona_reuse_client.build_persona_reuse_map(
                new_requirements=new_requirements,
                existing_requirements=existing_requirements,
                persona_ids=pending_personas
            )

            return reuse_map

        except Exception as e:
            logger.error(f"Error in persona reuse analysis: {e}")
            return None

    async def _reuse_persona_artifacts(
        self,
        persona_id: str,
        requirement: str,
        session: SDLCSession,
        reuse_decision: PersonaReuseDecision
    ) -> PersonaExecutionContext:
        """
        NEW V3.1: Reuse artifacts from similar project instead of executing

        This is where the magic happens - we skip execution entirely!
        """
        persona_context = PersonaExecutionContext(
            persona_id,
            requirement,
            self.output_dir
        )

        try:
            logger.info(f"âš¡ REUSING {persona_id} from project {reuse_decision.source_project_id}")
            logger.info(f"   Similarity: {reuse_decision.similarity_score:.0%}")
            logger.info(f"   Rationale: {reuse_decision.rationale}")

            # Fetch artifacts from source project
            artifacts = await self.persona_reuse_client.fetch_persona_artifacts(
                source_project_id=reuse_decision.source_project_id,
                persona_id=persona_id
            )

            if artifacts:
                logger.info(f"   ğŸ“¥ Fetched {len(artifacts)} artifacts")

                # Copy artifacts to current session
                for artifact_path in artifacts:
                    # In production, this would actually copy files
                    # For now, just track that they were reused
                    persona_context.add_file(artifact_path)

                persona_context.mark_reused(reuse_decision.source_project_id)
                logger.info(f"   âœ… Artifacts integrated into current session")

            else:
                logger.warning(f"   âš ï¸  No artifacts found, falling back to execution")
                # Fallback to execution
                return await self._execute_persona(persona_id, requirement, session)

        except Exception as e:
            logger.error(f"âŒ Error reusing artifacts for {persona_id}: {e}")
            # Fallback to execution
            return await self._execute_persona(persona_id, requirement, session)

        return persona_context

    async def _execute_persona(
        self,
        persona_id: str,
        requirement: str,
        session: SDLCSession
    ) -> PersonaExecutionContext:
        """
        V3.2: Execute a single persona with proper file tracking and quality validation
        """
        persona_context = PersonaExecutionContext(
            persona_id,
            requirement,
            self.output_dir
        )

        try:
            persona_config = self.persona_configs[persona_id]
            expected_deliverables = get_deliverables_for_persona(persona_id)

            # Build context from session history
            session_context = self.session_manager.get_session_context(session)
            prompt = self._build_persona_prompt(
                persona_config,
                requirement,
                expected_deliverables,
                session_context,
                persona_id  # Pass persona_id for context-specific instructions
            )

            options = ClaudeCodeOptions(
                system_prompt=persona_config["system_prompt"],
                model=CLAUDE_CONFIG["model"],
                cwd=str(self.output_dir),
                permission_mode=CLAUDE_CONFIG["permission_mode"]
            )

            logger.info(f"ğŸ¤– {persona_id} is working...")
            logger.info(f"ğŸ“¦ Expected deliverables: {', '.join(expected_deliverables[:5])}")

            # ===================================================================
            # NEW: Snapshot filesystem BEFORE execution
            # ===================================================================
            before_files = set(self.output_dir.rglob("*"))
            logger.debug(f"ğŸ“¸ Snapshot: {len(before_files)} files before execution")

            # Execute with Claude Code SDK
            # Create transport with explicit cli_path to work in poetry virtualenv
            transport = None
            if CLAUDE_CLI_PATH:
                transport = SubprocessCLITransport(
                    prompt=prompt,
                    options=options,
                    cli_path=CLAUDE_CLI_PATH
                )

            async for message in query(prompt=prompt, options=options, transport=transport):
                # Keep legacy tracking for backwards compatibility
                if hasattr(message, 'message_type') and message.message_type == 'tool_use':
                    if hasattr(message, 'name') and message.name == 'Write':
                        file_path = message.input.get('file_path') if hasattr(message, 'input') else None
                        if file_path:
                            logger.debug(f"  ğŸ“„ Creating: {file_path}")

            # ===================================================================
            # NEW: Snapshot filesystem AFTER execution
            # ===================================================================
            after_files = set(self.output_dir.rglob("*"))
            new_files = after_files - before_files

            # Filter to actual files (not directories) and make paths relative
            persona_context.files_created = [
                str(f.relative_to(self.output_dir))
                for f in new_files
                if f.is_file()
            ]

            logger.info(f"âœ… {persona_id} created {len(persona_context.files_created)} files")

            # ===================================================================
            # NEW: Map files to deliverables
            # ===================================================================
            persona_context.deliverables = self._map_files_to_deliverables(
                persona_id,
                expected_deliverables,
                persona_context.files_created
            )

            logger.info(
                f"ğŸ“¦ Deliverables: {len(persona_context.deliverables)}/{len(expected_deliverables)} "
                f"({len(persona_context.deliverables)/max(len(expected_deliverables), 1)*100:.0f}%)"
            )

            persona_context.mark_complete(success=True)

        except Exception as e:
            logger.exception(f"âŒ Error executing {persona_id}")
            persona_context.mark_complete(success=False, error=str(e))

        return persona_context

    def _map_files_to_deliverables(
        self,
        persona_id: str,
        expected_deliverables: List[str],
        files_created: List[str]
    ) -> Dict[str, List[str]]:
        """
        Map created files to expected deliverables using pattern matching

        Focus: Quality over quantity - match files to their purpose
        """
        import fnmatch

        # Comprehensive deliverable-to-file pattern mapping
        deliverable_patterns = {
            # Requirements Analyst
            "requirements_document": ["*requirements*.md", "REQUIREMENTS.md"],
            "user_stories": ["*user_stories*.md", "*stories*.md"],
            "acceptance_criteria": ["*acceptance*.md", "*criteria*.md"],
            "requirement_backlog": ["*backlog*.md"],

            # Solution Architect
            "architecture_document": ["*architecture*.md", "ARCHITECTURE.md"],
            "tech_stack": ["*tech_stack*.md", "*technology*.md"],
            "database_design": ["*database*.md", "*schema*.md", "*erd*.md"],
            "api_specifications": ["*api*.md", "*openapi*.yaml", "*swagger*.yaml", "*api*.yml"],
            "system_design": ["*system_design*.md", "*design*.md"],

            # Security Specialist
            "security_review": ["*security_review*.md", "*security*.md"],
            "threat_model": ["*threat*.md", "THREAT*.md"],
            "security_requirements": ["*security_requirements*.md"],
            "penetration_test_results": ["*pentest*.md", "*penetration*.md"],

            # Backend Developer
            "backend_code": ["backend/**/*.ts", "src/**/*.ts", "**/*service.ts", "**/*.py"],
            "api_implementation": ["**/routes/**/*.ts", "**/api/**/*.ts", "**/controllers/**/*.ts"],
            "database_schema": ["**/prisma/**/*.prisma", "**/migrations/**/*", "**/models/**/*"],
            "backend_tests": ["**/*.test.ts", "**/*.spec.ts", "**/*test*.py"],

            # Frontend Developer
            "frontend_code": ["frontend/**/*.tsx", "frontend/**/*.ts", "src/**/*.tsx", "src/**/*.jsx"],
            "components": ["**/components/**/*.tsx", "**/components/**/*.jsx"],
            "frontend_tests": ["**/*.test.tsx", "**/*.spec.tsx", "**/*.test.jsx"],
            "responsive_design": ["**/*.css", "**/*.scss", "**/styles/**/*", "**/*.module.css"],

            # DevOps Engineer
            "docker_config": ["Dockerfile*", "docker-compose*.yml", ".dockerignore", "**/*.dockerfile"],
            "ci_cd_pipeline": [".github/**/*.yml", ".gitlab-ci.yml", "Jenkinsfile", ".circleci/**/*"],
            "infrastructure_code": ["**/terraform/**/*", "**/k8s/**/*", "**/helm/**/*", "**/infra/**/*"],
            "deployment_scripts": ["**/scripts/deploy*", "**/scripts/setup*", "**/deploy.*"],

            # QA Engineer
            "test_plan": ["**/test_plan*.md", "**/testing/*test*.md", "**/*test*plan*.md"],
            "test_cases": ["**/test_cases*.md", "**/test_scenarios*.md"],
            "test_code": ["**/*test.ts", "**/*test.tsx", "**/*spec.ts", "**/*test*.py"],
            "test_report": ["**/test_report*.md", "**/test_results*.md", "**/completeness*.md"],
            "bug_reports": ["**/bugs*.md", "**/issues*.md"],

            # Technical Writer
            "readme": ["README.md", "**/README.md"],
            "api_documentation": ["**/api*.md", "**/docs/api*.md"],
            "user_guide": ["**/user*.md", "**/guide*.md"],
            "tutorials": ["**/tutorial*.md", "**/getting-started*.md", "**/quickstart*.md"],
            "architecture_diagrams": ["**/diagrams/**/*", "**/*diagram*.md"],

            # Deployment Specialist
            "deployment_guide": ["**/deployment*.md", "**/DEPLOYMENT*.md"],
            "rollback_procedures": ["**/rollback*.md"],
            "monitoring_setup": ["**/monitoring*.md", "**/observability*.md"],
            "release_notes": ["**/release*.md", "**/CHANGELOG*.md"],

            # Project Reviewer
            "project_maturity_report": ["**/reviews/*MATURITY*.md", "**/PROJECT_MATURITY*.md", "**/project_maturity_report.md"],
            "gap_analysis_report": ["**/reviews/*GAP*.md", "**/GAP_ANALYSIS*.md", "**/gap_analysis*.md"],
            "remediation_plan": ["**/reviews/*REMEDIATION*.md", "**/REMEDIATION*.md", "**/remediation_plan*.md"],
            "metrics_json": ["**/reviews/METRICS*.json", "**/PROJECT_METRICS.json", "**/metrics.json"],
            "final_quality_assessment": ["**/reviews/*QUALITY*.md", "**/FINAL_QUALITY*.md", "**/final_quality_assessment.md"],
        }

        deliverables_found = {}

        for deliverable_name in expected_deliverables:
            if deliverable_name not in deliverable_patterns:
                logger.debug(f"âš ï¸  No pattern defined for deliverable: {deliverable_name}")
                continue

            patterns = deliverable_patterns[deliverable_name]
            matched_files = []

            for file_path in files_created:
                for pattern in patterns:
                    if fnmatch.fnmatch(file_path, pattern):
                        matched_files.append(file_path)
                        break

            if matched_files:
                deliverables_found[deliverable_name] = matched_files
                logger.debug(f"  âœ“ {deliverable_name}: {len(matched_files)} files")

        return deliverables_found

    async def _run_quality_gate(
        self,
        persona_id: str,
        persona_context: PersonaExecutionContext
    ) -> Dict[str, Any]:
        """
        Run quality gate validation after persona execution

        Focus: Quality metrics, not file counts
        - Completeness (deliverables created)
        - Correctness (no stubs/placeholders)
        - Context-aware (only validate what's relevant)

        Returns:
            {
                "passed": bool,
                "validation_report": Dict,
                "recommendations": List[str]
            }
        """
        expected_deliverables = get_deliverables_for_persona(persona_id)

        logger.info(f"\nğŸ” Running Quality Gate for {persona_id}")
        logger.info("=" * 80)

        # Detect project context if not already done
        if not self.project_context:
            self.project_context = detect_project_type(self.output_dir)
            logger.info(f"ğŸ“‹ Project type detected: {self.project_context['type']}")

        # Validate deliverables with context awareness
        validation = validate_persona_deliverables(
            persona_id,
            expected_deliverables,
            persona_context.deliverables,
            self.output_dir,
            self.project_context
        )

        logger.info(f"ğŸ“Š Completeness: {validation['completeness_percentage']:.1f}%")
        logger.info(f"â­ Quality Score: {validation['quality_score']:.2f}")
        logger.info(f"ğŸ¯ Combined Score: {validation['combined_score']:.2f}")

        if validation["missing"]:
            logger.warning(f"âŒ Missing deliverables: {', '.join(validation['missing'])}")

        if validation["partial"]:
            logger.warning(f"âš ï¸  Partial/stub deliverables: {', '.join(validation['partial'])}")

        # Log quality issues
        if validation["quality_issues"]:
            logger.warning(f"\nâš ï¸  Quality Issues Found: {len(validation['quality_issues'])}")
            for issue in validation["quality_issues"][:5]:  # Show top 5
                logger.warning(f"   ğŸ“„ {issue['file']} ({issue['severity']})")
                for problem in issue["issues"][:2]:  # Show top 2 issues per file
                    logger.warning(f"      - {problem}")

        # Determine if quality gate passed
        # Adjusted thresholds (less strict than original to avoid false positives)
        passed = (
            validation["completeness_percentage"] >= 70.0 and  # 70% completeness
            validation["quality_score"] >= 0.60 and            # 60% quality
            len([i for i in validation["quality_issues"] if i.get("severity") == "critical"]) == 0
        )

        # Generate recommendations
        recommendations = []

        if validation["completeness_percentage"] < 70:
            recommendations.append(
                f"Increase completeness from {validation['completeness_percentage']:.1f}% to â‰¥70%"
            )

        if validation["quality_score"] < 0.60:
            recommendations.append(
                f"Improve quality score from {validation['quality_score']:.2f} to â‰¥0.60"
            )

        if validation["missing"]:
            missing_list = validation['missing'][:3]
            recommendations.append(
                f"Create missing deliverables: {', '.join(missing_list)}"
            )

        if validation["partial"]:
            partial_list = validation['partial'][:3]
            recommendations.append(
                f"Complete stub implementations: {', '.join(partial_list)}"
            )

        # Persona-specific checks
        if persona_id == "qa_engineer":
            # QA must produce test results or completeness reports
            has_validation_evidence = any(
                "result" in f or "report" in f or "completeness" in f
                for f in persona_context.files_created
            )

            if not has_validation_evidence:
                passed = False
                recommendations.append(
                    "QA Engineer must produce validation evidence (test results, completeness reports)"
                )

        if persona_id in ["backend_developer", "frontend_developer"]:
            # Check for excessive critical issues
            critical_issues = [
                i for i in validation["quality_issues"]
                if i.get("severity") in ["critical", "high"]
            ]

            if len(critical_issues) > 3:
                passed = False
                recommendations.append(
                    f"Fix {len(critical_issues)} critical/high issues before proceeding"
                )

        # Log result
        logger.info("\n" + "=" * 80)
        if passed:
            logger.info(f"âœ… Quality Gate PASSED for {persona_id}")
        else:
            logger.warning(f"âš ï¸  Quality Gate FAILED for {persona_id}")
            logger.warning("ğŸ“‹ Recommendations:")
            for rec in recommendations:
                logger.warning(f"   - {rec}")
        logger.info("=" * 80 + "\n")

        return {
            "passed": passed,
            "validation_report": validation,
            "recommendations": recommendations
        }

    def _save_validation_report(
        self,
        persona_id: str,
        quality_gate_result: Dict[str, Any],
        persona_context: PersonaExecutionContext
    ):
        """
        Save validation report to disk for post-mortem analysis

        Creates:
        - validation_reports/{persona_id}_validation.json
        - validation_reports/summary.json (overall summary)
        """
        try:
            # Create validation_reports directory
            reports_dir = self.output_dir / "validation_reports"
            reports_dir.mkdir(exist_ok=True)

            # Build detailed report
            report = {
                "persona_id": persona_id,
                "timestamp": datetime.now().isoformat(),
                "success": persona_context.success,
                "reused": persona_context.reused,
                "files_created": persona_context.files_created,
                "files_count": len(persona_context.files_created),
                "deliverables": {
                    name: files for name, files in persona_context.deliverables.items()
                },
                "deliverables_count": len(persona_context.deliverables),
                "duration_seconds": persona_context.duration(),
                "quality_gate": {
                    "passed": quality_gate_result["passed"],
                    "completeness_percentage": quality_gate_result["validation_report"]["completeness_percentage"],
                    "quality_score": quality_gate_result["validation_report"]["quality_score"],
                    "combined_score": quality_gate_result["validation_report"].get("combined_score", 0.0),
                    "missing_deliverables": quality_gate_result["validation_report"]["missing"],
                    "partial_deliverables": quality_gate_result["validation_report"]["partial"],
                    "quality_issues_count": len(quality_gate_result["validation_report"]["quality_issues"]),
                    "quality_issues": quality_gate_result["validation_report"]["quality_issues"],
                    "recommendations": quality_gate_result["recommendations"]
                }
            }

            # Save persona-specific report
            report_file = reports_dir / f"{persona_id}_validation.json"
            report_file.write_text(json.dumps(report, indent=2))
            logger.debug(f"ğŸ’¾ Saved validation report: {report_file}")

            # Update summary report
            summary_file = reports_dir / "summary.json"
            if summary_file.exists():
                summary = json.loads(summary_file.read_text())
            else:
                summary = {
                    "session_id": self.session_manager.current_session.session_id if hasattr(self.session_manager, 'current_session') else "unknown",
                    "created_at": datetime.now().isoformat(),
                    "personas": {},
                    "overall_stats": {
                        "total_personas": 0,
                        "passed_quality_gates": 0,
                        "failed_quality_gates": 0,
                        "avg_completeness": 0.0,
                        "avg_quality": 0.0,
                        "total_issues": 0
                    }
                }

            # Add persona to summary
            summary["personas"][persona_id] = {
                "passed": quality_gate_result["passed"],
                "completeness": quality_gate_result["validation_report"]["completeness_percentage"],
                "quality": quality_gate_result["validation_report"]["quality_score"],
                "issues": len(quality_gate_result["validation_report"]["quality_issues"])
            }

            # Recalculate overall stats
            total = len(summary["personas"])
            passed = sum(1 for p in summary["personas"].values() if p["passed"])
            failed = total - passed
            avg_completeness = sum(p["completeness"] for p in summary["personas"].values()) / max(total, 1)
            avg_quality = sum(p["quality"] for p in summary["personas"].values()) / max(total, 1)
            total_issues = sum(p["issues"] for p in summary["personas"].values())

            summary["overall_stats"] = {
                "total_personas": total,
                "passed_quality_gates": passed,
                "failed_quality_gates": failed,
                "avg_completeness": round(avg_completeness, 2),
                "avg_quality": round(avg_quality, 2),
                "total_issues": total_issues
            }

            summary["last_updated"] = datetime.now().isoformat()

            summary_file.write_text(json.dumps(summary, indent=2))
            logger.debug(f"ğŸ’¾ Updated summary report: {summary_file}")

        except Exception as e:
            logger.warning(f"âš ï¸  Could not save validation report: {e}")

    def _generate_final_quality_report(
        self,
        session: SDLCSession,
        execution_order: List[str]
    ):
        """
        Generate final quality summary report for the entire workflow

        Creates:
        - validation_reports/FINAL_QUALITY_REPORT.md (human-readable)
        """
        try:
            reports_dir = self.output_dir / "validation_reports"
            if not reports_dir.exists():
                return  # No validation reports generated

            # Read summary.json
            summary_file = reports_dir / "summary.json"
            if not summary_file.exists():
                return

            summary = json.loads(summary_file.read_text())

            # Generate markdown report
            report_lines = [
                "# Final Quality Validation Report",
                "",
                f"**Session ID:** {session.session_id}",
                f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                f"**Total Personas Executed:** {summary['overall_stats']['total_personas']}",
                "",
                "---",
                "",
                "## Overall Quality Metrics",
                "",
                f"- **Quality Gates Passed:** {summary['overall_stats']['passed_quality_gates']} / {summary['overall_stats']['total_personas']}",
                f"- **Quality Gates Failed:** {summary['overall_stats']['failed_quality_gates']} / {summary['overall_stats']['total_personas']}",
                f"- **Average Completeness:** {summary['overall_stats']['avg_completeness']:.1f}%",
                f"- **Average Quality Score:** {summary['overall_stats']['avg_quality']:.2f}",
                f"- **Total Quality Issues:** {summary['overall_stats']['total_issues']}",
                "",
            ]

            # Overall status
            all_passed = summary['overall_stats']['failed_quality_gates'] == 0
            if all_passed:
                report_lines.extend([
                    "## âœ… Overall Status: PASSED",
                    "",
                    "All personas passed their quality gates. Project is ready for deployment.",
                    ""
                ])
            else:
                report_lines.extend([
                    "## âš ï¸ Overall Status: NEEDS ATTENTION",
                    "",
                    f"{summary['overall_stats']['failed_quality_gates']} persona(s) failed quality gates.",
                    "Review individual reports below and address issues before deployment.",
                    ""
                ])

            report_lines.append("---")
            report_lines.append("")
            report_lines.append("## Persona Quality Reports")
            report_lines.append("")

            # Per-persona breakdown
            for persona_id in execution_order:
                if persona_id not in summary["personas"]:
                    continue

                persona_data = summary["personas"][persona_id]
                status_icon = "âœ…" if persona_data["passed"] else "âš ï¸"

                report_lines.extend([
                    f"### {status_icon} {persona_id}",
                    "",
                    f"- **Quality Gate:** {'PASSED' if persona_data['passed'] else 'FAILED'}",
                    f"- **Completeness:** {persona_data['completeness']:.1f}%",
                    f"- **Quality Score:** {persona_data['quality']:.2f}",
                    f"- **Issues Found:** {persona_data['issues']}",
                    ""
                ])

                # Load detailed report for recommendations
                detail_file = reports_dir / f"{persona_id}_validation.json"
                if detail_file.exists():
                    detail = json.loads(detail_file.read_text())

                    if detail["quality_gate"]["recommendations"]:
                        report_lines.append("**Recommendations:**")
                        for rec in detail["quality_gate"]["recommendations"]:
                            report_lines.append(f"- {rec}")
                        report_lines.append("")

                    if detail["quality_gate"]["missing_deliverables"]:
                        report_lines.append(f"**Missing Deliverables:** {', '.join(detail['quality_gate']['missing_deliverables'])}")
                        report_lines.append("")

                    if detail["quality_gate"]["partial_deliverables"]:
                        report_lines.append(f"**Partial/Stub Deliverables:** {', '.join(detail['quality_gate']['partial_deliverables'])}")
                        report_lines.append("")

            # Add recommendations section
            report_lines.extend([
                "---",
                "",
                "## Next Steps",
                ""
            ])

            if all_passed:
                report_lines.extend([
                    "1. âœ… All quality gates passed",
                    "2. âœ… Review individual validation reports in `validation_reports/` directory",
                    "3. âœ… Proceed with deployment",
                    ""
                ])
            else:
                report_lines.extend([
                    "1. âš ï¸ Review failed personas above",
                    "2. âš ï¸ Address quality issues and recommendations",
                    "3. âš ï¸ Re-run failed personas:",
                    "   ```bash",
                    f"   python team_execution.py <failed_personas> --resume {session.session_id}",
                    "   ```",
                    "4. âš ï¸ Verify all quality gates pass before deployment",
                    ""
                ])

            # Footer
            report_lines.extend([
                "---",
                "",
                "## Report Files",
                "",
                "- `summary.json` - Overall statistics",
                "- `{persona_id}_validation.json` - Detailed per-persona reports",
                "- `FINAL_QUALITY_REPORT.md` - This file",
                "",
                "---",
                "",
                f"*Report generated by Quality Validation System v3.2 on {datetime.now().isoformat()}*"
            ])

            # Write report
            final_report = reports_dir / "FINAL_QUALITY_REPORT.md"
            final_report.write_text("\n".join(report_lines))

            logger.info(f"\nğŸ“Š Final quality report: {final_report}")

        except Exception as e:
            logger.warning(f"âš ï¸  Could not generate final quality report: {e}")

    def _build_persona_prompt(
        self,
        persona_config: Dict[str, Any],
        requirement: str,
        expected_deliverables: List[str],
        session_context: str,
        persona_id: Optional[str] = None
    ) -> str:
        """Build prompt with session context and validation instructions"""
        persona_name = persona_config["name"]
        expertise = persona_config.get("expertise", [])

        prompt = f"""You are the {persona_name} for this project.

SESSION CONTEXT (work already done):
{session_context}

Your task is to build on the existing work and create your deliverables.

Your expertise areas:
{chr(10).join(f"- {exp}" for exp in expertise[:5])}

Expected deliverables for your role:
{chr(10).join(f"- {d}" for d in expected_deliverables)}

Using the Claude Code tools (Write, Edit, Read, Bash, WebSearch):
1. Review the work done by previous personas (check existing files)
2. Build on their work - don't duplicate, extend and enhance
3. Create your deliverables using best practices
4. Ensure consistency with existing files

Work autonomously. Focus on your specialized domain.

Output directory: {self.output_dir}
"""

        # Add persona-specific validation instructions
        if persona_id == "qa_engineer":
            prompt += """

================================================================================
CRITICAL: QA VALIDATION RESPONSIBILITIES
================================================================================

You are the QUALITY GATEKEEPER. Your primary job is VALIDATION, not just test creation.

MANDATORY STEPS:

1. VERIFY IMPLEMENTATION COMPLETENESS
   - Read requirements document
   - List ALL expected features
   - Check backend routes: grep -r "router\\.(get|post|put)" backend/src/routes/
   - Check for commented routes: grep -r "// router\\.use" backend/
   - Check frontend pages: find frontend/src/pages -name "*.tsx"
   - Check for stubs: grep -ri "coming soon\\|placeholder\\|TODO" .

2. CREATE COMPLETENESS REPORT
   Create a file: completeness_report.md with:
   ```
   # Implementation Completeness Report

   ## Expected Features (from requirements)
   - Feature 1: âœ… Implemented | âš ï¸ Partial | âŒ Missing
   - Feature 2: ...

   ## Backend API Endpoints
   - POST /api/auth/login: âœ… Implemented
   - GET /api/workspaces: âŒ Commented out
   ...

   ## Frontend Pages
   - /login: âœ… Fully implemented
   - /workspace: âš ï¸ "Coming Soon" stub
   ...

   ## Summary
   - Completeness: XX%
   - Quality Gate: PASS/FAIL
   ```

3. RUN ACTUAL TESTS (not just create test plans)
   - cd backend && npm test (if exists)
   - cd frontend && npm test (if exists)
   - Capture results in test_results.md

4. QUALITY DECISION
   - If completeness < 80%: FAIL and document gaps
   - If critical features missing: FAIL
   - If stubs/placeholders: FAIL

Your deliverables MUST include:
- test_plan.md
- completeness_report.md â† CRITICAL
- test_results.md (actual results, not plans)

"""

        elif persona_id == "deployment_integration_tester":
            prompt += """

================================================================================
CRITICAL: PRE-DEPLOYMENT VALIDATION
================================================================================

Before approving deployment, you MUST verify:

1. SMOKE TESTS
   - Check backend starts: grep -r "export\\|app\\.listen" backend/src/
   - Check routes not commented: grep -c "// router\\.use" backend/src/routes/
   - If count > 0: FAIL

2. CREATE DEPLOYMENT READINESS REPORT
   File: deployment_readiness_report.md
   ```
   # Deployment Readiness Report

   ## Smoke Tests
   - âœ…/âŒ Backend server configuration
   - âœ…/âŒ No commented-out routes
   - âœ…/âŒ Frontend builds successfully

   ## Critical Issues
   - List any blockers

   ## Decision: GO / NO-GO
   ```

3. DO NOT APPROVE if:
   - Routes are commented out
   - "Coming Soon" pages exist
   - Major features missing

"""

        elif persona_id in ["backend_developer", "frontend_developer"]:
            prompt += """

================================================================================
IMPLEMENTATION QUALITY STANDARDS
================================================================================

Your work will be validated. Requirements:

1. NO STUBS
   - No "Coming Soon" text
   - No commented-out routes
   - No empty functions
   - No excessive TODOs (max 2-3 is OK)

2. COMPLETE FEATURES
   - All routes from architecture spec
   - All pages functional (no placeholders)

3. ERROR HANDLING
   - Try-catch blocks for async operations
   - Input validation
   - Meaningful error messages

"""

        return prompt

    def _build_result(
        self,
        session: SDLCSession,
        execution_order: List[str],
        start_time: datetime,
        reuse_map: Optional[PersonaReuseMap] = None
    ) -> Dict[str, Any]:
        """Build final result with V3.1 stats"""
        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds()

        result = {
            "success": True,
            "session_id": session.session_id,
            "requirement": session.requirement,
            "executed_personas": execution_order,
            "all_completed_personas": session.completed_personas,
            "files": session.get_all_files(),
            "file_count": len(session.get_all_files()),
            "project_dir": str(self.output_dir),
            "total_duration": total_duration,
            "resumable": True,

            # NEW V3.1: Reuse statistics
            "persona_reuse_enabled": self.enable_persona_reuse,
            "reuse_stats": {
                "personas_reused": self.reuse_stats["personas_reused"],
                "personas_executed": self.reuse_stats["personas_executed"],
                "cost_saved_dollars": self.reuse_stats["cost_saved_dollars"],
                "time_saved_percent": reuse_map.time_savings_percent if reuse_map else 0
            }
        }

        if reuse_map:
            result["persona_reuse_map"] = {
                "overall_similarity": reuse_map.overall_similarity,
                "personas_reused": reuse_map.personas_to_reuse,
                "personas_executed": reuse_map.personas_to_execute,
                "persona_decisions": {
                    pid: {
                        "similarity_score": decision.similarity_score,
                        "should_reuse": decision.should_reuse,
                        "rationale": decision.rationale
                    }
                    for pid, decision in reuse_map.persona_decisions.items()
                }
            }

        # Log summary
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š EXECUTION SUMMARY - V3.1")
        logger.info("="*80)
        logger.info(f"âœ… Success: {result['success']}")
        logger.info(f"ğŸ†” Session: {result['session_id']}")
        logger.info(f"ğŸ‘¥ Executed in this run: {len(execution_order)}")
        logger.info(f"ğŸ‘¥ Total completed: {len(session.completed_personas)}")
        logger.info(f"ğŸ“ Files created: {result['file_count']}")
        logger.info(f"â±ï¸  Duration: {total_duration:.2f}s")

        if reuse_map:
            logger.info(f"\nğŸ¯ V3.1 PERSONA-LEVEL REUSE:")
            logger.info(f"   âš¡ Personas reused: {self.reuse_stats['personas_reused']}")
            logger.info(f"   ğŸ”¨ Personas executed: {self.reuse_stats['personas_executed']}")
            logger.info(f"   ğŸ’° Cost saved: ${self.reuse_stats['cost_saved_dollars']:.2f}")
            logger.info(f"   â±ï¸  Time saved: {reuse_map.time_savings_percent:.1f}%")

        logger.info(f"\nğŸ“‚ Output: {result['project_dir']}")
        logger.info(f"\nğŸ’¡ Resume command:")
        logger.info(f"   python {Path(__file__).name} <new_personas> --resume {session.session_id}")
        logger.info("="*80)

        return result


# ============================================================================
# SESSION MANAGEMENT HELPERS
# ============================================================================

def list_sessions(session_manager: SessionManager):
    """List all available sessions"""
    sessions = session_manager.list_sessions()

    if not sessions:
        print("No sessions found.")
        return

    print("\n" + "="*80)
    print("ğŸ“‹ AVAILABLE SESSIONS")
    print("="*80)

    for i, session in enumerate(sessions, 1):
        print(f"\n{i}. Session: {session['session_id']}")
        print(f"   Requirement: {session['requirement']}")
        print(f"   Created: {session['created_at']}")
        print(f"   Last Updated: {session['last_updated']}")
        print(f"   Completed Personas: {session['completed_personas']}")
        print(f"   Files: {session['files_count']}")

    print("\n" + "="*80)
    print("ğŸ’¡ Resume a session:")
    print(f"   python {Path(__file__).name} <personas> --resume <session_id>")
    print("="*80 + "\n")


async def main():
    """CLI entry point"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Autonomous SDLC Engine V3.1 - Persona-Level Reuse + Resumable",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # New session with V3.1 persona-level reuse
  python autonomous_sdlc_engine_v3_1_resumable.py requirement_analyst backend_developer \\
      --requirement "Create blog platform" \\
      --session-id blog_v1

  # Resume existing session
  python autonomous_sdlc_engine_v3_1_resumable.py frontend_developer \\
      --resume blog_v1

  # Disable persona-level reuse (V3 mode)
  python autonomous_sdlc_engine_v3_1_resumable.py requirement_analyst \\
      --requirement "Create e-commerce" \\
      --disable-persona-reuse

  # List sessions
  python autonomous_sdlc_engine_v3_1_resumable.py --list-sessions
        """
    )

    parser.add_argument("personas", nargs="*", help="Personas to execute")
    parser.add_argument("--requirement", help="Project requirement (for new sessions)")
    parser.add_argument("--session-id", help="Session ID for new session")
    parser.add_argument("--resume", help="Resume existing session ID")
    parser.add_argument("--list-sessions", action="store_true", help="List all sessions")
    parser.add_argument("--output", help="Output directory")
    parser.add_argument("--maestro-ml-url", default="http://localhost:8001", help="Maestro ML URL")
    parser.add_argument("--disable-persona-reuse", action="store_true", help="Disable V3.1 persona-level reuse")
    parser.add_argument("--force", action="store_true", help="Force re-execution of completed personas (for iterative improvements)")

    args = parser.parse_args()

    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )

    session_manager = SessionManager()

    # List sessions
    if args.list_sessions:
        list_sessions(session_manager)
        return

    # Validate arguments
    if not args.resume and not args.requirement:
        parser.error("--requirement is required for new sessions")

    if not args.personas and not args.resume:
        parser.error("Specify personas to execute")

    # Create engine
    engine = AutonomousSDLCEngineV3_1_Resumable(
        selected_personas=args.personas if args.personas else [],
        output_dir=args.output,
        session_manager=session_manager,
        maestro_ml_url=args.maestro_ml_url,
        enable_persona_reuse=not args.disable_persona_reuse,
        force_rerun=args.force
    )

    # Execute
    result = await engine.execute(
        requirement=args.requirement,
        session_id=args.session_id,
        resume_session_id=args.resume
    )

    # Print result
    print("\n" + "="*80)
    print("âœ… EXECUTION COMPLETE")
    print("="*80)
    print(f"Session ID: {result['session_id']}")
    print(f"Files created: {result['file_count']}")
    print(f"Output directory: {result['project_dir']}")

    if result.get("persona_reuse_enabled"):
        print(f"\nâš¡ V3.1 Persona-Level Reuse:")
        print(f"   Reused: {result['reuse_stats']['personas_reused']} personas")
        print(f"   Executed: {result['reuse_stats']['personas_executed']} personas")
        print(f"   Cost saved: ${result['reuse_stats']['cost_saved_dollars']:.2f}")

    print("="*80)


if __name__ == "__main__":
    asyncio.run(main())
