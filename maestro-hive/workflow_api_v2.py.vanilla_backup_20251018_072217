"""
Workflow API Service V2 - Frontend Coordination
Clear execution tracking with phase-by-phase status updates.

Key Identifiers:
- execution_id: Unique ID for each workflow run
- workflow_id: Workflow definition ID
- node_id: Specific phase node ID (from blueprint)
- phase_type: Type of phase (requirements, architecture, etc.)

Message Flow:
1. Frontend â†’ POST /api/workflow/execute â†’ Backend
2. Backend â†’ Immediate response with execution_id
3. Backend â†’ WebSocket: execution_accepted
4. Backend â†’ WebSocket: phase_started (Phase 1)
5. Backend â†’ [10 seconds processing]
6. Backend â†’ WebSocket: phase_completed (Phase 1)
7. Backend â†’ WebSocket: phase_started (Phase 2)
8. Backend â†’ [10 seconds processing]
9. Backend â†’ WebSocket: phase_completed (Phase 2)
... continues for all phases
10. Backend â†’ WebSocket: workflow_completed
"""

import asyncio
import logging
import os
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional

from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Data Models
# ---------------------------------------------------------------------------

class PhaseConfig(BaseModel):
    """Configuration for a workflow phase"""
    requirementText: Optional[str] = None
    architectureDesign: Optional[str] = None
    implementationDetails: Optional[str] = None
    testPlan: Optional[str] = None
    deploymentConfig: Optional[str] = None
    reviewCriteria: Optional[str] = None
    assigned_team: List[str] = Field(default_factory=list)
    executor_ai: Optional[str] = "claude-3-5-sonnet-20241022"


class WorkflowNode(BaseModel):
    """DAG workflow node definition"""
    id: str  # node_id - critical for frontend tracking
    phase_type: str  # requirements, architecture, design, etc.
    label: str  # Display name
    phase_config: PhaseConfig


class WorkflowEdge(BaseModel):
    """DAG workflow edge (dependency)"""
    source: str  # source node_id
    target: str  # target node_id


class DAGWorkflowExecute(BaseModel):
    """DAG workflow execution request"""
    workflow_id: str
    workflow_name: str
    nodes: List[WorkflowNode]
    edges: List[WorkflowEdge] = Field(default_factory=list)


# ---------------------------------------------------------------------------
# WebSocket Connection Manager
# ---------------------------------------------------------------------------

class ConnectionManager:
    """Manages WebSocket connections for workflow updates"""

    def __init__(self):
        self.active_connections: Dict[str, List[WebSocket]] = {}

    async def connect(self, websocket: WebSocket, workflow_id: str):
        """Accept and track WebSocket connection"""
        await websocket.accept()
        if workflow_id not in self.active_connections:
            self.active_connections[workflow_id] = []
        self.active_connections[workflow_id].append(websocket)
        logger.info(f"ðŸ“¡ WebSocket connected for workflow: {workflow_id} (total: {len(self.active_connections[workflow_id])})")

    def disconnect(self, websocket: WebSocket, workflow_id: str):
        """Remove WebSocket connection"""
        if workflow_id in self.active_connections:
            if websocket in self.active_connections[workflow_id]:
                self.active_connections[workflow_id].remove(websocket)
            if not self.active_connections[workflow_id]:
                del self.active_connections[workflow_id]
        logger.info(f"ðŸ“¡ WebSocket disconnected for workflow: {workflow_id}")

    async def broadcast(self, workflow_id: str, message: dict):
        """Broadcast message to all connected clients for this workflow"""
        if workflow_id in self.active_connections:
            logger.info(f"ðŸ“¤ Broadcasting to {len(self.active_connections[workflow_id])} clients: {message.get('type')}")
            for connection in self.active_connections[workflow_id]:
                try:
                    await connection.send_json(message)
                except Exception as e:
                    logger.error(f"Error broadcasting to WebSocket: {e}")


# ---------------------------------------------------------------------------
# In-Memory Execution Store
# ---------------------------------------------------------------------------

class ExecutionStore:
    """Simple in-memory store for execution status"""

    def __init__(self):
        self.executions: Dict[str, Dict[str, Any]] = {}
        self.phase_status: Dict[str, Dict[str, str]] = {}  # execution_id -> {node_id: status}

    def create(self, execution_id: str, workflow_id: str, workflow_name: str, nodes: List[WorkflowNode]) -> Dict[str, Any]:
        """Create new execution record"""
        execution = {
            'execution_id': execution_id,
            'workflow_id': workflow_id,
            'workflow_name': workflow_name,
            'status': 'pending',
            'total_phases': len(nodes),
            'completed_phases': 0,
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat(),
            'started_at': None,
            'completed_at': None,
            'error': None,
            'phases': [
                {
                    'node_id': node.id,
                    'phase_type': node.phase_type,
                    'label': node.label,
                    'status': 'pending',
                    'started_at': None,
                    'completed_at': None
                }
                for node in nodes
            ]
        }
        self.executions[execution_id] = execution

        # Initialize phase status tracking
        self.phase_status[execution_id] = {node.id: 'pending' for node in nodes}

        logger.info(f"ðŸ“ Created execution: {execution_id} with {len(nodes)} phases")
        return execution

    def update_status(self, execution_id: str, status: str, error: Optional[str] = None):
        """Update execution status"""
        if execution_id in self.executions:
            self.executions[execution_id]['status'] = status
            self.executions[execution_id]['updated_at'] = datetime.now().isoformat()

            if status == 'running' and not self.executions[execution_id]['started_at']:
                self.executions[execution_id]['started_at'] = datetime.now().isoformat()
            elif status in ['completed', 'failed', 'cancelled']:
                self.executions[execution_id]['completed_at'] = datetime.now().isoformat()

            if error:
                self.executions[execution_id]['error'] = error

    def update_phase_status(self, execution_id: str, node_id: str, status: str):
        """Update specific phase status"""
        if execution_id in self.executions:
            # Update phase status tracking
            if execution_id in self.phase_status:
                self.phase_status[execution_id][node_id] = status

            # Update phase in execution record
            for phase in self.executions[execution_id]['phases']:
                if phase['node_id'] == node_id:
                    phase['status'] = status
                    phase['updated_at'] = datetime.now().isoformat()

                    if status == 'running' and not phase['started_at']:
                        phase['started_at'] = datetime.now().isoformat()
                    elif status in ['completed', 'failed']:
                        phase['completed_at'] = datetime.now().isoformat()

                        if status == 'completed':
                            self.executions[execution_id]['completed_phases'] += 1

                    break

            logger.info(f"ðŸ“ Updated phase {node_id} in execution {execution_id}: {status}")

    def get(self, execution_id: str) -> Optional[Dict[str, Any]]:
        """Get execution record"""
        return self.executions.get(execution_id)


# ---------------------------------------------------------------------------
# FastAPI Application
# ---------------------------------------------------------------------------

app = FastAPI(
    title="Workflow API V2",
    description="Workflow execution service with clear frontend coordination",
    version="2.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global instances
manager = ConnectionManager()
execution_store = ExecutionStore()
background_tasks = set()


# ---------------------------------------------------------------------------
# Artifact Generation
# ---------------------------------------------------------------------------

def generate_dummy_artifact(execution_id: str, node_id: str, phase_type: str, phase_number: int) -> Dict[str, Any]:
    """
    Generate a dummy artifact (markdown file) for the completed phase.

    Returns artifact metadata including file path for download.
    """
    # Create artifacts directory if it doesn't exist
    artifacts_dir = "/tmp/workflow_artifacts"
    os.makedirs(artifacts_dir, exist_ok=True)

    # Generate artifact filename
    artifact_filename = f"{execution_id}_{node_id}_{phase_type}.md"
    artifact_path = os.path.join(artifacts_dir, artifact_filename)

    # Generate dummy markdown content based on phase type
    content_templates = {
        'requirements': f"""# Requirements Document
## Phase: {phase_type}
## Node ID: {node_id}
## Execution: {execution_id}

### Overview
This is a dummy requirements document generated for phase {phase_number}.

### Functional Requirements
1. **User Authentication**: System shall provide secure user authentication
2. **Data Management**: System shall enable CRUD operations on core entities
3. **Reporting**: System shall generate real-time analytics reports

### Non-Functional Requirements
1. **Performance**: Response time < 200ms for 95% of requests
2. **Scalability**: Support up to 10,000 concurrent users
3. **Security**: Implement encryption for data at rest and in transit

### Acceptance Criteria
- [ ] All functional requirements implemented
- [ ] Performance benchmarks met
- [ ] Security audit passed

Generated: {datetime.now().isoformat()}
""",
        'architecture': f"""# Architecture Design Document
## Phase: {phase_type}
## Node ID: {node_id}
## Execution: {execution_id}

### System Architecture
This is a dummy architecture document generated for phase {phase_number}.

### Components
1. **Frontend Layer**: React-based SPA with TypeScript
2. **API Gateway**: Kong/NGINX for request routing
3. **Backend Services**: Microservices architecture
4. **Database Layer**: PostgreSQL + Redis cache

### Technology Stack
- **Frontend**: React 18, TypeScript, TailwindCSS
- **Backend**: Python FastAPI, Node.js
- **Database**: PostgreSQL 15, Redis 7
- **Infrastructure**: Docker, Kubernetes

### Deployment Architecture
```
[Load Balancer]
      â†“
[API Gateway]
      â†“
[Microservices] â† â†’ [Database Cluster]
```

Generated: {datetime.now().isoformat()}
""",
        'implementation': f"""# Implementation Documentation
## Phase: {phase_type}
## Node ID: {node_id}
## Execution: {execution_id}

### Implementation Summary
This is a dummy implementation document generated for phase {phase_number}.

### Code Structure
```
src/
â”œâ”€â”€ components/
â”œâ”€â”€ services/
â”œâ”€â”€ utils/
â””â”€â”€ types/
```

### Key Features Implemented
1. âœ… User authentication module
2. âœ… Data access layer
3. âœ… Business logic services
4. âœ… API endpoints

### Code Quality Metrics
- **Test Coverage**: 85%
- **Code Complexity**: Low (avg cyclomatic complexity: 3.2)
- **Technical Debt**: Minimal

### Dependencies
- fastapi==0.104.1
- pydantic==2.5.0
- sqlalchemy==2.0.23

Generated: {datetime.now().isoformat()}
""",
        'testing': f"""# Test Report
## Phase: {phase_type}
## Node ID: {node_id}
## Execution: {execution_id}

### Test Summary
This is a dummy test report generated for phase {phase_number}.

### Test Results
- **Total Tests**: 156
- **Passed**: 152 âœ…
- **Failed**: 4 âŒ
- **Skipped**: 0

### Coverage Report
- **Line Coverage**: 87%
- **Branch Coverage**: 82%
- **Function Coverage**: 91%

### Failed Tests
1. test_user_authentication_edge_case
2. test_concurrent_data_access
3. test_cache_invalidation
4. test_error_handling_timeout

Generated: {datetime.now().isoformat()}
""",
        'deployment': f"""# Deployment Documentation
## Phase: {phase_type}
## Node ID: {node_id}
## Execution: {execution_id}

### Deployment Summary
This is a dummy deployment document generated for phase {phase_number}.

### Environment Configuration
- **Environment**: Production
- **Region**: us-east-1
- **Deployment Strategy**: Blue-Green

### Deployment Steps
1. Build Docker images
2. Push to container registry
3. Update Kubernetes manifests
4. Apply rolling update
5. Monitor health checks

### Post-Deployment Checks
- âœ… Health endpoints responding
- âœ… Database migrations applied
- âœ… Cache warmed up
- âœ… Monitoring alerts configured

### Rollback Plan
In case of issues, execute:
```bash
kubectl rollout undo deployment/app-deployment
```

Generated: {datetime.now().isoformat()}
""",
        'review': f"""# Code Review Report
## Phase: {phase_type}
## Node ID: {node_id}
## Execution: {execution_id}

### Review Summary
This is a dummy code review document generated for phase {phase_number}.

### Review Findings
#### Strengths
- Clean code structure
- Good test coverage
- Comprehensive documentation

#### Issues Found
1. **Minor**: Inconsistent error handling in 3 modules
2. **Minor**: Missing input validation in 2 endpoints
3. **Info**: Consider using async/await for database queries

### Recommendations
1. Add input validation middleware
2. Standardize error handling patterns
3. Implement request rate limiting

### Approval Status
âœ… **APPROVED** with minor recommendations

Generated: {datetime.now().isoformat()}
"""
    }

    # Get content template or default
    content = content_templates.get(phase_type, f"""# {phase_type.title()} Document
## Node ID: {node_id}
## Execution: {execution_id}

This is a dummy document generated for phase {phase_number} of type {phase_type}.

Generated: {datetime.now().isoformat()}
""")

    # Write content to file
    with open(artifact_path, 'w') as f:
        f.write(content)

    # Return artifact metadata
    artifact = {
        'id': f"artifact_{execution_id}_{node_id}",
        'name': f"{phase_type.title()} Document",
        'type': 'document',
        'format': 'markdown',
        'filename': artifact_filename,
        'file_path': artifact_path,
        'download_url': f"/api/workflow/artifacts/{artifact_filename}",
        'size_bytes': len(content),
        'created_at': datetime.now().isoformat(),
        'status': 'completed',
        'metadata': {
            'execution_id': execution_id,
            'node_id': node_id,
            'phase_type': phase_type,
            'phase_number': phase_number
        }
    }

    return artifact


# ---------------------------------------------------------------------------
# Background Task - Phase-by-Phase Execution
# ---------------------------------------------------------------------------

async def execute_workflow_background(
    execution_id: str,
    workflow_id: str,
    workflow_name: str,
    nodes: List[WorkflowNode],
    edges: List[WorkflowEdge]
):
    """
    Background task that executes workflow phase by phase.
    Sends clear status updates for frontend coordination.
    """
    try:
        logger.info(f"ðŸš€ Starting execution: {execution_id} ({len(nodes)} phases)")

        # 1. Send execution accepted confirmation
        await manager.broadcast(workflow_id, {
            'type': 'execution_accepted',
            'execution_id': execution_id,
            'workflow_id': workflow_id,
            'workflow_name': workflow_name,
            'total_phases': len(nodes),
            'message': 'Workflow execution accepted and queued',
            'timestamp': datetime.now().isoformat()
        })

        # 2. Update status to running
        execution_store.update_status(execution_id, 'running')

        await manager.broadcast(workflow_id, {
            'type': 'workflow_started',
            'execution_id': execution_id,
            'workflow_id': workflow_id,
            'message': 'Workflow execution started',
            'timestamp': datetime.now().isoformat()
        })

        # 3. Process each phase sequentially (10 seconds each)
        for i, node in enumerate(nodes, 1):
            node_id = node.id
            phase_type = node.phase_type

            logger.info(f"ðŸ“¦ Phase {i}/{len(nodes)}: {node_id} ({phase_type})")

            # 3a. Send phase started message
            execution_store.update_phase_status(execution_id, node_id, 'running')

            await manager.broadcast(workflow_id, {
                'type': 'phase_started',
                'execution_id': execution_id,
                'workflow_id': workflow_id,
                'node_id': node_id,
                'phase_type': phase_type,
                'phase_label': node.label,
                'phase_number': i,
                'total_phases': len(nodes),
                'assigned_team': node.phase_config.assigned_team,
                'message': f'Phase {i}/{len(nodes)} started: {phase_type}',
                'timestamp': datetime.now().isoformat()
            })

            # 3b. Simulate 10 seconds of processing
            logger.info(f"â³ Processing {node_id} for 10 seconds...")
            await asyncio.sleep(10)

            # 3c. Generate dummy artifact
            artifact = generate_dummy_artifact(execution_id, node_id, phase_type, i)
            logger.info(f"ðŸ“„ Generated artifact: {artifact['name']}")

            # 3d. Send phase completed message with artifact
            execution_store.update_phase_status(execution_id, node_id, 'completed')

            await manager.broadcast(workflow_id, {
                'type': 'phase_completed',
                'execution_id': execution_id,
                'workflow_id': workflow_id,
                'node_id': node_id,
                'phase_type': phase_type,
                'phase_label': node.label,
                'phase_number': i,
                'total_phases': len(nodes),
                'status': 'completed',
                'message': f'Phase {i}/{len(nodes)} completed: {phase_type}',
                'timestamp': datetime.now().isoformat(),
                'artifacts': [artifact]  # Include generated artifact
            })

            logger.info(f"âœ… Phase {i}/{len(nodes)} completed: {node_id}")

        # 4. Mark entire workflow as completed
        execution_store.update_status(execution_id, 'completed')

        await manager.broadcast(workflow_id, {
            'type': 'workflow_completed',
            'execution_id': execution_id,
            'workflow_id': workflow_id,
            'total_phases': len(nodes),
            'completed_phases': len(nodes),
            'message': 'All phases completed successfully',
            'timestamp': datetime.now().isoformat()
        })

        logger.info(f"ðŸŽ‰ Execution completed: {execution_id}")

    except asyncio.CancelledError:
        logger.warning(f"ðŸ›‘ Execution cancelled: {execution_id}")
        execution_store.update_status(execution_id, 'cancelled')
        await manager.broadcast(workflow_id, {
            'type': 'workflow_cancelled',
            'execution_id': execution_id,
            'workflow_id': workflow_id,
            'message': 'Workflow execution cancelled',
            'timestamp': datetime.now().isoformat()
        })
        raise

    except Exception as e:
        logger.error(f"âŒ Execution failed: {execution_id} - {e}", exc_info=True)
        execution_store.update_status(execution_id, 'failed', error=str(e))

        await manager.broadcast(workflow_id, {
            'type': 'workflow_failed',
            'execution_id': execution_id,
            'workflow_id': workflow_id,
            'error': str(e),
            'message': f'Workflow execution failed: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


# ---------------------------------------------------------------------------
# API Endpoints
# ---------------------------------------------------------------------------

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "workflow-api-v2",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/api/workflow/artifacts/{filename}")
async def download_artifact(filename: str):
    """
    Download artifact file.

    Returns the artifact file for download/viewing.
    """
    from fastapi.responses import FileResponse

    artifacts_dir = "/tmp/workflow_artifacts"
    file_path = os.path.join(artifacts_dir, filename)

    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"Artifact not found: {filename}")

    # Return file with appropriate content type
    return FileResponse(
        path=file_path,
        media_type='text/markdown',
        filename=filename,
        headers={
            'Content-Disposition': f'attachment; filename="{filename}"'
        }
    )


@app.post("/api/workflow/execute")
async def execute_workflow(params: DAGWorkflowExecute):
    """
    Execute workflow - accepts blueprint and returns execution_id immediately.

    Response includes:
    - execution_id: Unique identifier for tracking this execution
    - workflow_id: Workflow definition ID
    - total_phases: Number of phases to execute
    - status: Initial status (pending/queued)

    Frontend should:
    1. Store execution_id
    2. Connect to WebSocket: /ws/workflow/{workflow_id}
    3. Listen for messages with matching execution_id
    """
    try:
        # Generate execution ID
        execution_id = f"exec_{uuid.uuid4().hex[:12]}"

        logger.info(f"ðŸŽ¨ Workflow execution request")
        logger.info(f"   - Workflow ID: {params.workflow_id}")
        logger.info(f"   - Workflow Name: {params.workflow_name}")
        logger.info(f"   - Total Phases: {len(params.nodes)}")
        logger.info(f"   - Execution ID: {execution_id}")

        # Log phase details
        for i, node in enumerate(params.nodes, 1):
            logger.info(f"   - Phase {i}: {node.id} ({node.phase_type}) - {node.label}")

        # Validate input
        if not params.nodes:
            raise HTTPException(status_code=400, detail="Workflow must have at least one phase")

        # Create execution record
        execution = execution_store.create(
            execution_id=execution_id,
            workflow_id=params.workflow_id,
            workflow_name=params.workflow_name,
            nodes=params.nodes
        )

        # Start background execution
        task = asyncio.create_task(
            execute_workflow_background(
                execution_id=execution_id,
                workflow_id=params.workflow_id,
                workflow_name=params.workflow_name,
                nodes=params.nodes,
                edges=params.edges
            )
        )
        background_tasks.add(task)
        task.add_done_callback(background_tasks.discard)

        # Return immediately with execution details
        logger.info(f"âœ… Execution {execution_id} queued, returning to frontend")

        return {
            'execution_id': execution_id,
            'workflow_id': params.workflow_id,
            'workflow_name': params.workflow_name,
            'status': 'pending',
            'total_phases': len(params.nodes),
            'phases': [
                {
                    'node_id': node.id,
                    'phase_type': node.phase_type,
                    'label': node.label,
                    'status': 'pending'
                }
                for node in params.nodes
            ],
            'message': 'Workflow execution queued. Connect to WebSocket for real-time updates.',
            'websocket_url': f'/ws/workflow/{params.workflow_id}',
            'created_at': execution['created_at']
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error starting workflow execution: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to start execution: {str(e)}")


@app.get("/api/workflow/status/{execution_id}")
async def get_execution_status(execution_id: str):
    """
    Get detailed execution status including all phases.

    Response includes:
    - Overall execution status
    - Status of each individual phase
    - Timestamps for start/completion
    """
    execution = execution_store.get(execution_id)

    if not execution:
        raise HTTPException(status_code=404, detail=f"Execution not found: {execution_id}")

    return execution


@app.websocket("/ws/workflow/{workflow_id}")
async def websocket_endpoint(websocket: WebSocket, workflow_id: str):
    """
    WebSocket endpoint for real-time workflow updates.

    Message Types:
    - connected: Connection established
    - execution_accepted: Workflow queued
    - workflow_started: Execution began
    - phase_started: Specific phase started (includes node_id, phase_type)
    - phase_completed: Specific phase finished (includes node_id, status)
    - workflow_completed: All phases done
    - workflow_failed: Execution failed
    """
    await manager.connect(websocket, workflow_id)

    try:
        # Send connection confirmation
        await websocket.send_json({
            'type': 'connected',
            'workflow_id': workflow_id,
            'message': 'WebSocket connected. Listening for workflow updates.',
            'timestamp': datetime.now().isoformat()
        })

        # Keep connection alive and handle client messages
        while True:
            data = await websocket.receive_text()
            logger.debug(f"ðŸ“¨ Received from client: {data}")

            # Echo back (for heartbeat/ping-pong)
            await websocket.send_json({
                'type': 'pong',
                'message': 'Server received your message',
                'timestamp': datetime.now().isoformat()
            })

    except WebSocketDisconnect:
        manager.disconnect(websocket, workflow_id)
        logger.info(f"Client disconnected from workflow: {workflow_id}")
    except Exception as e:
        logger.error(f"WebSocket error: {e}", exc_info=True)
        manager.disconnect(websocket, workflow_id)


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "5001"))
    logger.info(f"ðŸš€ Starting Workflow API V2 on port {port}")
    logger.info("ðŸ“‹ Endpoints:")
    logger.info("   - POST /api/workflow/execute - Execute workflow")
    logger.info("   - GET  /api/workflow/status/{execution_id} - Get status")
    logger.info("   - WS   /ws/workflow/{workflow_id} - Real-time updates")

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
