#!/usr/bin/env python3
"""
Extensive DAG Platform Test Suite with Quality Fabric Integration

Comprehensive test coverage for:
- DAG workflow execution (linear, parallel, conditional)
- All SDLC phases with quality validation
- Persona output validation with quality gates
- Phase gate transitions
- Error handling and recovery
- Performance and timeout scenarios
- Database persistence
- WebSocket communication
"""

import asyncio
import logging
import sys
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
import json

# Test framework
import pytest

# DAG system imports
from dag_compatibility import generate_parallel_workflow, generate_linear_workflow
from dag_executor import DAGExecutor
from dag_workflow import WorkflowDAG, WorkflowNode, NodeStatus
from database.workflow_store import DatabaseWorkflowContextStore
from team_execution_v2_split_mode import TeamExecutionEngineV2SplitMode

# Quality Fabric integration
from quality_fabric_client import (
    QualityFabricClient,
    PersonaType,
    PersonaValidationResult
)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)
logger = logging.getLogger(__name__)

# Test configuration
TEST_OUTPUT_DIR = "./test_dag_quality_output"
TEST_CHECKPOINT_DIR = "./test_dag_quality_checkpoints"
DEFAULT_TIMEOUT = 600  # 10 minutes per test


class DAGQualityTestSuite:
    """Comprehensive test suite for DAG platform with quality validation"""

    def __init__(self):
        self.quality_client = QualityFabricClient()
        self.team_engine = None
        self.context_store = None
        self.test_results = []

    async def setup(self):
        """Initialize test environment"""
        logger.info("=" * 80)
        logger.info("DAG PLATFORM QUALITY TEST SUITE - SETUP")
        logger.info("=" * 80)

        # Create team engine
        self.team_engine = TeamExecutionEngineV2SplitMode(
            output_dir=TEST_OUTPUT_DIR,
            checkpoint_dir=TEST_CHECKPOINT_DIR
        )
        logger.info("‚úÖ Team engine created")

        # Create context store
        self.context_store = DatabaseWorkflowContextStore()
        logger.info("‚úÖ Context store created")

        # Verify quality fabric connectivity
        try:
            health = await self.quality_client._make_request("GET", "/health")
            logger.info(f"‚úÖ Quality Fabric connected: {health}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Quality Fabric using mock mode: {e}")

        logger.info("=" * 80)

    async def teardown(self):
        """Cleanup test environment"""
        logger.info("\n" + "=" * 80)
        logger.info("TEST SUITE TEARDOWN")
        logger.info("=" * 80)
        logger.info(f"Total tests run: {len(self.test_results)}")
        passed = sum(1 for r in self.test_results if r['status'] == 'PASSED')
        failed = sum(1 for r in self.test_results if r['status'] == 'FAILED')
        logger.info(f"Passed: {passed}, Failed: {failed}")

    def record_result(self, test_name: str, status: str, details: Dict[str, Any]):
        """Record test result"""
        self.test_results.append({
            'test_name': test_name,
            'status': status,
            'timestamp': datetime.now().isoformat(),
            'details': details
        })

    # ========================================================================
    # TEST CATEGORY 1: Basic Workflow Creation and Validation
    # ========================================================================

    async def test_parallel_workflow_creation(self):
        """Test creating parallel DAG workflow"""
        test_name = "test_parallel_workflow_creation"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            workflow = generate_parallel_workflow(
                workflow_name="test_parallel",
                team_engine=self.team_engine
            )

            # Validate workflow structure
            assert workflow is not None, "Workflow should not be None"
            assert len(workflow.nodes) == 6, "Should have 6 SDLC phases"
            assert workflow.name == "test_parallel", "Workflow name mismatch"

            # Validate parallel structure (backend & frontend can run in parallel)
            backend_node = workflow.nodes.get('backend_development')
            frontend_node = workflow.nodes.get('frontend_development')
            assert backend_node is not None, "Backend node should exist"
            assert frontend_node is not None, "Frontend node should exist"

            logger.info("‚úÖ Parallel workflow created successfully")
            logger.info(f"   Nodes: {list(workflow.nodes.keys())}")

            self.record_result(test_name, 'PASSED', {
                'node_count': len(workflow.nodes),
                'nodes': list(workflow.nodes.keys())
            })
            return True

        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    async def test_linear_workflow_creation(self):
        """Test creating linear DAG workflow"""
        test_name = "test_linear_workflow_creation"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            workflow = generate_linear_workflow(
                workflow_name="test_linear",
                team_engine=self.team_engine
            )

            # Validate workflow structure
            assert workflow is not None, "Workflow should not be None"
            assert len(workflow.nodes) >= 5, "Should have at least 5 phases"

            # Validate linear structure (each node has at most one dependency)
            for node_id, node in workflow.nodes.items():
                deps = list(workflow.graph.predecessors(node_id))
                assert len(deps) <= 1, f"Linear workflow node {node_id} should have at most 1 dependency"

            logger.info("‚úÖ Linear workflow created successfully")
            logger.info(f"   Nodes: {list(workflow.nodes.keys())}")

            self.record_result(test_name, 'PASSED', {
                'node_count': len(workflow.nodes),
                'nodes': list(workflow.nodes.keys())
            })
            return True

        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    # ========================================================================
    # TEST CATEGORY 2: SDLC Phase Execution with Quality Validation
    # ========================================================================

    async def test_requirement_analysis_with_quality_gates(self):
        """Test requirement analysis phase with quality validation"""
        test_name = "test_requirement_analysis_with_quality_gates"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            # Create workflow
            workflow = generate_parallel_workflow(
                workflow_name="test_requirements",
                team_engine=self.team_engine
            )

            executor = DAGExecutor(
                workflow=workflow,
                context_store=self.context_store
            )

            # Execute requirement analysis phase only
            requirement = "Build a REST API for user authentication with JWT tokens"
            initial_context = {
                'requirement': requirement,
                'workflow_id': workflow.workflow_id,
                'phases_to_execute': ['requirement_analysis']  # Only this phase
            }

            start_time = time.time()
            context = await asyncio.wait_for(
                executor.execute(initial_context=initial_context),
                timeout=120
            )
            execution_time = time.time() - start_time

            # Validate execution
            req_state = context.node_states.get('requirement_analysis')
            assert req_state is not None, "Requirement analysis state should exist"
            assert req_state.status == NodeStatus.COMPLETED, "Phase should complete"

            # Extract phase output
            phase_output = context.get_node_result('requirement_analysis')

            # Validate with Quality Fabric
            validation_result = await self.quality_client.validate_persona_output(
                persona_id="requirement_analyst_001",
                persona_type=PersonaType.REQUIREMENT_ANALYST,
                output=phase_output or {},
                custom_gates={
                    'completeness_threshold': 0.7,
                    'clarity_threshold': 0.7
                }
            )

            logger.info(f"‚úÖ Requirement analysis completed in {execution_time:.2f}s")
            logger.info(f"   Quality Score: {validation_result.overall_score:.2f}")
            logger.info(f"   Status: {validation_result.status}")

            # Assert quality gates
            assert validation_result.overall_score >= 0.6, "Quality score should be >= 0.6"

            self.record_result(test_name, 'PASSED', {
                'execution_time': execution_time,
                'quality_score': validation_result.overall_score,
                'quality_status': validation_result.status,
                'gates_passed': validation_result.gates_passed,
                'gates_failed': validation_result.gates_failed
            })
            return True

        except asyncio.TimeoutError:
            logger.error("‚ùå Test timeout")
            self.record_result(test_name, 'FAILED', {'error': 'Timeout'})
            return False
        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    async def test_backend_development_with_quality_gates(self):
        """Test backend development phase with quality validation"""
        test_name = "test_backend_development_with_quality_gates"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            # Create workflow
            workflow = generate_parallel_workflow(
                workflow_name="test_backend",
                team_engine=self.team_engine
            )

            executor = DAGExecutor(
                workflow=workflow,
                context_store=self.context_store
            )

            # Execute backend development (requires requirement analysis first)
            requirement = "Build a simple REST API endpoint for health check"
            initial_context = {
                'requirement': requirement,
                'workflow_id': workflow.workflow_id,
                'phases_to_execute': ['requirement_analysis', 'design', 'backend_development']
            }

            start_time = time.time()
            context = await asyncio.wait_for(
                executor.execute(initial_context=initial_context),
                timeout=300
            )
            execution_time = time.time() - start_time

            # Validate execution
            backend_state = context.node_states.get('backend_development')
            assert backend_state is not None, "Backend development state should exist"

            # Extract phase output
            phase_output = context.get_node_result('backend_development')

            # Validate with Quality Fabric
            validation_result = await self.quality_client.validate_persona_output(
                persona_id="backend_developer_001",
                persona_type=PersonaType.BACKEND_DEVELOPER,
                output=phase_output or {},
                custom_gates={
                    'code_quality_threshold': 0.7,
                    'test_coverage_threshold': 0.6,
                    'security_threshold': 0.7
                }
            )

            logger.info(f"‚úÖ Backend development completed in {execution_time:.2f}s")
            logger.info(f"   Quality Score: {validation_result.overall_score:.2f}")
            logger.info(f"   Status: {validation_result.status}")

            # Assert quality gates (relaxed for integration test)
            assert validation_result.overall_score >= 0.5, "Quality score should be >= 0.5"

            self.record_result(test_name, 'PASSED', {
                'execution_time': execution_time,
                'quality_score': validation_result.overall_score,
                'quality_status': validation_result.status,
                'gates_passed': validation_result.gates_passed,
                'gates_failed': validation_result.gates_failed
            })
            return True

        except asyncio.TimeoutError:
            logger.error("‚ùå Test timeout")
            self.record_result(test_name, 'FAILED', {'error': 'Timeout'})
            return False
        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    async def test_qa_testing_with_quality_gates(self):
        """Test QA testing phase with quality validation"""
        test_name = "test_qa_testing_with_quality_gates"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            # Create workflow
            workflow = generate_parallel_workflow(
                workflow_name="test_qa",
                team_engine=self.team_engine
            )

            executor = DAGExecutor(
                workflow=workflow,
                context_store=self.context_store
            )

            # Execute full pipeline through testing
            requirement = "Build a calculator function that adds two numbers"
            initial_context = {
                'requirement': requirement,
                'workflow_id': workflow.workflow_id,
                'phases_to_execute': [
                    'requirement_analysis',
                    'design',
                    'backend_development',
                    'testing'
                ]
            }

            start_time = time.time()
            context = await asyncio.wait_for(
                executor.execute(initial_context=initial_context),
                timeout=400
            )
            execution_time = time.time() - start_time

            # Validate execution
            qa_state = context.node_states.get('testing')
            assert qa_state is not None, "Testing state should exist"

            # Extract phase output
            phase_output = context.get_node_result('testing')

            # Validate with Quality Fabric
            validation_result = await self.quality_client.validate_persona_output(
                persona_id="qa_engineer_001",
                persona_type=PersonaType.QA_ENGINEER,
                output=phase_output or {},
                custom_gates={
                    'test_coverage_threshold': 0.7,
                    'bug_detection_threshold': 0.6
                }
            )

            logger.info(f"‚úÖ QA testing completed in {execution_time:.2f}s")
            logger.info(f"   Quality Score: {validation_result.overall_score:.2f}")
            logger.info(f"   Status: {validation_result.status}")

            self.record_result(test_name, 'PASSED', {
                'execution_time': execution_time,
                'quality_score': validation_result.overall_score,
                'quality_status': validation_result.status,
                'gates_passed': validation_result.gates_passed,
                'gates_failed': validation_result.gates_failed
            })
            return True

        except asyncio.TimeoutError:
            logger.error("‚ùå Test timeout")
            self.record_result(test_name, 'FAILED', {'error': 'Timeout'})
            return False
        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    # ========================================================================
    # TEST CATEGORY 3: Phase Gate Transitions
    # ========================================================================

    async def test_phase_gate_requirement_to_design(self):
        """Test phase gate transition from requirements to design"""
        test_name = "test_phase_gate_requirement_to_design"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            # Simulate requirement phase completion
            requirement_output = {
                'requirement_document': 'User authentication system with JWT',
                'functional_requirements': [
                    'User login endpoint',
                    'Token generation',
                    'Token validation'
                ],
                'non_functional_requirements': [
                    'Response time < 200ms',
                    'Security: HTTPS only'
                ]
            }

            persona_results = [{
                'persona_id': 'requirement_analyst_001',
                'persona_type': 'requirement_analyst',
                'score': 0.85,
                'status': 'pass'
            }]

            # Evaluate phase gate
            gate_result = await self.quality_client.evaluate_phase_gate(
                current_phase='requirement_analysis',
                next_phase='design',
                phase_outputs=requirement_output,
                persona_results=persona_results
            )

            logger.info(f"‚úÖ Phase gate evaluated")
            logger.info(f"   Decision: {gate_result.get('decision', 'unknown')}")
            logger.info(f"   Score: {gate_result.get('score', 0):.2f}")

            # Assert gate passed
            assert gate_result.get('decision') == 'proceed' or gate_result.get('decision') == 'approved', \
                "Phase gate should allow progression"

            self.record_result(test_name, 'PASSED', {
                'gate_decision': gate_result.get('decision'),
                'gate_score': gate_result.get('score'),
                'recommendations': gate_result.get('recommendations', [])
            })
            return True

        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    async def test_phase_gate_design_to_implementation(self):
        """Test phase gate transition from design to implementation"""
        test_name = "test_phase_gate_design_to_implementation"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            # Simulate design phase completion
            design_output = {
                'architecture_diagram': 'REST API with Express.js',
                'api_endpoints': [
                    {'path': '/api/auth/login', 'method': 'POST'},
                    {'path': '/api/auth/validate', 'method': 'GET'}
                ],
                'database_schema': {
                    'users': ['id', 'username', 'password_hash', 'created_at']
                },
                'technology_stack': ['Node.js', 'Express', 'PostgreSQL', 'JWT']
            }

            persona_results = [{
                'persona_id': 'architect_001',
                'persona_type': 'architect',
                'score': 0.80,
                'status': 'pass'
            }]

            # Evaluate phase gate
            gate_result = await self.quality_client.evaluate_phase_gate(
                current_phase='design',
                next_phase='backend_development',
                phase_outputs=design_output,
                persona_results=persona_results
            )

            logger.info(f"‚úÖ Phase gate evaluated")
            logger.info(f"   Decision: {gate_result.get('decision', 'unknown')}")
            logger.info(f"   Score: {gate_result.get('score', 0):.2f}")

            self.record_result(test_name, 'PASSED', {
                'gate_decision': gate_result.get('decision'),
                'gate_score': gate_result.get('score'),
                'recommendations': gate_result.get('recommendations', [])
            })
            return True

        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    # ========================================================================
    # TEST CATEGORY 4: Error Handling and Recovery
    # ========================================================================

    async def test_node_failure_and_retry(self):
        """Test node failure detection and retry mechanism"""
        test_name = "test_node_failure_and_retry"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            # This test verifies that the system handles failures gracefully
            # In a real scenario, we'd inject a failure and watch retry logic

            workflow = generate_parallel_workflow(
                workflow_name="test_retry",
                team_engine=self.team_engine
            )

            # Verify retry policy exists on nodes
            for node_id, node in workflow.nodes.items():
                assert hasattr(node, 'retry_policy'), f"Node {node_id} should have retry policy"
                if node.retry_policy:
                    logger.info(f"   Node {node_id}: max_attempts={node.retry_policy.max_attempts}")

            logger.info("‚úÖ Retry policies configured on all nodes")

            self.record_result(test_name, 'PASSED', {
                'nodes_with_retry': len([n for n in workflow.nodes.values() if n.retry_policy])
            })
            return True

        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    async def test_execution_timeout_handling(self):
        """Test execution timeout mechanism"""
        test_name = "test_execution_timeout_handling"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            workflow = generate_parallel_workflow(
                workflow_name="test_timeout",
                team_engine=self.team_engine
            )

            executor = DAGExecutor(
                workflow=workflow,
                context_store=self.context_store
            )

            # Execute with very short timeout to test timeout handling
            requirement = "Build a complex microservices architecture"
            initial_context = {
                'requirement': requirement,
                'workflow_id': workflow.workflow_id,
                'timeout_seconds': 5  # Very short timeout
            }

            try:
                context = await asyncio.wait_for(
                    executor.execute(initial_context=initial_context),
                    timeout=10
                )
                # If it completes within 5 seconds, that's also valid
                logger.info("‚úÖ Execution completed within timeout")

            except asyncio.TimeoutError:
                # Expected - timeout handling works
                logger.info("‚úÖ Timeout correctly triggered and handled")

            self.record_result(test_name, 'PASSED', {
                'timeout_mechanism': 'verified'
            })
            return True

        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    # ========================================================================
    # TEST CATEGORY 5: Database Persistence
    # ========================================================================

    async def test_context_persistence(self):
        """Test workflow context persistence to database"""
        test_name = "test_context_persistence"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            workflow = generate_parallel_workflow(
                workflow_name="test_persistence",
                team_engine=self.team_engine
            )

            executor = DAGExecutor(
                workflow=workflow,
                context_store=self.context_store
            )

            # Execute simple workflow
            requirement = "Build a Hello World function"
            initial_context = {
                'requirement': requirement,
                'workflow_id': workflow.workflow_id,
                'phases_to_execute': ['requirement_analysis']
            }

            context = await asyncio.wait_for(
                executor.execute(initial_context=initial_context),
                timeout=120
            )

            execution_id = context.execution_id

            # Load context from database
            loaded_context = await self.context_store.load_context(
                workflow_id=workflow.workflow_id,
                execution_id=execution_id
            )

            assert loaded_context is not None, "Context should be loaded from database"
            assert loaded_context.execution_id == execution_id, "Execution ID should match"
            assert loaded_context.workflow_id == workflow.workflow_id, "Workflow ID should match"

            logger.info("‚úÖ Context persistence verified")
            logger.info(f"   Execution ID: {execution_id}")
            logger.info(f"   Workflow ID: {workflow.workflow_id}")

            self.record_result(test_name, 'PASSED', {
                'execution_id': execution_id,
                'workflow_id': workflow.workflow_id,
                'nodes_persisted': len(loaded_context.node_states)
            })
            return True

        except asyncio.TimeoutError:
            logger.error("‚ùå Test timeout")
            self.record_result(test_name, 'FAILED', {'error': 'Timeout'})
            return False
        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    async def test_execution_history_listing(self):
        """Test listing execution history from database"""
        test_name = "test_execution_history_listing"
        logger.info(f"\nüß™ Running: {test_name}")

        try:
            # List all executions
            executions = await self.context_store.list_executions(limit=10)

            assert isinstance(executions, list), "Should return list of executions"
            logger.info(f"‚úÖ Listed {len(executions)} executions from database")

            # Display execution details
            for exec_info in executions[:5]:  # Show first 5
                logger.info(f"   Execution: {exec_info.get('execution_id', 'N/A')}")
                logger.info(f"     Status: {exec_info.get('status', 'N/A')}")
                logger.info(f"     Created: {exec_info.get('created_at', 'N/A')}")

            self.record_result(test_name, 'PASSED', {
                'executions_found': len(executions)
            })
            return True

        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    # ========================================================================
    # TEST CATEGORY 6: Full End-to-End Workflows
    # ========================================================================

    async def test_full_sdlc_workflow_simple_project(self):
        """Test complete SDLC workflow for simple project"""
        test_name = "test_full_sdlc_workflow_simple_project"
        logger.info(f"\nüß™ Running: {test_name}")
        logger.info("‚ö†Ô∏è  This is a long-running test (5-10 minutes)")

        try:
            workflow = generate_parallel_workflow(
                workflow_name="test_full_sdlc",
                team_engine=self.team_engine
            )

            executor = DAGExecutor(
                workflow=workflow,
                context_store=self.context_store
            )

            # Execute full SDLC pipeline
            requirement = "Build a simple REST API with one GET endpoint /api/hello that returns 'Hello World'"
            initial_context = {
                'requirement': requirement,
                'workflow_id': workflow.workflow_id,
                'timeout_seconds': 600
            }

            start_time = time.time()

            try:
                context = await asyncio.wait_for(
                    executor.execute(initial_context=initial_context),
                    timeout=DEFAULT_TIMEOUT
                )
                execution_time = time.time() - start_time

                # Analyze results
                completed_phases = [
                    node_id for node_id, state in context.node_states.items()
                    if state.status == NodeStatus.COMPLETED
                ]

                logger.info(f"‚úÖ Full SDLC workflow completed in {execution_time:.2f}s")
                logger.info(f"   Completed phases: {len(completed_phases)}/{len(workflow.nodes)}")
                logger.info(f"   Phases: {completed_phases}")

                # Validate each phase with quality fabric
                quality_scores = {}
                for phase_id in completed_phases:
                    phase_output = context.get_node_result(phase_id)
                    if phase_output:
                        # Map phase to persona type
                        persona_type_map = {
                            'requirement_analysis': PersonaType.REQUIREMENT_ANALYST,
                            'design': PersonaType.ARCHITECT,
                            'backend_development': PersonaType.BACKEND_DEVELOPER,
                            'frontend_development': PersonaType.FRONTEND_DEVELOPER,
                            'testing': PersonaType.QA_ENGINEER,
                            'review': PersonaType.PROJECT_REVIEWER
                        }

                        persona_type = persona_type_map.get(phase_id)
                        if persona_type:
                            validation = await self.quality_client.validate_persona_output(
                                persona_id=f"{phase_id}_final",
                                persona_type=persona_type,
                                output=phase_output
                            )
                            quality_scores[phase_id] = validation.overall_score
                            logger.info(f"   {phase_id}: Quality {validation.overall_score:.2f}")

                avg_quality = sum(quality_scores.values()) / len(quality_scores) if quality_scores else 0

                self.record_result(test_name, 'PASSED', {
                    'execution_time': execution_time,
                    'completed_phases': len(completed_phases),
                    'total_phases': len(workflow.nodes),
                    'quality_scores': quality_scores,
                    'average_quality': avg_quality
                })
                return True

            except asyncio.TimeoutError:
                execution_time = time.time() - start_time
                logger.warning(f"‚è±Ô∏è  Workflow timeout after {execution_time:.2f}s")
                logger.info("   This is expected for complex workflows")

                self.record_result(test_name, 'PASSED', {
                    'execution_time': execution_time,
                    'status': 'timeout_expected',
                    'note': 'Long-running workflow, timeout is acceptable'
                })
                return True

        except Exception as e:
            logger.error(f"‚ùå Test failed: {e}", exc_info=True)
            self.record_result(test_name, 'FAILED', {'error': str(e)})
            return False

    # ========================================================================
    # TEST RUNNER
    # ========================================================================

    async def run_all_tests(self):
        """Run all test categories"""
        logger.info("\n" + "=" * 80)
        logger.info("STARTING COMPREHENSIVE DAG PLATFORM TEST SUITE")
        logger.info("=" * 80)

        await self.setup()

        test_categories = [
            ("Basic Workflow Creation", [
                self.test_parallel_workflow_creation,
                self.test_linear_workflow_creation,
            ]),
            ("SDLC Phase Execution with Quality Gates", [
                self.test_requirement_analysis_with_quality_gates,
                self.test_backend_development_with_quality_gates,
                self.test_qa_testing_with_quality_gates,
            ]),
            ("Phase Gate Transitions", [
                self.test_phase_gate_requirement_to_design,
                self.test_phase_gate_design_to_implementation,
            ]),
            ("Error Handling and Recovery", [
                self.test_node_failure_and_retry,
                self.test_execution_timeout_handling,
            ]),
            ("Database Persistence", [
                self.test_context_persistence,
                self.test_execution_history_listing,
            ]),
            ("Full End-to-End Workflows", [
                self.test_full_sdlc_workflow_simple_project,
            ])
        ]

        all_passed = True

        for category_name, tests in test_categories:
            logger.info("\n" + "=" * 80)
            logger.info(f"TEST CATEGORY: {category_name}")
            logger.info("=" * 80)

            for test_func in tests:
                try:
                    result = await test_func()
                    if not result:
                        all_passed = False
                except Exception as e:
                    logger.error(f"‚ùå Test {test_func.__name__} crashed: {e}")
                    all_passed = False

        await self.teardown()

        # Generate final report
        self.generate_report()

        return all_passed

    def generate_report(self):
        """Generate comprehensive test report"""
        logger.info("\n" + "=" * 80)
        logger.info("COMPREHENSIVE TEST REPORT")
        logger.info("=" * 80)

        total = len(self.test_results)
        passed = sum(1 for r in self.test_results if r['status'] == 'PASSED')
        failed = sum(1 for r in self.test_results if r['status'] == 'FAILED')
        pass_rate = (passed / total * 100) if total > 0 else 0

        logger.info(f"\nTest Summary:")
        logger.info(f"  Total Tests: {total}")
        logger.info(f"  Passed: {passed} ‚úÖ")
        logger.info(f"  Failed: {failed} ‚ùå")
        logger.info(f"  Pass Rate: {pass_rate:.1f}%")

        # Category breakdown
        logger.info(f"\nTest Results by Category:")
        for result in self.test_results:
            status_symbol = "‚úÖ" if result['status'] == 'PASSED' else "‚ùå"
            logger.info(f"  {status_symbol} {result['test_name']}")
            if 'quality_score' in result['details']:
                logger.info(f"      Quality Score: {result['details']['quality_score']:.2f}")
            if 'execution_time' in result['details']:
                logger.info(f"      Execution Time: {result['details']['execution_time']:.2f}s")

        # Save report to file
        report_file = f"{TEST_OUTPUT_DIR}/test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            import os
            os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)
            with open(report_file, 'w') as f:
                json.dump({
                    'summary': {
                        'total': total,
                        'passed': passed,
                        'failed': failed,
                        'pass_rate': pass_rate
                    },
                    'results': self.test_results
                }, f, indent=2)
            logger.info(f"\nüìÑ Detailed report saved to: {report_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not save report file: {e}")

        logger.info("=" * 80)


async def main():
    """Main entry point"""
    suite = DAGQualityTestSuite()
    success = await suite.run_all_tests()

    if success:
        logger.info("\nüéâ ALL TESTS COMPLETED SUCCESSFULLY!")
        sys.exit(0)
    else:
        logger.error("\n‚ùå SOME TESTS FAILED - Review report above")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
