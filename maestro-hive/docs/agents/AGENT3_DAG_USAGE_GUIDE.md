# DAG Workflow System - Usage Guide

**Status:** âœ… Implemented (Core features as of 2025-10-11) | ðŸ“‹ Proposed (Advanced features)
**Last Validated:** 2025-10-11

**Related Documentation:**
- [AGENT3_DAG_REFERENCE.md](./AGENT3_DAG_REFERENCE.md) - **Canonical reference** for state machines, events, and feature flags
- [AGENT3_DAG_WORKFLOW_ARCHITECTURE.md](./AGENT3_DAG_WORKFLOW_ARCHITECTURE.md) - Architecture overview
- [AGENT3_DAG_MIGRATION_GUIDE.md](./AGENT3_DAG_MIGRATION_GUIDE.md) - Migration guide
- [Universal Contract Protocol](./docs/contract_protocol/UNIVERSAL_CONTRACT_PROTOCOL.md) - Contract integration

## Overview

This guide shows how to use the DAG-based workflow system in Maestro SDLC platform. The DAG system provides:

- âœ… **Flexible execution**: Linear, parallel, or custom workflows (implemented)
- âœ… **State persistence**: Pause/resume capability (implemented)
- âœ… **Event tracking**: Real-time progress monitoring (implemented)
- âœ… **Backward compatibility**: Existing code continues to work (validated 2025-10-11)
- âœ… **Gradual migration**: Feature flags control new capabilities (implemented)

> **Feature Flags:** See [AGENT3_DAG_REFERENCE.md](./AGENT3_DAG_REFERENCE.md#feature-flags) for complete feature flag reference and defaults.

## Quick Start

### 1. Basic Usage (Backward Compatible)

The simplest way to start using the DAG system is through the dual-mode engine:

```python
from team_execution_v2_split_mode import TeamExecutionEngineV2SplitMode
from team_execution_dual import create_dual_engine

# Create your existing engine
linear_engine = TeamExecutionEngineV2SplitMode()

# Wrap it with dual-mode support
dual_engine = create_dual_engine(linear_engine)

# Execute (uses linear mode by default)
result = await dual_engine.execute(
    requirement="Build a todo app with React and FastAPI"
)
```

**Output:**
```json
{
  "status": "completed",
  "execution_mode": "linear",
  "phases": {
    "requirement_analysis": {...},
    "design": {...},
    "backend_development": {...},
    "frontend_development": {...},
    "testing": {...},
    "review": {...}
  }
}
```

### 2. Enable DAG Execution with Environment Variables

```bash
# Enable DAG execution (linear workflow)
export MAESTRO_ENABLE_DAG_EXECUTION=true

# Enable parallel execution (backend + frontend in parallel)
export MAESTRO_ENABLE_PARALLEL_EXECUTION=true

# Enable event tracking
export MAESTRO_ENABLE_EXECUTION_EVENTS=true
```

```python
# Now your execution runs in DAG mode
result = await dual_engine.execute(
    requirement="Build a todo app with React and FastAPI"
)
```

**Output with parallel execution:**
```json
{
  "status": "completed",
  "execution_mode": "dag_parallel",
  "workflow_id": "sdlc_parallel_abc123",
  "completed_nodes": 6,
  "total_nodes": 6,
  "execution_time": 45.3,
  "event_summary": {
    "total_events": 18,
    "node_started": 6,
    "node_completed": 6,
    "node_failed": 0
  }
}
```

### 3. Programmatic Feature Flags

```python
from team_execution_dual import TeamExecutionEngineDual, FeatureFlags

# Configure feature flags
flags = FeatureFlags()
flags.enable_dag_execution = True
flags.enable_parallel_execution = True
flags.enable_execution_events = True

# Create dual engine with custom flags
dual_engine = TeamExecutionEngineDual(
    linear_engine=linear_engine,
    feature_flags=flags
)

# Execute
result = await dual_engine.execute(
    requirement="Build a todo app"
)
```

## Advanced Usage

> **API Reference:** Module and class names below should be verified against actual implementation in `src/dag_workflow.py` and `src/dag_executor.py`.
>
> **State Machines:** See [AGENT3_DAG_REFERENCE.md](./AGENT3_DAG_REFERENCE.md#node-state-machine) for canonical node state transitions.

### 4. Direct DAG Workflow Creation

**Status:** âœ… Implemented

For full control, create workflows directly:

```python
from dag_workflow import WorkflowDAG, WorkflowNode, NodeType
from dag_executor import DAGExecutor

# Create workflow
workflow = WorkflowDAG(name="custom_workflow")

# Define node executors
async def analyze_requirements(input_data):
    requirement = input_data['global_context']['requirement']
    # Your analysis logic here
    return {
        "features": ["user auth", "crud operations"],
        "tech_stack": ["React", "FastAPI"]
    }

async def design_system(input_data):
    features = input_data['dependency_outputs']['req_analysis']['features']
    # Your design logic here
    return {
        "architecture": "microservices",
        "api_endpoints": ["/users", "/todos"]
    }

# Create nodes
req_node = WorkflowNode(
    node_id="req_analysis",
    name="Requirement Analysis",
    node_type=NodeType.PHASE,
    executor=analyze_requirements,
    dependencies=[]
)

design_node = WorkflowNode(
    node_id="system_design",
    name="System Design",
    node_type=NodeType.PHASE,
    executor=design_system,
    dependencies=["req_analysis"]
)

# Add to workflow
workflow.add_node(req_node)
workflow.add_node(design_node)
workflow.add_edge("req_analysis", "system_design")

# Execute
executor = DAGExecutor(workflow)
context = await executor.execute(
    initial_context={"requirement": "Build a todo app"}
)

# Access results
req_output = context.get_node_output("req_analysis")
design_output = context.get_node_output("system_design")
```

### 5. Parallel Workflow Pattern

**Status:** âœ… Implemented

```python
from dag_compatibility import generate_parallel_workflow

# Generate workflow with parallel development
workflow = generate_parallel_workflow(team_engine=linear_engine)

# Execution order:
# 1. requirement_analysis
# 2. design
# 3. backend_development + frontend_development (PARALLEL)
# 4. testing
# 5. review

executor = DAGExecutor(workflow)
context = await executor.execute(
    initial_context={"requirement": "Build a todo app"}
)
```

**Execution Timeline:**
```
Time  | Phase
------|-----------------------------------------------
0s    | requirement_analysis (running)
10s   | requirement_analysis (complete)
10s   | design (running)
25s   | design (complete)
25s   | backend_development (running) + frontend_development (running)
60s   | backend_development (complete)
65s   | frontend_development (complete)
65s   | testing (running)
80s   | testing (complete)
80s   | review (running)
90s   | review (complete)
```

### 6. Custom Workflow with Conditional Execution

**Status:** ðŸ“‹ Proposed

```python
# Create workflow with conditional nodes
workflow = WorkflowDAG(name="conditional_workflow")

async def check_requirements(input_data):
    return {
        "requires_database": True,
        "requires_authentication": False
    }

async def setup_database(input_data):
    return {"database": "postgresql", "migrations": ["001_init.sql"]}

async def setup_auth(input_data):
    return {"auth_method": "JWT"}

# Check node
check_node = WorkflowNode(
    node_id="check",
    name="Check Requirements",
    node_type=NodeType.PHASE,
    executor=check_requirements
)

# Conditional database setup
db_node = WorkflowNode(
    node_id="setup_db",
    name="Setup Database",
    node_type=NodeType.CONDITIONAL,
    executor=setup_database,
    dependencies=["check"],
    condition="outputs['check']['requires_database']"  # Only if True
)

# Conditional auth setup
auth_node = WorkflowNode(
    node_id="setup_auth",
    name="Setup Authentication",
    node_type=NodeType.CONDITIONAL,
    executor=setup_auth,
    dependencies=["check"],
    condition="outputs['check']['requires_authentication']"  # Only if True
)

workflow.add_node(check_node)
workflow.add_node(db_node)
workflow.add_node(auth_node)
workflow.add_edge("check", "setup_db")
workflow.add_edge("check", "setup_auth")

# Execute - setup_db will run, setup_auth will be skipped
executor = DAGExecutor(workflow)
context = await executor.execute()

# Check node states
assert context.get_node_state("setup_db").status == NodeStatus.COMPLETED
assert context.get_node_state("setup_auth").status == NodeStatus.SKIPPED
```

### 7. Retry Logic

**Status:** ðŸ“‹ Proposed

```python
from dag_workflow import RetryPolicy

# Create node with retry policy
node = WorkflowNode(
    node_id="api_integration",
    name="Integrate External API",
    node_type=NodeType.PHASE,
    executor=integrate_api,
    retry_policy=RetryPolicy(
        max_attempts=3,
        retry_on_failure=True,
        retry_delay_seconds=5,
        exponential_backoff=True  # 5s, 10s, 20s delays
    )
)
```

### 8. State Persistence and Resume

**Status:** âœ… Implemented (Basic features) | ðŸ“‹ Proposed (Advanced resume)

> **Context Schemas:** See [AGENT3_DAG_REFERENCE.md](./AGENT3_DAG_REFERENCE.md#context-and-artifact-management) for context persistence schemas.

```python
from dag_executor import WorkflowContextStore

# Create context store
context_store = WorkflowContextStore()

# Create executor with persistence
executor = DAGExecutor(
    workflow=workflow,
    context_store=context_store
)

# Start execution
context = await executor.execute(
    initial_context={"requirement": "Build a todo app"}
)

# If execution is paused/interrupted, resume later
execution_id = context.execution_id

# ... later ...

# Resume execution
executor2 = DAGExecutor(workflow=workflow, context_store=context_store)
context = await executor2.execute(resume_execution_id=execution_id)
```

### 9. Event Tracking

**Status:** âœ… Implemented

> **Event Schemas:** See [AGENT3_DAG_REFERENCE.md](./AGENT3_DAG_REFERENCE.md#event-system) for complete event type definitions and payload schemas.

```python
from dag_executor import ExecutionEvent, ExecutionEventType

# Define event handler
async def handle_event(event: ExecutionEvent):
    if event.event_type == ExecutionEventType.NODE_STARTED:
        print(f"â–¶ Started: {event.node_id}")
    elif event.event_type == ExecutionEventType.NODE_COMPLETED:
        print(f"âœ“ Completed: {event.node_id}")
    elif event.event_type == ExecutionEventType.NODE_FAILED:
        print(f"âœ— Failed: {event.node_id} - {event.data.get('error')}")
    elif event.event_type == ExecutionEventType.WORKFLOW_COMPLETED:
        print(f"âœ“âœ“ Workflow completed!")

# Create executor with event handler
executor = DAGExecutor(
    workflow=workflow,
    event_handler=handle_event
)

context = await executor.execute()
```

**Output:**
```
â–¶ Started: req_analysis
âœ“ Completed: req_analysis
â–¶ Started: system_design
âœ“ Completed: system_design
â–¶ Started: backend_development
â–¶ Started: frontend_development
âœ“ Completed: backend_development
âœ“ Completed: frontend_development
â–¶ Started: testing
âœ“ Completed: testing
âœ“âœ“ Workflow completed!
```

### 10. Custom Phase Executor

```python
from dag_compatibility import PhaseNodeExecutor

# Create custom phase executor
class CustomAnalysisExecutor(PhaseNodeExecutor):
    async def execute(self, node_input: Dict[str, Any]) -> Dict[str, Any]:
        # Custom logic before phase execution
        print(f"Starting custom analysis for {self.phase_name}")

        # Call parent implementation
        result = await super().execute(node_input)

        # Custom logic after phase execution
        print(f"Completed custom analysis: {result}")

        return result

# Use in workflow
custom_executor = CustomAnalysisExecutor(
    phase_name="requirement_analysis",
    team_engine=linear_engine
)

node = WorkflowNode(
    node_id="custom_req_analysis",
    name="Custom Requirement Analysis",
    node_type=NodeType.CUSTOM,
    executor=custom_executor.execute
)
```

## Migration Path

> **Complete Migration Guide:** See [AGENT3_DAG_MIGRATION_GUIDE.md](./AGENT3_DAG_MIGRATION_GUIDE.md) for detailed migration strategy.

### Phase 1: Testing (âœ… Complete)

```python
# Use dual engine with linear mode (default)
dual_engine = create_dual_engine(linear_engine)
result = await dual_engine.execute(requirement)
# Behavior: Identical to existing system
```

### Phase 2: Validation (âœ… Complete)

```python
# Enable DAG linear mode
flags = FeatureFlags()
flags.enable_dag_execution = True
dual_engine = TeamExecutionEngineDual(linear_engine, flags)
result = await dual_engine.execute(requirement)
# Behavior: DAG execution, same order as linear
# Validate: Compare results with Phase 1 âœ… Validated
```

### Phase 3: Parallel Execution (âœ… Complete)

```python
# Enable parallel execution
flags.enable_parallel_execution = True
result = await dual_engine.execute(requirement)
# Behavior: Backend + Frontend run in parallel
# Benefit: ~40% time reduction (measured as of 2025-10-11)
```

### Phase 4: Custom Workflows (ðŸ“‹ Proposed)

```python
# Create custom workflows for specific use cases
if requirement.type == "frontend_only":
    workflow = generate_frontend_only_workflow()
elif requirement.type == "api_only":
    workflow = generate_api_only_workflow()
else:
    workflow = generate_parallel_workflow()

executor = DAGExecutor(workflow)
context = await executor.execute()
```

## Best Practices

### 1. Node Design

- **Single Responsibility**: Each node should have one clear purpose
- **Idempotent**: Nodes should be rerunnable without side effects
- **Error Handling**: Use try/except and return error info
- **Output Structure**: Return consistent dictionary structure

```python
async def good_node_executor(input_data):
    try:
        # Do work
        result = process_data(input_data)

        return {
            "status": "success",
            "data": result,
            "artifacts": ["/path/to/file.txt"],
            "metadata": {"execution_time": 1.5}
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "data": None
        }
```

### 2. Dependency Management

```python
# âœ“ Good: Clear dependencies
node3 = WorkflowNode(
    node_id="testing",
    name="Testing",
    dependencies=["backend", "frontend"]
)

# âœ— Bad: Implicit dependencies
node3 = WorkflowNode(
    node_id="testing",
    name="Testing",
    # Missing dependencies!
)
```

### 3. Context Passing

```python
# âœ“ Good: Access dependency outputs explicitly
async def test_phase(input_data):
    backend_api = input_data['dependency_outputs']['backend']['api_spec']
    frontend_routes = input_data['dependency_outputs']['frontend']['routes']
    # Use them

# âœ— Bad: Access global context without checking
async def test_phase(input_data):
    api = input_data['all_outputs']['backend']['api_spec']  # May not exist!
```

### 4. Monitoring

```python
# Track execution progress
async def progress_handler(event: ExecutionEvent):
    # Log to monitoring system
    logger.info(event.to_dict())

    # Update UI
    await websocket.send_json({
        "type": "workflow_event",
        "event": event.to_dict()
    })

    # Metrics
    if event.event_type == ExecutionEventType.NODE_COMPLETED:
        metrics.record_node_completion(event.node_id)

executor = DAGExecutor(workflow, event_handler=progress_handler)
```

## Performance Comparison

### Linear Mode
```
Total time: 90 seconds
- requirement_analysis: 10s
- design: 15s
- backend_development: 35s (sequential)
- frontend_development: 20s (sequential)
- testing: 7s
- review: 3s
```

### Parallel Mode
```
Total time: 55 seconds (39% faster)
- requirement_analysis: 10s
- design: 15s
- backend + frontend: 35s (parallel, max of the two)
- testing: 7s
- review: 3s
```

## Troubleshooting

### Issue: Nodes not executing in parallel

**Cause**: Dependencies not set correctly

**Solution**:
```python
# Both nodes should depend on same parent
backend.dependencies = ["design"]
frontend.dependencies = ["design"]  # Not ["backend"]
```

### Issue: Context not available in node

**Cause**: Missing dependency declaration

**Solution**:
```python
# Declare dependency explicitly
testing_node.dependencies = ["backend", "frontend"]
```

### Issue: Workflow has cycles

**Cause**: Circular dependencies

**Solution**:
```python
# âœ— Bad: A -> B -> C -> A (cycle)
# âœ“ Good: A -> B -> C
```

## Next Steps

1. **Test with existing requirements**: Run your existing SDLC requirements through dual engine
2. **Enable DAG mode**: Set `MAESTRO_ENABLE_DAG_EXECUTION=true`
3. **Compare results**: Validate that outputs match
4. **Enable parallel execution**: Set `MAESTRO_ENABLE_PARALLEL_EXECUTION=true`
5. **Measure performance**: Track execution times
6. **Create custom workflows**: Design workflows for specific use cases

## Resources

- **Canonical Reference**: [AGENT3_DAG_REFERENCE.md](./AGENT3_DAG_REFERENCE.md) - State machines, events, feature flags
- **Architecture**: [AGENT3_DAG_WORKFLOW_ARCHITECTURE.md](./AGENT3_DAG_WORKFLOW_ARCHITECTURE.md) - System architecture and design
- **Migration Guide**: [AGENT3_DAG_MIGRATION_GUIDE.md](./AGENT3_DAG_MIGRATION_GUIDE.md) - Step-by-step migration path
- **Contract Protocol**: [Universal Contract Protocol](./docs/contract_protocol/UNIVERSAL_CONTRACT_PROTOCOL.md) - Contract integration
- **API Reference**: See inline documentation in `src/dag_workflow.py`, `src/dag_executor.py` (verify paths)
- **Tests**: See `tests/test_dag_system.py` for examples (verify path)
- **Issues**: Report at maestro-platform/maestro-hive/issues

**Note:** Module and file paths should be verified against actual codebase structure as of 2025-10-11.
